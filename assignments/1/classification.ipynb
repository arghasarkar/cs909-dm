{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "In this lab, we will be building and evaluating different classifiers for recognising handwritten digits of MNIST dataset. We will be doing the following:\n",
    "\n",
    "1. Binary Classification of MNIST Dataset\n",
    "\n",
    "In the first set of tasks, you will evaluate a number of popular classifiers for the task of recognizing handwritten digits from MNIST dataset. Specifically, we will focus on distinguishing between 7 and 9 which are known to be a hard pair. \n",
    "\n",
    "2. Exploration of Different Evaluation Metrics. \n",
    "\n",
    "In the second set of tasks, you will learn how (and when) to use evaluation metrics for classifiers.\n",
    "\n",
    "3. Parameter Tuning through Grid Search/Cross Validation and Parallelization.\n",
    "\n",
    "You will learn how to tune your classifier and find optimal parameters using grid search. This is a very computationally intensive task - so you will also explore how to leverage parallelization capabilities of IPython kernel to get results sooner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Applications\\Python3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "#Array processing\n",
    "import numpy as np\n",
    "\n",
    "#Data analysis, wrangling and common exploratory operations\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "#For visualization. Matplotlib for basic viz and seaborn for more stylish figures + statistical figures not in MPL.\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.core.display import Image\n",
    "\n",
    "from sklearn.datasets import fetch_mldata                                                                       \n",
    "from sklearn.utils import shuffle                                                                                   \n",
    "from sklearn import metrics                                                                                                  \n",
    "from sklearn import tree                                                                                                     \n",
    "from sklearn.tree import DecisionTreeClassifier                                                       \n",
    "from sklearn.naive_bayes import MultinomialNB  \n",
    "from sklearn.linear_model import LogisticRegression                                            \n",
    "from sklearn.ensemble import RandomForestClassifier                                                                                                          \n",
    "from sklearn.model_selection import KFold, train_test_split, cross_val_score, GridSearchCV                                                \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pydot, io\n",
    "import time\n",
    "\n",
    "#######################End imports###################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Binary Classification of MNIST Dataset\n",
    "\n",
    "In the first set of tasks, you will evaluate a number of popular classifiers for the task of recognizing handwritten digits from MNIST dataset. Specifically, we will focus on distinguishing between 7 and 9 which are known to be a hard pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Images = 70000 and #Pixel per image = 784\n",
      "First image shows 0\n",
      "The corresponding matrix version of image is \n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  51 159 253 159  50   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  48 238 252 252 252 237   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  54 227 253 252 239 233 252  57   6   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0  10  60 224 252 253 252 202  84 252\n",
      " 253 122   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0 163 252 252 252 253 252 252  96 189 253 167   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  51 238 253 253 190 114 253 228\n",
      "  47  79 255 168   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  48 238 252 252 179  12  75 121  21   0   0 253 243  50   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  38 165 253 233 208  84   0   0\n",
      "   0   0   0   0 253 252 165   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   7 178 252 240  71  19  28   0   0   0   0   0   0 253 252 195   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0  57 252 252  63   0   0   0\n",
      "   0   0   0   0   0   0 253 252 195   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0 198 253 190   0   0   0   0   0   0   0   0   0   0 255 253\n",
      " 196   0   0   0   0   0   0   0   0   0   0   0  76 246 252 112   0   0\n",
      "   0   0   0   0   0   0   0   0 253 252 148   0   0   0   0   0   0   0\n",
      "   0   0   0   0  85 252 230  25   0   0   0   0   0   0   0   0   7 135\n",
      " 253 186  12   0   0   0   0   0   0   0   0   0   0   0  85 252 223   0\n",
      "   0   0   0   0   0   0   0   7 131 252 225  71   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0  85 252 145   0   0   0   0   0   0   0  48 165\n",
      " 252 173   0   0   0   0   0   0   0   0   0   0   0   0   0   0  86 253\n",
      " 225   0   0   0   0   0   0 114 238 253 162   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0  85 252 249 146  48  29  85 178 225 253\n",
      " 223 167  56   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  85 252 252 252 229 215 252 252 252 196 130   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  28 199 252 252 253 252 252 233\n",
      " 145   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0  25 128 252 253 252 141  37   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n",
      "The image in grey shape is \n",
      "Shape of data and labels are : (14251, 784) (14251,)\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.04313725 0.34117647 0.54117647 0.54117647 0.54117647\n",
      " 0.74901961 0.50196078 0.0745098  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.05098039 0.34901961 0.79607843\n",
      " 0.89411765 0.80784314 0.68235294 0.36078431 0.36078431 0.62352941\n",
      " 0.9254902  0.17254902 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.17647059 0.6745098  0.98823529 0.74117647 0.1254902  0.\n",
      " 0.         0.         0.         0.09411765 0.98823529 0.67058824\n",
      " 0.05098039 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.24313725 0.9254902  0.89019608\n",
      " 0.25882353 0.01568627 0.         0.         0.         0.\n",
      " 0.         0.09411765 0.98823529 0.98823529 0.17647059 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.41568627 0.95294118 0.90588235 0.18823529 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.29803922\n",
      " 0.98823529 0.98823529 0.17647059 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.40784314 0.99215686 0.86666667\n",
      " 0.16470588 0.         0.         0.         0.         0.\n",
      " 0.         0.24313725 0.74901961 1.         0.99215686 0.80392157\n",
      " 0.08235294 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.25882353 0.9254902  0.98823529 0.         0.         0.\n",
      " 0.         0.         0.         0.03529412 0.18431373 0.84313725\n",
      " 0.98823529 0.97647059 0.80784314 0.21960784 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.05098039 0.79607843 0.98823529\n",
      " 0.53333333 0.         0.         0.         0.         0.\n",
      " 0.1254902  0.30588235 0.98823529 0.98823529 0.98823529 0.45098039\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.18431373 0.98823529 0.69019608 0.01568627 0.\n",
      " 0.         0.         0.         0.33333333 0.89803922 0.85882353\n",
      " 0.51764706 0.98823529 0.82352941 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.10196078\n",
      " 0.86666667 0.62745098 0.         0.         0.29019608 0.57254902\n",
      " 0.90196078 0.98039216 0.29411765 0.07058824 0.68235294 0.98823529\n",
      " 0.33333333 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.65882353 0.97647059\n",
      " 0.99215686 1.         0.97647059 0.78039216 0.45490196 0.04313725\n",
      " 0.         0.22352941 0.99215686 0.75294118 0.04313725 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.4627451  0.80784314 0.36078431\n",
      " 0.29411765 0.         0.         0.         0.01960784 0.69803922\n",
      " 0.94117647 0.14509804 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.42352941 0.98823529 0.59215686 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.91372549 0.92156863 0.17254902 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.61960784 0.99215686 0.6\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.03529412 0.91372549 0.94901961 0.16470588 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.36470588 0.98823529\n",
      " 0.52941176 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.48627451 0.98823529 0.1254902  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.10196078\n",
      " 0.87843137 0.98823529 0.29019608 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.14901961 0.70196078 0.37254902\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAADpdJREFUeJzt3X2MVGWWx/HfkRl8ASWiLUEHbRZx40tis6mQTYZs2IwzQZ0EiS+BqGEMkQkRdcz4FoxZYzSRdWcQ4mpsFiKss8xsGIz8YdZRshEnGSeW4Iro7upiI3SQLiJkHI0ODWf/6OukR7ueKqpu1a3u8/0kna665z59Twp+favuU12PubsAxHNS0Q0AKAbhB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8Q1LfaebCzzz7bu7u723lIIJS+vj4dOnTI6tm3qfCb2TxJqyWNk/Qv7v5Yav/u7m6Vy+VmDgkgoVQq1b1vw0/7zWycpH+WdKWkSyQtMrNLGv15ANqrmdf8syV94O573P1Pkn4paX4+bQFotWbCf56kfcPu78+2/QUzW2pmZTMrVyqVJg4HIE8tv9rv7r3uXnL3UldXV6sPB6BOzYS/X9K0Yfe/k20DMAo0E/43JM00s+lmNl7SQklb82kLQKs1PNXn7oNmtlzSSxqa6lvv7rtz6wxASzU1z+/uL0p6MadeALQRb+8FgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+IKi2LtGNsWffvn3J+urVq6vWVq1alRx71113Jet33nlnsj5t2rRkPTrO/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QVFPz/GbWJ+lTScckDbp7KY+m0Dn6+/uT9VmzZiXrR44cqVozs+TYJ554IlnfsGFDsl6pVJL16PJ4k8/fu/uhHH4OgDbiaT8QVLPhd0m/MbM3zWxpHg0BaI9mn/bPcfd+MztH0stm9t/uvn34DtkvhaWSdP755zd5OAB5aerM7+792fcBSc9Lmj3CPr3uXnL3UldXVzOHA5CjhsNvZhPM7PSvbkv6gaR38moMQGs187R/iqTns+mab0n6N3f/j1y6AtByDYff3fdIujzHXlCAvXv3Jutz585N1g8fPpysp+byJ02alBx78sknJ+sDAwPJ+p49e6rWLrjgguTYcePGJetjAVN9QFCEHwiK8ANBEX4gKMIPBEX4gaD46O4x4OjRo1Vrtaby5s2bl6zX+mjuZvT09CTrjz76aLI+Z86cZH3mzJlVa729vcmxS5YsSdbHAs78QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU8/xjwD333FO19uSTT7axkxPz6quvJuufffZZsr5gwYJkfcuWLVVrO3fuTI6NgDM/EBThB4Ii/EBQhB8IivADQRF+ICjCDwTFPP8oUOtv6p977rmqNXdv6ti15tKvvfbaZP2mm26qWps2bVpy7MUXX5ys33fffcn65s2bq9aafVzGAs78QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxCU1ZrvNLP1kn4oacDdL8u2TZb0K0ndkvok3eDu6bWaJZVKJS+Xy022PPb09/cn65dfnl4J/ciRIw0f+8Ybb0zW165dm6y/++67yfqOHTuq1hYuXJgce9pppyXrtaSW2Z4wYUJy7O7du5P1Wu9RKEqpVFK5XK6+Lvow9Zz5n5X09ZUd7pe0zd1nStqW3QcwitQMv7tvl/TJ1zbPl7Qhu71B0jU59wWgxRp9zT/F3Q9ktz+WNCWnfgC0SdMX/HzookHVCwdmttTMymZWrlQqzR4OQE4aDf9BM5sqSdn3gWo7unuvu5fcvdTV1dXg4QDkrdHwb5W0OLu9WNIL+bQDoF1qht/MNkn6naS/NrP9ZrZE0mOSvm9m70u6IrsPYBSp+ff87r6oSul7OfcyZh06dChZX7lyZbJ++HD6LRRTplS/3jp9+vTk2GXLliXr48ePT9Z7enqaqhfl888/T9Yff/zxZH3NmjV5tlMI3uEHBEX4gaAIPxAU4QeCIvxAUIQfCIqP7s7B4OBgsn733Xcn66mP3pakSZMmJesvvfRS1dqFF16YHHv06NFkPaoPP/yw6BZajjM/EBThB4Ii/EBQhB8IivADQRF+ICjCDwTFPH8OPvroo2S91jx+La+//nqyftFFFzX8s0899dSGx2J048wPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0Exz5+D2267LVmvtQz6ggULkvVm5vEjO378eNXaSSelz3u1/s3GAs78QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxBUzXl+M1sv6YeSBtz9smzbQ5JulVTJdlvh7i+2qslOsHPnzqq17du3J8eaWbJ+/fXXN9QT0lJz+bX+TUqlUt7tdJx6zvzPSpo3wvZV7t6TfY3p4ANjUc3wu/t2SZ+0oRcAbdTMa/7lZva2ma03szNz6whAWzQa/qclzZDUI+mApJ9V29HMlppZ2czKlUql2m4A2qyh8Lv7QXc/5u7HJa2VNDuxb6+7l9y91NXV1WifAHLWUPjNbOqwuwskvZNPOwDapZ6pvk2S5ko628z2S/oHSXPNrEeSS+qT9OMW9gigBWqG390XjbB5XQt66WhffPFF1dqXX36ZHHvuuecm61dffXVDPY11g4ODyfqaNWsa/tnXXXddsr5ixYqGf/ZowTv8gKAIPxAU4QeCIvxAUIQfCIrwA0Hx0d1tcMoppyTrEydObFMnnaXWVN7TTz+drN97773Jend3d9XaAw88kBw7fvz4ZH0s4MwPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0Exz98GN998c9EtFKa/v79qbeXKlcmxTz31VLJ+yy23JOtr165N1qPjzA8ERfiBoAg/EBThB4Ii/EBQhB8IivADQTHPXyd3b6gmSc8++2yy/uCDDzbSUkfYtGlTsn777bdXrR0+fDg59o477kjWV61alawjjTM/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRVc57fzKZJ2ihpiiSX1Ovuq81ssqRfSeqW1CfpBndPT9yOYmbWUE2S9u/fn6w//PDDyfqSJUuS9dNPP71qbffu3cmxzzzzTLL+2muvJet9fX3J+owZM6rWFi5cmBxba54fzannzD8o6afufomkv5V0m5ldIul+Sdvcfaakbdl9AKNEzfC7+wF335Hd/lTSe5LOkzRf0oZstw2SrmlVkwDyd0Kv+c2sW9IsSb+XNMXdD2SljzX0sgDAKFF3+M1soqRfS/qJu/9heM2H3tw+4hvczWypmZXNrFypVJpqFkB+6gq/mX1bQ8H/hbtvyTYfNLOpWX2qpIGRxrp7r7uX3L3U1dWVR88AclAz/DZ0KXudpPfc/efDSlslLc5uL5b0Qv7tAWiVev6k97uSbpa0y8zeyratkPSYpH83syWS9kq6oTUtjn7Hjh1L1mtN9a1bty5Znzx5ctXarl27kmObdeWVVybr8+bNq1pbvnx53u3gBNQMv7v/VlK1iezv5dsOgHbhHX5AUIQfCIrwA0ERfiAowg8ERfiBoPjo7jpdeumlVWtXXHFFcuwrr7zS1LFr/UlwahnsWs4555xkfdmyZcn6aP7Y8eg48wNBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUMzz1+mMM86oWtu8eXNy7MaNG5P1Vn5E9SOPPJKs33rrrcn6WWedlWc76CCc+YGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKBtaaas9SqWSl8vlth0PiKZUKqlcLqfXjM9w5geCIvxAUIQfCIrwA0ERfiAowg8ERfiBoGqG38ymmdl/mtm7ZrbbzO7Mtj9kZv1m9lb2dVXr2wWQl3o+zGNQ0k/dfYeZnS7pTTN7Oautcvd/al17AFqlZvjd/YCkA9ntT83sPUnntboxAK11Qq/5zaxb0ixJv882LTezt81svZmdWWXMUjMrm1m5Uqk01SyA/NQdfjObKOnXkn7i7n+Q9LSkGZJ6NPTM4GcjjXP3XncvuXupq6srh5YB5KGu8JvZtzUU/F+4+xZJcveD7n7M3Y9LWitpduvaBJC3eq72m6R1kt5z958P2z512G4LJL2Tf3sAWqWeq/3flXSzpF1m9la2bYWkRWbWI8kl9Un6cUs6BNAS9Vzt/62kkf4++MX82wHQLrzDDwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EFRbl+g2s4qkvcM2nS3pUNsaODGd2lun9iXRW6Py7O0Cd6/r8/LaGv5vHNys7O6lwhpI6NTeOrUvid4aVVRvPO0HgiL8QFBFh7+34OOndGpvndqXRG+NKqS3Ql/zAyhO0Wd+AAUpJPxmNs/M/sfMPjCz+4vooRoz6zOzXdnKw+WCe1lvZgNm9s6wbZPN7GUzez/7PuIyaQX11hErNydWli70seu0Fa/b/rTfzMZJ+l9J35e0X9Ibkha5+7ttbaQKM+uTVHL3wueEzezvJP1R0kZ3vyzb9o+SPnH3x7JfnGe6+30d0ttDkv5Y9MrN2YIyU4evLC3pGkk/UoGPXaKvG1TA41bEmX+2pA/cfY+7/0nSLyXNL6CPjufu2yV98rXN8yVtyG5v0NB/nrar0ltHcPcD7r4ju/2ppK9Wli70sUv0VYgiwn+epH3D7u9XZy357ZJ+Y2ZvmtnSopsZwZRs2XRJ+ljSlCKbGUHNlZvb6WsrS3fMY9fIitd544LfN81x97+RdKWk27Kntx3Jh16zddJ0TV0rN7fLCCtL/1mRj12jK17nrYjw90uaNuz+d7JtHcHd+7PvA5KeV+etPnzwq0VSs+8DBffzZ520cvNIK0urAx67TlrxuojwvyFppplNN7PxkhZK2lpAH99gZhOyCzEyswmSfqDOW314q6TF2e3Fkl4osJe/0CkrN1dbWVoFP3Ydt+K1u7f9S9JVGrri/3+SHiiihyp9/ZWk/8q+dhfdm6RNGnoaeFRD10aWSDpL0jZJ70t6RdLkDurtXyXtkvS2hoI2taDe5mjoKf3bkt7Kvq4q+rFL9FXI48Y7/ICguOAHBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiCo/wfNDnvJ0xlPmwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27571b0ce80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "####################Do not change anything below\n",
    "#Load MNIST data. fetch_mldata will download the dataset and put it in a folder called mldata. \n",
    "#Some things to be aware of:\n",
    "#   The folder mldata will be created in the folder in which you started the notebook\n",
    "#   So to make your life easy, always start IPython notebook from same folder.\n",
    "#   Else the following code will keep downloading MNIST data\n",
    "mnist = fetch_mldata(\"MNIST original\")                      \n",
    "#The data is organized as follows:\n",
    "#  Each row corresponds to an image\n",
    "#  Each image has 28*28 pixels which is then linearized to a vector of size 784 (ie. 28*28)\n",
    "# mnist.data gives the image information while mnist.target gives the number in the image\n",
    "print(\"#Images = %d and #Pixel per image = %s\" % (mnist.data.shape[0], mnist.data.shape[1]))\n",
    "\n",
    "#Print first row of the dataset \n",
    "img = mnist.data[0]                                                                                                          \n",
    "print(\"First image shows %d\" % (mnist.target[0]))\n",
    "print(\"The corresponding matrix version of image is \\n\" , img)\n",
    "print(\"The image in grey shape is \")\n",
    "plt.imshow(img.reshape(28, 28), cmap=\"Greys\")                                                                                \n",
    "                                                                                                                             \n",
    "#First 60K images are for training and last 10K are for testing\n",
    "all_train_data = mnist.data[:60000]                                                                                          \n",
    "all_test_data = mnist.data[60000:]                                                                                           \n",
    "all_train_labels = mnist.target[:60000]                                                                                      \n",
    "all_test_labels = mnist.target[60000:]                                                                                       \n",
    "                                                              \n",
    "                                                                                                                             \n",
    "#For the first task, we will be doing binary classification and focus  on two pairs of \n",
    "#  numbers: 7 and 9 which are known to be hard to distinguish\n",
    "#Get all the seven images\n",
    "sevens_data = mnist.data[mnist.target==7]      \n",
    "#Get all the nine images\n",
    "nines_data = mnist.data[mnist.target==9]       \n",
    "#Merge them to create a new dataset\n",
    "binary_class_data = np.vstack([sevens_data, nines_data])    \n",
    "binary_class_labels = np.hstack([np.repeat(7, sevens_data.shape[0]), np.repeat(9, nines_data.shape[0])])    \n",
    " \n",
    "#In order to make the experiments repeatable, we will seed the random number generator to a known value\n",
    "# That way the results of the experiments will always be same\n",
    "np.random.seed(1234)                        \n",
    "#randomly shuffle the data\n",
    "binary_class_data, binary_class_labels = shuffle(binary_class_data, binary_class_labels)  \n",
    "print(\"Shape of data and labels are :\" , binary_class_data.shape, binary_class_labels.shape)  \n",
    "\n",
    "#There are approximately 14K images of 7 and 9. \n",
    "#Let us take the first 5000 as training and remaining as test data                                          \n",
    "orig_binary_class_training_data = binary_class_data[:5000]                                                  \n",
    "binary_class_training_labels = binary_class_labels[:5000]                                                   \n",
    "orig_binary_class_testing_data = binary_class_data[5000:]                                                   \n",
    "binary_class_testing_labels = binary_class_labels[5000:] \n",
    "\n",
    "#The images are in grey scale where each number is between 0 to 255\n",
    "# Now let us normalize them so that the values are between 0 and 1. \n",
    "# This will be the only modification we will make to the image\n",
    "binary_class_training_data = orig_binary_class_training_data / 255.0                                        \n",
    "binary_class_testing_data = orig_binary_class_testing_data / 255.0                                          \n",
    "scaled_training_data = all_train_data / 255.0                                                                                \n",
    "scaled_testing_data = all_test_data / 255.0  \n",
    "\n",
    "print(binary_class_training_data[0,:])                                                                 \n",
    "     \n",
    "###########Make sure that you remember the variable names and their meaning\n",
    "#binary_class_training_data, binary_class_training_labels: Normalized images of 7 and 9 and the correct labels for training\n",
    "#binary_class_testing_data, binary_class_testing_labels : Normalized images of 7 and 9 and correct labels for testing\n",
    "#orig_binary_class_training_data, orig_binary_class_testing_data: Unnormalized images of 7 and 9\n",
    "#all_train_data, all_test_data: un normalized images of all digits \n",
    "#all_train_labels, all_test_labels: labels for all digits\n",
    "#scaled_training_data, scaled_testing_data: Normalized version of all_train_data, all_test_data for all digits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification in scikit-learn\n",
    "\n",
    "All classifiers in scikit-learn follow a common pattern that makes life much easier. \n",
    "Follow these steps for all the tasks below.\n",
    "\n",
    "1. Instantiate the classifier with appropriate parameters\n",
    "2. Train/fit the classifier with training data and correct labels\n",
    "3. Test the classifier with unseen data\n",
    "4. Evaluate the performance of classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Decision Trees (10 marks)\n",
    "\n",
    "In the first task, you will use Decision trees (see url\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\n",
    ") for classification. \n",
    "\n",
    "Implement a CART decision tree with splitting criterion as entropy. Also print the learned decision tree using the supplied function \"plot_tree\" below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Do not make any change below\n",
    "def plot_dtree(model,fileName):                                                                                              \n",
    "    #You would have to install a Python package pydot                                                                        \n",
    "    #You would also have to install graphviz for your system - see http://www.graphviz.org/Download..php                     \n",
    "    #If you get any pydot error, see url\n",
    "    # http://stackoverflow.com/questions/15951748/pydot-and-graphviz-error-couldnt-import-dot-parser-loading-of-dot-files-will\n",
    "    dot_tree_data = io.StringIO()                                                                                      \n",
    "    tree.export_graphviz(model, out_file = dot_tree_data)                                                                   \n",
    "    (dtree_graph,) = pydot.graph_from_dot_data(dot_tree_data.getvalue())                                                        \n",
    "    dtree_graph.write_png(fileName)                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 (10 marks)\n",
    "# Create a CART decision tree with splitting criterion as entropy\n",
    "# Remember to set the random state to 1234\n",
    "\n",
    "RANDOM_STATE = 1234\n",
    "\n",
    "cart = DecisionTreeClassifier(random_state = RANDOM_STATE, criterion = 'entropy').fit(binary_class_training_data, binary_class_training_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction accuracy is:  96.94087125716139 %\n"
     ]
    }
   ],
   "source": [
    "prediction = cart.predict(binary_class_testing_data)\n",
    "###########################################################################################################\n",
    "##########################################################################################################\n",
    "\"\"\"\n",
    "Check the accuracy\n",
    "\"\"\"\n",
    "print(\"The prediction accuracy is: \", cart.score(binary_class_testing_data, binary_class_testing_labels)*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.52380952, 0.4       , 0.55      , 0.5       , 0.45      ,\n",
       "       0.6       , 0.3       , 0.5       , 0.45      , 0.57894737])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(cart, binary_class_training_data[:200], binary_class_training_labels[200:400], cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 9 7 ... 7 9 9]\n"
     ]
    }
   ],
   "source": [
    "prediction = cart.predict(binary_class_testing_data)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the decision tree as .png image\n",
    "\n",
    "The image saving is not working on my personal machines so it'll only run on other machines (DCS machines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArghaWin10\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "print(username)\n",
    "\n",
    "if username is \"ArghaWin10\":\n",
    "    plot_dtree(cart, \"tree.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Naive Bayes (10 marks)\n",
    "\n",
    "In this task, you will create a multinomial Naive Bayes classifiers and evaluate it. \n",
    "\n",
    "You might want to use the following url\n",
    "http://scikit-learn.org/stable/modules/naive_bayes.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 (10 marks)\n",
    "# Create multinomial NB\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "mnb_model = mnb.fit(binary_class_training_data, binary_class_training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction accuracy is:  91.59009836774403 %\n"
     ]
    }
   ],
   "source": [
    "print(\"The prediction accuracy is: \", mnb_model.score(binary_class_testing_data, binary_class_testing_labels)*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Logistic Regression (10 marks)\n",
    "\n",
    "Logistic regression is a simple classifier that converts a regression model into a classification one.\n",
    "You can read the details at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3 (10 marks)\n",
    "# Create a model with default parameters. Remember to set random state to 1234\n",
    "lgr = LogisticRegression(random_state = RANDOM_STATE, solver = \"lbfgs\", multi_class=\"multinomial\")\n",
    "\n",
    "lgr_model = lgr.fit(binary_class_training_data, binary_class_training_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction accuracy is:  95.65452383526105 %\n"
     ]
    }
   ],
   "source": [
    "print(\"The prediction accuracy is: \", lgr_model.score(binary_class_testing_data, binary_class_testing_labels)*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Random Forests (10 marks)\n",
    "\n",
    "Random Forests is a very popular ensemble method. See url \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "for details. Implement a Random Forest (10 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4 (10 marks)\n",
    "# Create a random forest classifier with Default parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Exploration of Different Evaluation Metrics for Binary Classification\n",
    "\n",
    "Let us evaluate different metrics for the binary classification models that we created so far. \n",
    "You may want to check the url http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics for additional details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Print the classification results (total: 20 marks)\n",
    "\n",
    "For each of the models above:\n",
    "\n",
    "- Task 5a: Print the classification report and confusion matrix (5 marks)\n",
    "- Task 5b: Print the ROC curve (5 marks)\n",
    "- Task 5c: Print the AUC curve (5 marks)\n",
    "- Task 5d: Print the precision/recall curve (5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task t5a (5 marks)\n",
    "# Print the classification report and confusion matrix for each of the models above\n",
    "# Write code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task t5b (5 marks)\n",
    "# Each of the model above has some probabilistic interpretation\n",
    "# So sklearn allows you to get the probability values as part of classification\n",
    "# Using this information, you can print roc_curve\n",
    "# See http://nbviewer.ipython.org/github/datadave/GADS9-NYC-Spring2014-Lectures/blob/master/lessons/lesson09_decision_trees_random_forests/sklearn_decision_trees.ipynb\n",
    "# Write code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task t5c (5 marks)\n",
    "# Print the AUC value for each of the models above\n",
    "# Write code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task t5d (5 marks)\n",
    "# Print the precision recall curve for each of the models above\n",
    "# print the curve based on http://scikit-learn.org/stable/auto_examples/plot_precision_recall.html   \n",
    "# Write code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Parameter Tuning through Grid Search/Cross Validation and Parallelization\n",
    "\n",
    "So far in this assignment, you manually tweaked the model till it became better.\n",
    "For complex models, this is often cumbersome. A common trick people use is called Grid Search where you exhaustively test various parameter combinations\n",
    "and pick the best set of parameter values. This is a VERY computationally intensive process and hence it will require some parallelization.\n",
    "\n",
    "In this part, you will learn how to tune Randm Forest for MNIST dataset\n",
    "and then parallelize it so as to get results faster.\n",
    "You might want to take a look at the url\n",
    "http://scikit-learn.org/stable/modules/grid_search.html\n",
    "for additional details.\n",
    "\n",
    "One thing you might want to note is that the GridSearchCV uses cross validation for comparing models.\n",
    "\n",
    "So you have to send the ENTIRE MNIST dataset - i.e. mnist.data and mnist.target. \n",
    "\n",
    "The following cell creates two variables all_scaled_data and all_scaled_target that you can pass to GridSearchCV.\n",
    "\n",
    "In order to get the results in reasonable time, set the **cv** parameter of GridSearchCV to 3.\n",
    "Also remember to set the **verbose** parameter to 2 to get some details about what happens internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Do not make any change below\n",
    "all_scaled_data = binary_class_data / 255.0\n",
    "all_scaled_target = binary_class_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Tuning parameters for Random Forest using grid search (15 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 6 (15 marks)\n",
    "# Tuning Random Forest for MNIST\n",
    "tuned_parameters = [{'max_features': ['sqrt', 'log2'], 'n_estimators': [1000, 1500]}] \n",
    "\n",
    "# Write code here\n",
    "\n",
    "# print the details of the best model and its accuracy\n",
    "# Write code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

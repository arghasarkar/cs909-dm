{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "In this lab, we will be building and evaluating different classifiers for recognising handwritten digits of MNIST dataset. We will be doing the following:\n",
    "\n",
    "1. Binary Classification of MNIST Dataset\n",
    "\n",
    "In the first set of tasks, you will evaluate a number of popular classifiers for the task of recognizing handwritten digits from MNIST dataset. Specifically, we will focus on distinguishing between 7 and 9 which are known to be a hard pair. \n",
    "\n",
    "2. Exploration of Different Evaluation Metrics. \n",
    "\n",
    "In the second set of tasks, you will learn how (and when) to use evaluation metrics for classifiers.\n",
    "\n",
    "3. Parameter Tuning through Grid Search/Cross Validation and Parallelization.\n",
    "\n",
    "You will learn how to tune your classifier and find optimal parameters using grid search. This is a very computationally intensive task - so you will also explore how to leverage parallelization capabilities of IPython kernel to get results sooner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "#Array processing\n",
    "import numpy as np\n",
    "\n",
    "#Data analysis, wrangling and common exploratory operations\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "#For visualization. Matplotlib for basic viz and seaborn for more stylish figures + statistical figures not in MPL.\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.core.display import Image\n",
    "\n",
    "from sklearn.datasets import fetch_mldata                                                                       \n",
    "from sklearn.utils import shuffle                                                                                   \n",
    "from sklearn import metrics                                                                                                  \n",
    "from sklearn import tree                                                                                                     \n",
    "from sklearn.tree import DecisionTreeClassifier                                                       \n",
    "from sklearn.naive_bayes import MultinomialNB  \n",
    "from sklearn.linear_model import LogisticRegression                                            \n",
    "from sklearn.ensemble import RandomForestClassifier                                                                                                          \n",
    "from sklearn.model_selection import KFold, train_test_split, cross_val_score, GridSearchCV                                                \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pydot, io\n",
    "import time\n",
    "\n",
    "#######################End imports###################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot-ng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Binary Classification of MNIST Dataset\n",
    "\n",
    "In the first set of tasks, you will evaluate a number of popular classifiers for the task of recognizing handwritten digits from MNIST dataset. Specifically, we will focus on distinguishing between 7 and 9 which are known to be a hard pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################Do not change anything below\n",
    "#Load MNIST data. fetch_mldata will download the dataset and put it in a folder called mldata. \n",
    "#Some things to be aware of:\n",
    "#   The folder mldata will be created in the folder in which you started the notebook\n",
    "#   So to make your life easy, always start IPython notebook from same folder.\n",
    "#   Else the following code will keep downloading MNIST data\n",
    "mnist = fetch_mldata(\"MNIST original\")                      \n",
    "#The data is organized as follows:\n",
    "#  Each row corresponds to an image\n",
    "#  Each image has 28*28 pixels which is then linearized to a vector of size 784 (ie. 28*28)\n",
    "# mnist.data gives the image information while mnist.target gives the number in the image\n",
    "print(\"#Images = %d and #Pixel per image = %s\" % (mnist.data.shape[0], mnist.data.shape[1]))\n",
    "\n",
    "#Print first row of the dataset \n",
    "img = mnist.data[0]                                                                                                          \n",
    "print(\"First image shows %d\" % (mnist.target[0]))\n",
    "print(\"The corresponding matrix version of image is \\n\" , img)\n",
    "print(\"The image in grey shape is \")\n",
    "plt.imshow(img.reshape(28, 28), cmap=\"Greys\")                                                                                \n",
    "                                                                                                                             \n",
    "#First 60K images are for training and last 10K are for testing\n",
    "all_train_data = mnist.data[:60000]                                                                                          \n",
    "all_test_data = mnist.data[60000:]                                                                                           \n",
    "all_train_labels = mnist.target[:60000]                                                                                      \n",
    "all_test_labels = mnist.target[60000:]                                                                                       \n",
    "                                                              \n",
    "                                                                                                                             \n",
    "#For the first task, we will be doing binary classification and focus  on two pairs of \n",
    "#  numbers: 7 and 9 which are known to be hard to distinguish\n",
    "#Get all the seven images\n",
    "sevens_data = mnist.data[mnist.target==7]      \n",
    "#Get all the nine images\n",
    "nines_data = mnist.data[mnist.target==9]       \n",
    "#Merge them to create a new dataset\n",
    "binary_class_data = np.vstack([sevens_data, nines_data])    \n",
    "binary_class_labels = np.hstack([np.repeat(7, sevens_data.shape[0]), np.repeat(9, nines_data.shape[0])])    \n",
    " \n",
    "#In order to make the experiments repeatable, we will seed the random number generator to a known value\n",
    "# That way the results of the experiments will always be same\n",
    "np.random.seed(1234)                        \n",
    "#randomly shuffle the data\n",
    "binary_class_data, binary_class_labels = shuffle(binary_class_data, binary_class_labels)  \n",
    "print(\"Shape of data and labels are :\" , binary_class_data.shape, binary_class_labels.shape)  \n",
    "\n",
    "#There are approximately 14K images of 7 and 9. \n",
    "#Let us take the first 5000 as training and remaining as test data                                          \n",
    "orig_binary_class_training_data = binary_class_data[:5000]                                                  \n",
    "binary_class_training_labels = binary_class_labels[:5000]                                                   \n",
    "orig_binary_class_testing_data = binary_class_data[5000:]                                                   \n",
    "binary_class_testing_labels = binary_class_labels[5000:] \n",
    "\n",
    "#The images are in grey scale where each number is between 0 to 255\n",
    "# Now let us normalize them so that the values are between 0 and 1. \n",
    "# This will be the only modification we will make to the image\n",
    "binary_class_training_data = orig_binary_class_training_data / 255.0                                        \n",
    "binary_class_testing_data = orig_binary_class_testing_data / 255.0                                          \n",
    "scaled_training_data = all_train_data / 255.0                                                                                \n",
    "scaled_testing_data = all_test_data / 255.0  \n",
    "\n",
    "print(binary_class_training_data[0,:])                                                                 \n",
    "     \n",
    "###########Make sure that you remember the variable names and their meaning\n",
    "#binary_class_training_data, binary_class_training_labels: Normalized images of 7 and 9 and the correct labels for training\n",
    "#binary_class_testing_data, binary_class_testing_labels : Normalized images of 7 and 9 and correct labels for testing\n",
    "#orig_binary_class_training_data, orig_binary_class_testing_data: Unnormalized images of 7 and 9\n",
    "#all_train_data, all_test_data: un normalized images of all digits \n",
    "#all_train_labels, all_test_labels: labels for all digits\n",
    "#scaled_training_data, scaled_testing_data: Normalized version of all_train_data, all_test_data for all digits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification in scikit-learn\n",
    "\n",
    "All classifiers in scikit-learn follow a common pattern that makes life much easier. \n",
    "Follow these steps for all the tasks below.\n",
    "\n",
    "1. Instantiate the classifier with appropriate parameters\n",
    "2. Train/fit the classifier with training data and correct labels\n",
    "3. Test the classifier with unseen data\n",
    "4. Evaluate the performance of classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Decision Trees (10 marks)\n",
    "\n",
    "In the first task, you will use Decision trees (see url\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\n",
    ") for classification. \n",
    "\n",
    "Implement a CART decision tree with splitting criterion as entropy. Also print the learned decision tree using the supplied function \"plot_tree\" below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Do not make any change below\n",
    "def plot_dtree(model,fileName):                                                                                              \n",
    "    #You would have to install a Python package pydot                                                                        \n",
    "    #You would also have to install graphviz for your system - see http://www.graphviz.org/Download..php                     \n",
    "    #If you get any pydot error, see url\n",
    "    # http://stackoverflow.com/questions/15951748/pydot-and-graphviz-error-couldnt-import-dot-parser-loading-of-dot-files-will\n",
    "    dot_tree_data = io.StringIO()                                                                                      \n",
    "    tree.export_graphviz(model, out_file = dot_tree_data)                                                                   \n",
    "    (dtree_graph,) = pydot.graph_from_dot_data(dot_tree_data.getvalue())                                                        \n",
    "    dtree_graph.write_png(fileName)                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 (10 marks)\n",
    "# Create a CART decision tree with splitting criterion as entropy\n",
    "# Remember to set the random state to 1234\n",
    "\n",
    "RANDOM_STATE = 1234\n",
    "\n",
    "cart = DecisionTreeClassifier(random_state = RANDOM_STATE, criterion = 'entropy').fit(binary_class_training_data, binary_class_training_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = cart.predict(binary_class_testing_data)\n",
    "###########################################################################################################\n",
    "##########################################################################################################\n",
    "\"\"\"\n",
    "Check the accuracy\n",
    "\"\"\"\n",
    "print(\"The prediction accuracy is: \", cart.score(binary_class_testing_data, binary_class_testing_labels)*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(cart, binary_class_training_data[:200], binary_class_training_labels[200:400], cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = cart.predict(binary_class_testing_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dtree(cart, \"tree.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Naive Bayes (10 marks)\n",
    "\n",
    "In this task, you will create a multinomial Naive Bayes classifiers and evaluate it. \n",
    "\n",
    "You might want to use the following url\n",
    "http://scikit-learn.org/stable/modules/naive_bayes.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 (10 marks)\n",
    "# Create multinomial NB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Logistic Regression (10 marks)\n",
    "\n",
    "Logistic regression is a simple classifier that converts a regression model into a classification one.\n",
    "You can read the details at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3 (10 marks)\n",
    "# Create a model with default parameters. Remember to set random state to 1234\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Random Forests (10 marks)\n",
    "\n",
    "Random Forests is a very popular ensemble method. See url \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "for details. Implement a Random Forest (10 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4 (10 marks)\n",
    "# Create a random forest classifier with Default parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Exploration of Different Evaluation Metrics for Binary Classification\n",
    "\n",
    "Let us evaluate different metrics for the binary classification models that we created so far. \n",
    "You may want to check the url http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics for additional details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Print the classification results (total: 20 marks)\n",
    "\n",
    "For each of the models above:\n",
    "\n",
    "- Task 5a: Print the classification report and confusion matrix (5 marks)\n",
    "- Task 5b: Print the ROC curve (5 marks)\n",
    "- Task 5c: Print the AUC curve (5 marks)\n",
    "- Task 5d: Print the precision/recall curve (5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task t5a (5 marks)\n",
    "# Print the classification report and confusion matrix for each of the models above\n",
    "# Write code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task t5b (5 marks)\n",
    "# Each of the model above has some probabilistic interpretation\n",
    "# So sklearn allows you to get the probability values as part of classification\n",
    "# Using this information, you can print roc_curve\n",
    "# See http://nbviewer.ipython.org/github/datadave/GADS9-NYC-Spring2014-Lectures/blob/master/lessons/lesson09_decision_trees_random_forests/sklearn_decision_trees.ipynb\n",
    "# Write code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task t5c (5 marks)\n",
    "# Print the AUC value for each of the models above\n",
    "# Write code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task t5d (5 marks)\n",
    "# Print the precision recall curve for each of the models above\n",
    "# print the curve based on http://scikit-learn.org/stable/auto_examples/plot_precision_recall.html   \n",
    "# Write code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Parameter Tuning through Grid Search/Cross Validation and Parallelization\n",
    "\n",
    "So far in this assignment, you manually tweaked the model till it became better.\n",
    "For complex models, this is often cumbersome. A common trick people use is called Grid Search where you exhaustively test various parameter combinations\n",
    "and pick the best set of parameter values. This is a VERY computationally intensive process and hence it will require some parallelization.\n",
    "\n",
    "In this part, you will learn how to tune Randm Forest for MNIST dataset\n",
    "and then parallelize it so as to get results faster.\n",
    "You might want to take a look at the url\n",
    "http://scikit-learn.org/stable/modules/grid_search.html\n",
    "for additional details.\n",
    "\n",
    "One thing you might want to note is that the GridSearchCV uses cross validation for comparing models.\n",
    "\n",
    "So you have to send the ENTIRE MNIST dataset - i.e. mnist.data and mnist.target. \n",
    "\n",
    "The following cell creates two variables all_scaled_data and all_scaled_target that you can pass to GridSearchCV.\n",
    "\n",
    "In order to get the results in reasonable time, set the **cv** parameter of GridSearchCV to 3.\n",
    "Also remember to set the **verbose** parameter to 2 to get some details about what happens internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Do not make any change below\n",
    "all_scaled_data = binary_class_data / 255.0\n",
    "all_scaled_target = binary_class_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Tuning parameters for Random Forest using grid search (15 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 6 (15 marks)\n",
    "# Tuning Random Forest for MNIST\n",
    "tuned_parameters = [{'max_features': ['sqrt', 'log2'], 'n_estimators': [1000, 1500]}] \n",
    "\n",
    "# Write code here\n",
    "\n",
    "# print the details of the best model and its accuracy\n",
    "# Write code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

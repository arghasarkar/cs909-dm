{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "Dimensionality reduction is a key data pre-processing technique. You will perform PCA, a popular dimensionality reduction technique to MNIST data to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "#Array processing\n",
    "import numpy as np\n",
    "\n",
    "#Data analysis, wrangling and common exploratory operations\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "#For visualization. Matplotlib for basic viz and seaborn for more stylish figures + statistical figures not in MPL.\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.core.display import Image\n",
    "\n",
    "from sklearn.datasets import fetch_mldata                                                                       \n",
    "from sklearn.utils import shuffle                                                                                                                                                                      \n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC, LinearSVC , SVR \n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier                                                       \n",
    "from sklearn.model_selection import KFold, train_test_split, cross_val_score, GridSearchCV                                                \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import pydot, io\n",
    "import time\n",
    "\n",
    "#######################End imports###################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################Do not change anything below\n",
    "#Load MNIST data. fetch_mldata will download the dataset and put it in a folder called mldata. \n",
    "#Some things to be aware of:\n",
    "#   The folder mldata will be created in the folder in which you started the notebook\n",
    "#   So to make your life easy, always start IPython notebook from same folder.\n",
    "#   Else the following code will keep downloading MNIST data\n",
    "mnist = fetch_mldata(\"MNIST original\") \n",
    "                                                                                                             \n",
    "#In order to make the experiments repeatable, we will seed the random number generator to a known value\n",
    "# That way the results of the experiments will always be same\n",
    "np.random.seed(1234)                        \n",
    "\n",
    "# Recall that we previously used shuffle and assigned first 5000 data as training and remaining as testing\n",
    "# Here is another way to do this\n",
    "# Here we are using a function in cross validation module to split \n",
    "# By convention, let us use a 70/30 split\n",
    "train_data, test_data, train_labels, test_labels = \\\n",
    "        train_test_split(mnist.data, mnist.target, test_size=0.3)\n",
    "\n",
    "#The images are in grey scale where each number is between 0 to 255\n",
    "# Now let us normalize them so that the values are between 0 and 1. \n",
    "# This will be the only modification we will make to the image\n",
    "train_data = train_data / 255.0                                        \n",
    "test_data = test_data / 255.0\n",
    "\n",
    "# Plot the average value of all digits\n",
    "plt.figure()\n",
    "fig,axes = plt.subplots(2, 5, figsize=(15,4))\n",
    "\n",
    "for i in range(10):\n",
    "    row, col = i // 5, i%5\n",
    "    \n",
    "    #Change below: Subset p3_train_data with images for digit i only \n",
    "    # Possible to do it 1 liner (similar to how it is done in Pandas)\n",
    "    digit_i_subset = train_data[train_labels == i]\n",
    "\n",
    "    #Change below: compute avg value of t3a_digit_i_subset\n",
    "    # remember to use a vectorized version of mean for efficiency\n",
    "    digit_i_subset_mean = np.mean(digit_i_subset, axis=0)\n",
    "\n",
    "    #Do not change below\n",
    "    axes[row][col].imshow( digit_i_subset_mean.reshape(28, 28), cmap=\"Greys\") \n",
    "    axes[row][col].grid(False)\n",
    "    axes[row][col].get_xaxis().set_ticks([])\n",
    "    axes[row][col].get_yaxis().set_ticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: Dimensionality Reduction (25 marks)\n",
    "\n",
    "- Task 7a: Train a multi-class classifier (OneVsRest) with LinearSVC class and make predictions and print the training time and classification accuracy on the test set. (5 marks)\n",
    "\n",
    "- Task 7b: Perform PCA with 100 components on the training data, map both training and test data into 100-dimensional space by PCA, train a multi-class classifier (OneVsRest) with LinearSVC class using the trainformed training data, make predictions and print the training time and the classification accuracy on the test set. (10 marks)\n",
    "\n",
    "- Task 7c: One way to determine how much components needs for PCA is to find the smallest value such that it explained 95% of the variance. Using the PCA results obtained above, print the cumulative variance that is explained by 100 components. (10 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task t7a (5 marks)\n",
    "# Train a multi-class classifier (OneVsRest) with LinearSVC class and make predictions\n",
    "# Print the training time and classification accuracy on the test set\n",
    "# Write your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task t7b (10 marks)\n",
    "# Perform PCA on the training data and map both training and test data into 100-dimensional space by PCA, \n",
    "# Train a multi-class classifier (OneVsRest) with LinearSVC class using the trainformed training data,\n",
    "# Print the training time and classification accuracy on the test set\n",
    "# Remember that MNIST images are 28x28 => 784 features.\n",
    "\n",
    "# You might want to check http://scikit-learn.org/stable/modules/decomposition.html#decompositions for details\n",
    "# Write your code below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task t7c (10 marks)\n",
    "\n",
    "# One way to determine how much components needs for PCA is to find the smallest value \n",
    "# such that it explained 95% of the variance. \n",
    "# Using the PCA results obtained above, print the cumulative variance that is explained \n",
    "# by 100 components.\n",
    "# Write your code below"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

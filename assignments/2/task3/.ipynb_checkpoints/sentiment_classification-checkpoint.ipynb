{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Classification: classifying IMDB reviews\n",
    "\n",
    "In this task, you will learn how to process text data and how to train neural networks with limited input text data using pre-trained embeddings for sentiment classification (classifying a review document as \"positive\" or \"negative\" based solely on the text content of the review)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `Embedding` layer in Keras to represent text input. The `Embedding` layer is best understood as a dictionary mapping integer indices (which stand for specific words) to dense vectors. It takes as input integers, then looks up these integers into an internal dictionary, and finally returns the associated vectors. It's effectively a dictionary lookup.\n",
    "\n",
    "The `Embedding` layer takes as input a 2D tensor of integers, of shape `(samples, sequence_length)`, where each entry is a sequence of integers. It can embed sequences of variable lengths, so for instance we could feed into our embedding layer above batches that could have  shapes `(32, 10)` (batch of 32 sequences of length 10) or `(64, 15)` (batch of 64 sequences of length 15). All sequences in a batch must have the same length, though (since we need to pack them into a single tensor), so sequences that are shorter than others should be padded with zeros, and sequences that are longer should be truncated.\n",
    "\n",
    "This layer returns a 3D floating point tensor, of shape `(samples, sequence_length, embedding_dimensionality)`. Such a 3D tensor can then be processed by a RNN layer or a 1D convolution layer.\n",
    "\n",
    "You can instantiate the `Embedding` layer by randomly initialising its weights (its internal dictionary of token vectors). During training, these word vectors will be gradually adjusted via backpropagation, structuring the space into something that the downstream model can exploit. Once fully trained, your embedding space will show a lot of structure -- a kind of structure specialized for the specific problem you were training your model for. You can also instantiate the `Embedding` layer by intialising its weights using the pre-trained word embeddings, such as GloVe word embeddings pretrained from Wikipedia articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Download the IMDB data as raw text\n",
    "\n",
    "First, create a \"data\" directory, then head to `http://ai.stanford.edu/~amaas/data/sentiment/` and download the raw IMDB dataset (if the URL isn't working anymore, just Google \"IMDB dataset\"). Save it into the \"data\" directory. Uncompress it. Store the individual reviews into a list of strings, one string per review, and also collect the review labels (positive / negative) into a separate `labels` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/train/pos\n",
      "./data/test/neg\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "\n",
    "# write your code here\n",
    "\n",
    "# Numpy random seed\n",
    "SEED = 200\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_DIR = './data'\n",
    "TRAIN_DIR = DATA_DIR+ \"/\" +\"train\"\n",
    "TEST_DIR = DATA_DIR+ \"/\" +\"test\"\n",
    "\n",
    "POS_DIR = \"pos\"\n",
    "NEG_DIR = \"neg\"\n",
    "\n",
    "TRAIN_POS_DIR = TRAIN_DIR + \"/\" + POS_DIR\n",
    "TRAIN_NEG_DIR = TRAIN_DIR + \"/\" + NEG_DIR\n",
    "\n",
    "TEST_POS_DIR = TEST_DIR + \"/\" + POS_DIR\n",
    "TEST_NEG_DIR = TEST_DIR+ \"/\" + NEG_DIR\n",
    "\n",
    "print(TRAIN_POS_DIR)\n",
    "print(TEST_NEG_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_review(file_path):\n",
    "    \n",
    "    f = open(file_path, \"r\", encoding=\"utf8\")\n",
    "    return f.read()\n",
    "\n",
    "\n",
    "def get_file_paths_in_dir(dir_path):\n",
    "    \n",
    "    return [ dir_path + \"/\" + file_path for file_path in os.listdir(dir_path) ]\n",
    "\n",
    "\n",
    "def read_all_reviews_in_dir(dir_path):\n",
    "    \n",
    "    _reviews = []\n",
    "    file_paths = get_file_paths_in_dir(dir_path)\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        _reviews.append(read_review(file_path))\n",
    "    \n",
    "    return _reviews\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reviews = []\n",
    "labels = []\n",
    "\n",
    "rev_train_pos = read_all_reviews_in_dir(TRAIN_POS_DIR)\n",
    "rev_train_neg = read_all_reviews_in_dir(TRAIN_NEG_DIR)\n",
    "\n",
    "for i in range(len(rev_train_pos)):\n",
    "    labels.append(1) #1: positive 0: negative\n",
    "\n",
    "for i in range(len(rev_train_neg)):\n",
    "    labels.append(0)\n",
    "    \n",
    "reviews.extend(rev_train_pos)\n",
    "reviews.extend(rev_train_neg)\n",
    "\n",
    "\n",
    "\n",
    "test_reviews = []\n",
    "test_labels = []\n",
    "\n",
    "rev_test_pos = read_all_reviews_in_dir(TEST_POS_DIR)\n",
    "rev_test_neg = read_all_reviews_in_dir(TEST_NEG_DIR)\n",
    "\n",
    "test_reviews.extend(rev_test_pos)\n",
    "test_reviews.extend(rev_test_neg)\n",
    "\n",
    "for i in range(len(rev_test_pos)):\n",
    "    test_labels.append(1) #1: positive 0: negative\n",
    "\n",
    "for i in range(len(rev_test_neg)):\n",
    "    test_labels.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "25000\n",
      "25000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(len(reviews))\n",
    "print(len(labels))\n",
    "print(len(test_reviews))\n",
    "print(len(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Pre-process the review documents \n",
    "\n",
    "Pre-process review documents by tokenisation and split the data into the training and testing sets. You can restrict the training data to the first 1000 reviews and only consider the top 5,000 words in the dataset. You can also cut reviews after 100 words (that is, each review contains a maximum of 100 words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Applications\\Python3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TRAIN_SIZE = 1000 / len(reviews)\n",
    "\n",
    "\n",
    "print(TRAIN_SIZE)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(reviews, labels, test_size=1 - TRAIN_SIZE, random_state=SEED)\n",
    "\n",
    "x_val = x_val[:1000]\n",
    "y_val = y_val[:1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_WORD_COUNT = 5000\n",
    "SENT_LEN = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=TOP_WORD_COUNT+1, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ')\n",
    "\n",
    "tokenizer.fit_on_texts(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train_tokenized = tokenizer.texts_to_sequences(x_train)\n",
    "x_test_tokenized = tokenizer.texts_to_sequences(test_reviews)\n",
    "x_val_tokenized = tokenizer.texts_to_sequences(x_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train_cut = pad_sequences(x_train_tokenized, maxlen=SENT_LEN)\n",
    "x_test_cut = pad_sequences(x_test_tokenized, maxlen=SENT_LEN)\n",
    "x_val_cut = pad_sequences(x_val_tokenized, maxlen=SENT_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "25000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train_cut))\n",
    "print(len(x_test_cut))\n",
    "print(len(x_val_cut))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19171"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(tokenizer.word_index.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Download the GloVe word embeddings and map each word in the dataset into its pre-trained GloVe word embedding.\n",
    "\n",
    "First go to `https://nlp.stanford.edu/projects/glove/` and download the pre-trained \n",
    "embeddings from 2014 English Wikipedia into the \"data\" directory. It's a 822MB zip file named `glove.6B.zip`, containing 100-dimensional embedding vectors for \n",
    "400,000 words (or non-word tokens). Un-zip it.\n",
    "\n",
    "Parse the un-zipped file (it's a `txt` file) to build an index mapping words (as strings) to their vector representation (as number vectors).\n",
    "\n",
    "Build an embedding matrix that will be loaded into an `Embedding` layer later. It must be a matrix of shape `(max_words, embedding_dim)`, where each entry `i` contains the `embedding_dim`-dimensional vector for the word of index `i` in our reference word index \n",
    "(built during tokenization). Note that the index `0` is not supposed to stand for any word or token -- it's a placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19171"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write your code here\n",
    "len(list(tokenizer.word_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GLOVE_FILE_PATH = \"./glove.6B.100d.txt\"\n",
    "\n",
    "def load_glove(file_path):\n",
    "    \n",
    "    embeddings_index = {}\n",
    "    f = open(file_path, \"r\", encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    \n",
    "    return embeddings_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word_emb = load_glove(GLOVE_FILE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = word_emb.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19172, 100)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_to_two_labels(label_arr):\n",
    "    \n",
    "    _lab_arr = []\n",
    "    for i in range(len(label_arr)):\n",
    "        if label_arr[i] == 0:\n",
    "            _lab_arr.append(np.array([0, 1]))\n",
    "        else:\n",
    "            _lab_arr.append(np.array([1, 0]))\n",
    "    \n",
    "    return np.array(_lab_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = convert_to_two_labels(y_train)\n",
    "y_val = convert_to_two_labels(y_val)\n",
    "y_test = convert_to_two_labels(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Build and train a simple Sequential model\n",
    "\n",
    "The model contains an Embedding Layer with maximum number of tokens to be 10,000 and embedding dimensionality as 100. Initialise the Embedding Layer with the pre-trained GloVe word vectors. Set the maximum length of each review to 100. Flatten the 3D embedding output to 2D and add a Dense Layer which is the classifier. Train the model with a 'rmsprop' optimiser. You need to freeze the embedding layer by setting its `trainable` attribute to `False` so that its weights will not be updated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.layers.embeddings.Embedding object at 0x0000027786EAACC0>\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense, Dropout, LSTM\n",
    "\n",
    "\n",
    "# write your code here\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=SENT_LEN,\n",
    "                            trainable=False)\n",
    "\n",
    "print(embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(embedding_layer)\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.00001)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 1000 samples\n",
      "Epoch 1/600\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.8337 - acc: 0.4920 - val_loss: 0.7779 - val_acc: 0.5290\n",
      "Epoch 2/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.8008 - acc: 0.4910 - val_loss: 0.7605 - val_acc: 0.5280\n",
      "Epoch 3/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.7840 - acc: 0.4950 - val_loss: 0.7499 - val_acc: 0.5220\n",
      "Epoch 4/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.7732 - acc: 0.5200 - val_loss: 0.7430 - val_acc: 0.5310\n",
      "Epoch 5/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.7660 - acc: 0.5230 - val_loss: 0.7386 - val_acc: 0.5310\n",
      "Epoch 6/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.7606 - acc: 0.5240 - val_loss: 0.7355 - val_acc: 0.5350\n",
      "Epoch 7/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.7567 - acc: 0.5210 - val_loss: 0.7333 - val_acc: 0.5350\n",
      "Epoch 8/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.7537 - acc: 0.5290 - val_loss: 0.7319 - val_acc: 0.5370\n",
      "Epoch 9/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.7512 - acc: 0.5240 - val_loss: 0.7310 - val_acc: 0.5380\n",
      "Epoch 10/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.7492 - acc: 0.5280 - val_loss: 0.7303 - val_acc: 0.5330\n",
      "Epoch 11/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.7475 - acc: 0.5320 - val_loss: 0.7299 - val_acc: 0.5340\n",
      "Epoch 12/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.7458 - acc: 0.5310 - val_loss: 0.7295 - val_acc: 0.5370\n",
      "Epoch 13/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.7447 - acc: 0.5330 - val_loss: 0.7292 - val_acc: 0.5390\n",
      "Epoch 14/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.7431 - acc: 0.5330 - val_loss: 0.7289 - val_acc: 0.5390\n",
      "Epoch 15/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.7415 - acc: 0.5370 - val_loss: 0.7286 - val_acc: 0.5430\n",
      "Epoch 16/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.7409 - acc: 0.5350 - val_loss: 0.7283 - val_acc: 0.5430\n",
      "Epoch 17/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.7386 - acc: 0.5370 - val_loss: 0.7280 - val_acc: 0.5460\n",
      "Epoch 18/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.7373 - acc: 0.5410 - val_loss: 0.7277 - val_acc: 0.5450\n",
      "Epoch 19/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.7358 - acc: 0.5440 - val_loss: 0.7274 - val_acc: 0.5500\n",
      "Epoch 20/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.7343 - acc: 0.5450 - val_loss: 0.7271 - val_acc: 0.5500\n",
      "Epoch 21/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.7330 - acc: 0.5470 - val_loss: 0.7268 - val_acc: 0.5470\n",
      "Epoch 22/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.7313 - acc: 0.5500 - val_loss: 0.7265 - val_acc: 0.5490\n",
      "Epoch 23/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.7301 - acc: 0.5520 - val_loss: 0.7261 - val_acc: 0.5480\n",
      "Epoch 24/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.7284 - acc: 0.5550 - val_loss: 0.7258 - val_acc: 0.5480\n",
      "Epoch 25/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.7268 - acc: 0.5550 - val_loss: 0.7255 - val_acc: 0.5480\n",
      "Epoch 26/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.7253 - acc: 0.5570 - val_loss: 0.7252 - val_acc: 0.5480\n",
      "Epoch 27/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.7237 - acc: 0.5610 - val_loss: 0.7249 - val_acc: 0.5480\n",
      "Epoch 28/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.7225 - acc: 0.5600 - val_loss: 0.7246 - val_acc: 0.5490\n",
      "Epoch 29/600\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.7207 - acc: 0.5610 - val_loss: 0.7242 - val_acc: 0.5470\n",
      "Epoch 30/600\n",
      "1000/1000 [==============================] - 0s 16us/step - loss: 0.7193 - acc: 0.5630 - val_loss: 0.7240 - val_acc: 0.5490\n",
      "Epoch 31/600\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.7176 - acc: 0.5630 - val_loss: 0.7236 - val_acc: 0.5490\n",
      "Epoch 32/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.7161 - acc: 0.5650 - val_loss: 0.7233 - val_acc: 0.5490\n",
      "Epoch 33/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.7145 - acc: 0.5670 - val_loss: 0.7230 - val_acc: 0.5480\n",
      "Epoch 34/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.7137 - acc: 0.5680 - val_loss: 0.7226 - val_acc: 0.5500\n",
      "Epoch 35/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.7115 - acc: 0.5680 - val_loss: 0.7223 - val_acc: 0.5510\n",
      "Epoch 36/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.7100 - acc: 0.5700 - val_loss: 0.7221 - val_acc: 0.5540\n",
      "Epoch 37/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.7088 - acc: 0.5710 - val_loss: 0.7217 - val_acc: 0.5520\n",
      "Epoch 38/600\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.7070 - acc: 0.5720 - val_loss: 0.7215 - val_acc: 0.5530\n",
      "Epoch 39/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.7055 - acc: 0.5750 - val_loss: 0.7211 - val_acc: 0.5520\n",
      "Epoch 40/600\n",
      "1000/1000 [==============================] - 0s 16us/step - loss: 0.7039 - acc: 0.5740 - val_loss: 0.7209 - val_acc: 0.5560\n",
      "Epoch 41/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.7028 - acc: 0.5800 - val_loss: 0.7206 - val_acc: 0.5550\n",
      "Epoch 42/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.7012 - acc: 0.5820 - val_loss: 0.7203 - val_acc: 0.5520\n",
      "Epoch 43/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.6998 - acc: 0.5820 - val_loss: 0.7200 - val_acc: 0.5520\n",
      "Epoch 44/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.6980 - acc: 0.5810 - val_loss: 0.7197 - val_acc: 0.5540\n",
      "Epoch 45/600\n",
      "1000/1000 [==============================] - 0s 16us/step - loss: 0.6966 - acc: 0.5840 - val_loss: 0.7194 - val_acc: 0.5550\n",
      "Epoch 46/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.6950 - acc: 0.5850 - val_loss: 0.7192 - val_acc: 0.5560\n",
      "Epoch 47/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.6936 - acc: 0.5900 - val_loss: 0.7189 - val_acc: 0.5570\n",
      "Epoch 48/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.6922 - acc: 0.5880 - val_loss: 0.7187 - val_acc: 0.5600\n",
      "Epoch 49/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.6906 - acc: 0.5900 - val_loss: 0.7184 - val_acc: 0.5570\n",
      "Epoch 50/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.6896 - acc: 0.5930 - val_loss: 0.7180 - val_acc: 0.5550\n",
      "Epoch 51/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.6877 - acc: 0.5940 - val_loss: 0.7178 - val_acc: 0.5540\n",
      "Epoch 52/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.6863 - acc: 0.5950 - val_loss: 0.7175 - val_acc: 0.5570\n",
      "Epoch 53/600\n",
      "1000/1000 [==============================] - 0s 15us/step - loss: 0.6847 - acc: 0.5960 - val_loss: 0.7173 - val_acc: 0.5550\n",
      "Epoch 54/600\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.6833 - acc: 0.5980 - val_loss: 0.7170 - val_acc: 0.5540\n",
      "Epoch 55/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.6818 - acc: 0.5980 - val_loss: 0.7167 - val_acc: 0.5560\n",
      "Epoch 56/600\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.6807 - acc: 0.5990 - val_loss: 0.7166 - val_acc: 0.5550\n",
      "Epoch 57/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.6805 - acc: 0.5950 - val_loss: 0.7164 - val_acc: 0.5610\n",
      "Epoch 58/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.6776 - acc: 0.6030 - val_loss: 0.7161 - val_acc: 0.5570\n",
      "Epoch 59/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.6762 - acc: 0.6050 - val_loss: 0.7158 - val_acc: 0.5590\n",
      "Epoch 60/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.6748 - acc: 0.6070 - val_loss: 0.7156 - val_acc: 0.5590\n",
      "Epoch 61/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.6737 - acc: 0.6090 - val_loss: 0.7153 - val_acc: 0.5540\n",
      "Epoch 62/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.6721 - acc: 0.6110 - val_loss: 0.7151 - val_acc: 0.5570\n",
      "Epoch 63/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.6709 - acc: 0.6090 - val_loss: 0.7149 - val_acc: 0.5590\n",
      "Epoch 64/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.6698 - acc: 0.6150 - val_loss: 0.7146 - val_acc: 0.5580\n",
      "Epoch 65/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.6679 - acc: 0.6120 - val_loss: 0.7144 - val_acc: 0.5580\n",
      "Epoch 66/600\n",
      "1000/1000 [==============================] - 0s 16us/step - loss: 0.6667 - acc: 0.6130 - val_loss: 0.7142 - val_acc: 0.5590\n",
      "Epoch 67/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.6653 - acc: 0.6150 - val_loss: 0.7139 - val_acc: 0.5580\n",
      "Epoch 68/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.6638 - acc: 0.6150 - val_loss: 0.7137 - val_acc: 0.5600\n",
      "Epoch 69/600\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.6624 - acc: 0.6200 - val_loss: 0.7135 - val_acc: 0.5600\n",
      "Epoch 70/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.6611 - acc: 0.6210 - val_loss: 0.7133 - val_acc: 0.5590\n",
      "Epoch 71/600\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.6598 - acc: 0.6200 - val_loss: 0.7131 - val_acc: 0.5590\n",
      "Epoch 72/600\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.6582 - acc: 0.6220 - val_loss: 0.7129 - val_acc: 0.5580\n",
      "Epoch 73/600\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.6568 - acc: 0.6250 - val_loss: 0.7126 - val_acc: 0.5580\n",
      "Epoch 74/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.6554 - acc: 0.6250 - val_loss: 0.7124 - val_acc: 0.5580\n",
      "Epoch 75/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.6552 - acc: 0.6260 - val_loss: 0.7123 - val_acc: 0.5620\n",
      "Epoch 76/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.6531 - acc: 0.6260 - val_loss: 0.7121 - val_acc: 0.5630\n",
      "Epoch 77/600\n",
      "1000/1000 [==============================] - 0s 16us/step - loss: 0.6514 - acc: 0.6280 - val_loss: 0.7118 - val_acc: 0.5620\n",
      "Epoch 78/600\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 0.6501 - acc: 0.6290 - val_loss: 0.7116 - val_acc: 0.5620\n",
      "Epoch 79/600\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.6487 - acc: 0.6300 - val_loss: 0.7114 - val_acc: 0.5610\n",
      "Epoch 80/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.6474 - acc: 0.6300 - val_loss: 0.7112 - val_acc: 0.5600\n",
      "Epoch 81/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.6460 - acc: 0.6310 - val_loss: 0.7109 - val_acc: 0.5610\n",
      "Epoch 82/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.6447 - acc: 0.6320 - val_loss: 0.7108 - val_acc: 0.5630\n",
      "Epoch 83/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.6433 - acc: 0.6350 - val_loss: 0.7105 - val_acc: 0.5620\n",
      "Epoch 84/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.6419 - acc: 0.6330 - val_loss: 0.7103 - val_acc: 0.5620\n",
      "Epoch 85/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.6413 - acc: 0.6330 - val_loss: 0.7102 - val_acc: 0.5610\n",
      "Epoch 86/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.6394 - acc: 0.6400 - val_loss: 0.7099 - val_acc: 0.5620\n",
      "Epoch 87/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.6380 - acc: 0.6380 - val_loss: 0.7098 - val_acc: 0.5650\n",
      "Epoch 88/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.6368 - acc: 0.6420 - val_loss: 0.7095 - val_acc: 0.5630\n",
      "Epoch 89/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.6358 - acc: 0.6410 - val_loss: 0.7094 - val_acc: 0.5650\n",
      "Epoch 90/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.6340 - acc: 0.6440 - val_loss: 0.7092 - val_acc: 0.5670\n",
      "Epoch 91/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.6327 - acc: 0.6450 - val_loss: 0.7090 - val_acc: 0.5670\n",
      "Epoch 92/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.6315 - acc: 0.6500 - val_loss: 0.7088 - val_acc: 0.5630\n",
      "Epoch 93/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.6302 - acc: 0.6490 - val_loss: 0.7087 - val_acc: 0.5660\n",
      "Epoch 94/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.6287 - acc: 0.6530 - val_loss: 0.7085 - val_acc: 0.5660\n",
      "Epoch 95/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.6280 - acc: 0.6450 - val_loss: 0.7084 - val_acc: 0.5640\n",
      "Epoch 96/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.6271 - acc: 0.6550 - val_loss: 0.7082 - val_acc: 0.5640\n",
      "Epoch 97/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.6250 - acc: 0.6540 - val_loss: 0.7079 - val_acc: 0.5660\n",
      "Epoch 98/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.6242 - acc: 0.6620 - val_loss: 0.7078 - val_acc: 0.5640\n",
      "Epoch 99/600\n",
      "1000/1000 [==============================] - 0s 22us/step - loss: 0.6224 - acc: 0.6600 - val_loss: 0.7076 - val_acc: 0.5650\n",
      "Epoch 100/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.6212 - acc: 0.6640 - val_loss: 0.7074 - val_acc: 0.5670\n",
      "Epoch 101/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.6199 - acc: 0.6630 - val_loss: 0.7072 - val_acc: 0.5670\n",
      "Epoch 102/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.6187 - acc: 0.6650 - val_loss: 0.7070 - val_acc: 0.5700\n",
      "Epoch 103/600\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.6282 - acc: 0.654 - 0s 18us/step - loss: 0.6175 - acc: 0.6660 - val_loss: 0.7069 - val_acc: 0.5630\n",
      "Epoch 104/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.6164 - acc: 0.6700 - val_loss: 0.7068 - val_acc: 0.5640\n",
      "Epoch 105/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.6150 - acc: 0.6670 - val_loss: 0.7065 - val_acc: 0.5690\n",
      "Epoch 106/600\n",
      "1000/1000 [==============================] - 0s 22us/step - loss: 0.6137 - acc: 0.6700 - val_loss: 0.7064 - val_acc: 0.5650\n",
      "Epoch 107/600\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.6123 - acc: 0.6690 - val_loss: 0.7062 - val_acc: 0.5660\n",
      "Epoch 108/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.6114 - acc: 0.6700 - val_loss: 0.7060 - val_acc: 0.5690\n",
      "Epoch 109/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.6098 - acc: 0.6750 - val_loss: 0.7059 - val_acc: 0.5670\n",
      "Epoch 110/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.6086 - acc: 0.6730 - val_loss: 0.7057 - val_acc: 0.5670\n",
      "Epoch 111/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.6074 - acc: 0.6780 - val_loss: 0.7056 - val_acc: 0.5650\n",
      "Epoch 112/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.6060 - acc: 0.6740 - val_loss: 0.7054 - val_acc: 0.5680\n",
      "Epoch 113/600\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.6049 - acc: 0.6770 - val_loss: 0.7052 - val_acc: 0.5670\n",
      "Epoch 114/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.6039 - acc: 0.6780 - val_loss: 0.7051 - val_acc: 0.5670\n",
      "Epoch 115/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.6028 - acc: 0.6720 - val_loss: 0.7049 - val_acc: 0.5670\n",
      "Epoch 116/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.6017 - acc: 0.6820 - val_loss: 0.7048 - val_acc: 0.5670\n",
      "Epoch 117/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.6003 - acc: 0.6790 - val_loss: 0.7047 - val_acc: 0.5680\n",
      "Epoch 118/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.5987 - acc: 0.6790 - val_loss: 0.7045 - val_acc: 0.5680\n",
      "Epoch 119/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.5977 - acc: 0.6820 - val_loss: 0.7043 - val_acc: 0.5690\n",
      "Epoch 120/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.5964 - acc: 0.6850 - val_loss: 0.7042 - val_acc: 0.5680\n",
      "Epoch 121/600\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.5956 - acc: 0.6850 - val_loss: 0.7041 - val_acc: 0.5670\n",
      "Epoch 122/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.5940 - acc: 0.6880 - val_loss: 0.7039 - val_acc: 0.5700\n",
      "Epoch 123/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.5929 - acc: 0.6870 - val_loss: 0.7038 - val_acc: 0.5720\n",
      "Epoch 124/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.5916 - acc: 0.6900 - val_loss: 0.7036 - val_acc: 0.5720\n",
      "Epoch 125/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.5909 - acc: 0.6880 - val_loss: 0.7035 - val_acc: 0.5730\n",
      "Epoch 126/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.5893 - acc: 0.6900 - val_loss: 0.7034 - val_acc: 0.5730\n",
      "Epoch 127/600\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 0.5881 - acc: 0.6900 - val_loss: 0.7033 - val_acc: 0.5700\n",
      "Epoch 128/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.5868 - acc: 0.6920 - val_loss: 0.7031 - val_acc: 0.5710\n",
      "Epoch 129/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.5857 - acc: 0.6920 - val_loss: 0.7030 - val_acc: 0.5690\n",
      "Epoch 130/600\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.5878 - acc: 0.707 - 0s 18us/step - loss: 0.5846 - acc: 0.6930 - val_loss: 0.7029 - val_acc: 0.5690\n",
      "Epoch 131/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.5832 - acc: 0.6920 - val_loss: 0.7027 - val_acc: 0.5710\n",
      "Epoch 132/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.5824 - acc: 0.6940 - val_loss: 0.7026 - val_acc: 0.5680\n",
      "Epoch 133/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.5808 - acc: 0.6930 - val_loss: 0.7024 - val_acc: 0.5720\n",
      "Epoch 134/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.5796 - acc: 0.6950 - val_loss: 0.7023 - val_acc: 0.5700\n",
      "Epoch 135/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.5784 - acc: 0.6930 - val_loss: 0.7021 - val_acc: 0.5740\n",
      "Epoch 136/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.5772 - acc: 0.6940 - val_loss: 0.7020 - val_acc: 0.5760\n",
      "Epoch 137/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.5760 - acc: 0.6970 - val_loss: 0.7019 - val_acc: 0.5730\n",
      "Epoch 138/600\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.5736 - acc: 0.691 - 0s 18us/step - loss: 0.5747 - acc: 0.6970 - val_loss: 0.7017 - val_acc: 0.5760\n",
      "Epoch 139/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.5735 - acc: 0.7020 - val_loss: 0.7016 - val_acc: 0.5760\n",
      "Epoch 140/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.5723 - acc: 0.7030 - val_loss: 0.7014 - val_acc: 0.5770\n",
      "Epoch 141/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.5711 - acc: 0.7010 - val_loss: 0.7013 - val_acc: 0.5760\n",
      "Epoch 142/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.5698 - acc: 0.7040 - val_loss: 0.7012 - val_acc: 0.5770\n",
      "Epoch 143/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.5686 - acc: 0.7040 - val_loss: 0.7011 - val_acc: 0.5770\n",
      "Epoch 144/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.5675 - acc: 0.7070 - val_loss: 0.7010 - val_acc: 0.5730\n",
      "Epoch 145/600\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.5662 - acc: 0.7070 - val_loss: 0.7008 - val_acc: 0.5730\n",
      "Epoch 146/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.5655 - acc: 0.7100 - val_loss: 0.7006 - val_acc: 0.5760\n",
      "Epoch 147/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.5645 - acc: 0.7100 - val_loss: 0.7005 - val_acc: 0.5770\n",
      "Epoch 148/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.5627 - acc: 0.7130 - val_loss: 0.7004 - val_acc: 0.5750\n",
      "Epoch 149/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.5616 - acc: 0.7130 - val_loss: 0.7004 - val_acc: 0.5760\n",
      "Epoch 150/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.5604 - acc: 0.7150 - val_loss: 0.7002 - val_acc: 0.5750\n",
      "Epoch 151/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.5592 - acc: 0.7160 - val_loss: 0.7001 - val_acc: 0.5740\n",
      "Epoch 152/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.5581 - acc: 0.7160 - val_loss: 0.7000 - val_acc: 0.5740\n",
      "Epoch 153/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.5569 - acc: 0.7160 - val_loss: 0.6999 - val_acc: 0.5740\n",
      "Epoch 154/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.5558 - acc: 0.7190 - val_loss: 0.6997 - val_acc: 0.5730\n",
      "Epoch 155/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.5550 - acc: 0.7200 - val_loss: 0.6996 - val_acc: 0.5780\n",
      "Epoch 156/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.5539 - acc: 0.7260 - val_loss: 0.6995 - val_acc: 0.5790\n",
      "Epoch 157/600\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.5524 - acc: 0.7220 - val_loss: 0.6994 - val_acc: 0.5760\n",
      "Epoch 158/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.5514 - acc: 0.7210 - val_loss: 0.6993 - val_acc: 0.5720\n",
      "Epoch 159/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.5503 - acc: 0.7240 - val_loss: 0.6993 - val_acc: 0.5730\n",
      "Epoch 160/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.5490 - acc: 0.7280 - val_loss: 0.6991 - val_acc: 0.5720\n",
      "Epoch 161/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.5478 - acc: 0.7310 - val_loss: 0.6990 - val_acc: 0.5740\n",
      "Epoch 162/600\n",
      "1000/1000 [==============================] - 0s 15us/step - loss: 0.5471 - acc: 0.7300 - val_loss: 0.6988 - val_acc: 0.5780\n",
      "Epoch 163/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.5456 - acc: 0.7270 - val_loss: 0.6988 - val_acc: 0.5750\n",
      "Epoch 164/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.5445 - acc: 0.7310 - val_loss: 0.6987 - val_acc: 0.5740\n",
      "Epoch 165/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.5435 - acc: 0.7320 - val_loss: 0.6987 - val_acc: 0.5740\n",
      "Epoch 166/600\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.5424 - acc: 0.7370 - val_loss: 0.6985 - val_acc: 0.5770\n",
      "Epoch 167/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.5411 - acc: 0.7380 - val_loss: 0.6984 - val_acc: 0.5780\n",
      "Epoch 168/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.5404 - acc: 0.7390 - val_loss: 0.6982 - val_acc: 0.5770\n",
      "Epoch 169/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.5389 - acc: 0.7330 - val_loss: 0.6982 - val_acc: 0.5790\n",
      "Epoch 170/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.5378 - acc: 0.7400 - val_loss: 0.6981 - val_acc: 0.5790\n",
      "Epoch 171/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.5368 - acc: 0.7410 - val_loss: 0.6980 - val_acc: 0.5770\n",
      "Epoch 172/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.5361 - acc: 0.7450 - val_loss: 0.6979 - val_acc: 0.5760\n",
      "Epoch 173/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.5346 - acc: 0.7410 - val_loss: 0.6978 - val_acc: 0.5790\n",
      "Epoch 174/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.5339 - acc: 0.7410 - val_loss: 0.6978 - val_acc: 0.5800\n",
      "Epoch 175/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.5325 - acc: 0.7520 - val_loss: 0.6976 - val_acc: 0.5780\n",
      "Epoch 176/600\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.5171 - acc: 0.763 - 0s 29us/step - loss: 0.5313 - acc: 0.7500 - val_loss: 0.6975 - val_acc: 0.5770\n",
      "Epoch 177/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.5307 - acc: 0.7510 - val_loss: 0.6974 - val_acc: 0.5760\n",
      "Epoch 178/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.5292 - acc: 0.7510 - val_loss: 0.6974 - val_acc: 0.5780\n",
      "Epoch 179/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.5290 - acc: 0.7570 - val_loss: 0.6972 - val_acc: 0.5770\n",
      "Epoch 180/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.5274 - acc: 0.7570 - val_loss: 0.6971 - val_acc: 0.5760\n",
      "Epoch 181/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.5262 - acc: 0.7550 - val_loss: 0.6971 - val_acc: 0.5780\n",
      "Epoch 182/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.5251 - acc: 0.7570 - val_loss: 0.6970 - val_acc: 0.5770\n",
      "Epoch 183/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.5241 - acc: 0.7620 - val_loss: 0.6969 - val_acc: 0.5790\n",
      "Epoch 184/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.5230 - acc: 0.7630 - val_loss: 0.6968 - val_acc: 0.5790\n",
      "Epoch 185/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.5221 - acc: 0.7590 - val_loss: 0.6968 - val_acc: 0.5780\n",
      "Epoch 186/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.5210 - acc: 0.7690 - val_loss: 0.6967 - val_acc: 0.5790\n",
      "Epoch 187/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.5199 - acc: 0.7710 - val_loss: 0.6966 - val_acc: 0.5770\n",
      "Epoch 188/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.5188 - acc: 0.7750 - val_loss: 0.6965 - val_acc: 0.5760\n",
      "Epoch 189/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.5181 - acc: 0.7770 - val_loss: 0.6964 - val_acc: 0.5790\n",
      "Epoch 190/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.5167 - acc: 0.7690 - val_loss: 0.6963 - val_acc: 0.5780\n",
      "Epoch 191/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.5156 - acc: 0.7760 - val_loss: 0.6963 - val_acc: 0.5810\n",
      "Epoch 192/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.5146 - acc: 0.7800 - val_loss: 0.6962 - val_acc: 0.5810\n",
      "Epoch 193/600\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.5135 - acc: 0.7800 - val_loss: 0.6961 - val_acc: 0.5770\n",
      "Epoch 194/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.5125 - acc: 0.7800 - val_loss: 0.6961 - val_acc: 0.5820\n",
      "Epoch 195/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.5114 - acc: 0.7840 - val_loss: 0.6960 - val_acc: 0.5790\n",
      "Epoch 196/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.5105 - acc: 0.7830 - val_loss: 0.6960 - val_acc: 0.5840\n",
      "Epoch 197/600\n",
      "1000/1000 [==============================] - 0s 22us/step - loss: 0.5096 - acc: 0.7780 - val_loss: 0.6959 - val_acc: 0.5830\n",
      "Epoch 198/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.5102 - acc: 0.7780 - val_loss: 0.6958 - val_acc: 0.5790\n",
      "Epoch 199/600\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.5073 - acc: 0.7840 - val_loss: 0.6957 - val_acc: 0.5850\n",
      "Epoch 200/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.5064 - acc: 0.7850 - val_loss: 0.6956 - val_acc: 0.5780\n",
      "Epoch 201/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.5054 - acc: 0.7860 - val_loss: 0.6956 - val_acc: 0.5800\n",
      "Epoch 202/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.5045 - acc: 0.7900 - val_loss: 0.6955 - val_acc: 0.5790\n",
      "Epoch 203/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.5035 - acc: 0.7890 - val_loss: 0.6954 - val_acc: 0.5800\n",
      "Epoch 204/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.5026 - acc: 0.7920 - val_loss: 0.6954 - val_acc: 0.5830\n",
      "Epoch 205/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.5015 - acc: 0.7930 - val_loss: 0.6953 - val_acc: 0.5800\n",
      "Epoch 206/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.5006 - acc: 0.7920 - val_loss: 0.6953 - val_acc: 0.5810\n",
      "Epoch 207/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.4995 - acc: 0.7930 - val_loss: 0.6951 - val_acc: 0.5810\n",
      "Epoch 208/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.4985 - acc: 0.7910 - val_loss: 0.6951 - val_acc: 0.5820\n",
      "Epoch 209/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.4979 - acc: 0.7950 - val_loss: 0.6950 - val_acc: 0.5780\n",
      "Epoch 210/600\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.4967 - acc: 0.7940 - val_loss: 0.6949 - val_acc: 0.5790\n",
      "Epoch 211/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.4957 - acc: 0.7970 - val_loss: 0.6949 - val_acc: 0.5800\n",
      "Epoch 212/600\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.4947 - acc: 0.7970 - val_loss: 0.6948 - val_acc: 0.5810\n",
      "Epoch 213/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.4937 - acc: 0.7970 - val_loss: 0.6948 - val_acc: 0.5800\n",
      "Epoch 214/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.4926 - acc: 0.8010 - val_loss: 0.6947 - val_acc: 0.5820\n",
      "Epoch 215/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.4917 - acc: 0.8040 - val_loss: 0.6946 - val_acc: 0.5810\n",
      "Epoch 216/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.4911 - acc: 0.8020 - val_loss: 0.6945 - val_acc: 0.5810\n",
      "Epoch 217/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.4897 - acc: 0.8030 - val_loss: 0.6945 - val_acc: 0.5800\n",
      "Epoch 218/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.4888 - acc: 0.8060 - val_loss: 0.6945 - val_acc: 0.5780\n",
      "Epoch 219/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.4877 - acc: 0.8090 - val_loss: 0.6944 - val_acc: 0.5820\n",
      "Epoch 220/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.4870 - acc: 0.8120 - val_loss: 0.6943 - val_acc: 0.5830\n",
      "Epoch 221/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.4858 - acc: 0.8120 - val_loss: 0.6943 - val_acc: 0.5810\n",
      "Epoch 222/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.4848 - acc: 0.8140 - val_loss: 0.6942 - val_acc: 0.5820\n",
      "Epoch 223/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.4839 - acc: 0.8140 - val_loss: 0.6942 - val_acc: 0.5780\n",
      "Epoch 224/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.4829 - acc: 0.8150 - val_loss: 0.6942 - val_acc: 0.5770\n",
      "Epoch 225/600\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 0.4819 - acc: 0.8160 - val_loss: 0.6941 - val_acc: 0.5790\n",
      "Epoch 226/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.4810 - acc: 0.8210 - val_loss: 0.6940 - val_acc: 0.5830\n",
      "Epoch 227/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.4799 - acc: 0.8180 - val_loss: 0.6939 - val_acc: 0.5830\n",
      "Epoch 228/600\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.4789 - acc: 0.8220 - val_loss: 0.6939 - val_acc: 0.5800\n",
      "Epoch 229/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.4779 - acc: 0.8250 - val_loss: 0.6939 - val_acc: 0.5790\n",
      "Epoch 230/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.4770 - acc: 0.8240 - val_loss: 0.6938 - val_acc: 0.5820\n",
      "Epoch 231/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.4763 - acc: 0.8240 - val_loss: 0.6937 - val_acc: 0.5810\n",
      "Epoch 232/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.4750 - acc: 0.8300 - val_loss: 0.6937 - val_acc: 0.5780\n",
      "Epoch 233/600\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.4694 - acc: 0.849 - 0s 20us/step - loss: 0.4740 - acc: 0.8340 - val_loss: 0.6937 - val_acc: 0.5790\n",
      "Epoch 234/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.4738 - acc: 0.8330 - val_loss: 0.6937 - val_acc: 0.5800\n",
      "Epoch 235/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.4722 - acc: 0.8360 - val_loss: 0.6936 - val_acc: 0.5810\n",
      "Epoch 236/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.4713 - acc: 0.8330 - val_loss: 0.6935 - val_acc: 0.5780\n",
      "Epoch 237/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.4703 - acc: 0.8400 - val_loss: 0.6934 - val_acc: 0.5800\n",
      "Epoch 238/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.4707 - acc: 0.8360 - val_loss: 0.6934 - val_acc: 0.5790\n",
      "Epoch 239/600\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.4686 - acc: 0.8380 - val_loss: 0.6934 - val_acc: 0.5770\n",
      "Epoch 240/600\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.4685 - acc: 0.8390 - val_loss: 0.6934 - val_acc: 0.5810\n",
      "Epoch 241/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.4669 - acc: 0.8440 - val_loss: 0.6933 - val_acc: 0.5760\n",
      "Epoch 242/600\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.4661 - acc: 0.8450 - val_loss: 0.6933 - val_acc: 0.5780\n",
      "Epoch 243/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.4651 - acc: 0.8460 - val_loss: 0.6932 - val_acc: 0.5780\n",
      "Epoch 244/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.4642 - acc: 0.8500 - val_loss: 0.6931 - val_acc: 0.5770\n",
      "Epoch 245/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.4637 - acc: 0.8510 - val_loss: 0.6931 - val_acc: 0.5780\n",
      "Epoch 246/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.4626 - acc: 0.8460 - val_loss: 0.6931 - val_acc: 0.5770\n",
      "Epoch 247/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.4617 - acc: 0.8500 - val_loss: 0.6931 - val_acc: 0.5800\n",
      "Epoch 248/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.4608 - acc: 0.8490 - val_loss: 0.6930 - val_acc: 0.5800\n",
      "Epoch 249/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.4599 - acc: 0.8560 - val_loss: 0.6929 - val_acc: 0.5750\n",
      "Epoch 250/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.4593 - acc: 0.8530 - val_loss: 0.6929 - val_acc: 0.5760\n",
      "Epoch 251/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.4583 - acc: 0.8530 - val_loss: 0.6928 - val_acc: 0.5780\n",
      "Epoch 252/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.4573 - acc: 0.8560 - val_loss: 0.6928 - val_acc: 0.5770\n",
      "Epoch 253/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.4565 - acc: 0.8510 - val_loss: 0.6928 - val_acc: 0.5780\n",
      "Epoch 254/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.4555 - acc: 0.8600 - val_loss: 0.6927 - val_acc: 0.5740\n",
      "Epoch 255/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.4545 - acc: 0.8570 - val_loss: 0.6927 - val_acc: 0.5740\n",
      "Epoch 256/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.4544 - acc: 0.8570 - val_loss: 0.6928 - val_acc: 0.5780\n",
      "Epoch 257/600\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.4528 - acc: 0.8620 - val_loss: 0.6927 - val_acc: 0.5790\n",
      "Epoch 258/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.4520 - acc: 0.8600 - val_loss: 0.6927 - val_acc: 0.5790\n",
      "Epoch 259/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.4512 - acc: 0.8600 - val_loss: 0.6927 - val_acc: 0.5780\n",
      "Epoch 260/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.4502 - acc: 0.8620 - val_loss: 0.6926 - val_acc: 0.5780\n",
      "Epoch 261/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.4493 - acc: 0.8610 - val_loss: 0.6925 - val_acc: 0.5760\n",
      "Epoch 262/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.4484 - acc: 0.8620 - val_loss: 0.6925 - val_acc: 0.5760\n",
      "Epoch 263/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.4475 - acc: 0.8640 - val_loss: 0.6924 - val_acc: 0.5760\n",
      "Epoch 264/600\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.4466 - acc: 0.8630 - val_loss: 0.6924 - val_acc: 0.5760\n",
      "Epoch 265/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.4459 - acc: 0.8670 - val_loss: 0.6925 - val_acc: 0.5750\n",
      "Epoch 266/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.4451 - acc: 0.8680 - val_loss: 0.6924 - val_acc: 0.5770\n",
      "Epoch 267/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.4444 - acc: 0.8670 - val_loss: 0.6923 - val_acc: 0.5750\n",
      "Epoch 268/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.4431 - acc: 0.8700 - val_loss: 0.6923 - val_acc: 0.5760\n",
      "Epoch 269/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.4424 - acc: 0.8770 - val_loss: 0.6923 - val_acc: 0.5760\n",
      "Epoch 270/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.4414 - acc: 0.8710 - val_loss: 0.6923 - val_acc: 0.5750\n",
      "Epoch 271/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.4407 - acc: 0.8780 - val_loss: 0.6923 - val_acc: 0.5760\n",
      "Epoch 272/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.4396 - acc: 0.8780 - val_loss: 0.6922 - val_acc: 0.5750\n",
      "Epoch 273/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.4388 - acc: 0.8750 - val_loss: 0.6922 - val_acc: 0.5730\n",
      "Epoch 274/600\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.4379 - acc: 0.8780 - val_loss: 0.6921 - val_acc: 0.5760\n",
      "Epoch 275/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.4371 - acc: 0.8770 - val_loss: 0.6922 - val_acc: 0.5750\n",
      "Epoch 276/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.4361 - acc: 0.8800 - val_loss: 0.6921 - val_acc: 0.5740\n",
      "Epoch 277/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.4353 - acc: 0.8810 - val_loss: 0.6921 - val_acc: 0.5730\n",
      "Epoch 278/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.4344 - acc: 0.8810 - val_loss: 0.6920 - val_acc: 0.5760\n",
      "Epoch 279/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.4337 - acc: 0.8840 - val_loss: 0.6920 - val_acc: 0.5740\n",
      "Epoch 280/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.4326 - acc: 0.8830 - val_loss: 0.6920 - val_acc: 0.5740\n",
      "Epoch 281/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.4322 - acc: 0.8820 - val_loss: 0.6921 - val_acc: 0.5720\n",
      "Epoch 282/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.4310 - acc: 0.8870 - val_loss: 0.6920 - val_acc: 0.5740\n",
      "Epoch 283/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.4301 - acc: 0.8870 - val_loss: 0.6920 - val_acc: 0.5750\n",
      "Epoch 284/600\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.4295 - acc: 0.8850 - val_loss: 0.6919 - val_acc: 0.5740\n",
      "Epoch 285/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.4289 - acc: 0.8900 - val_loss: 0.6920 - val_acc: 0.5730\n",
      "Epoch 286/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.4276 - acc: 0.8910 - val_loss: 0.6919 - val_acc: 0.5740\n",
      "Epoch 287/600\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.4270 - acc: 0.8870 - val_loss: 0.6919 - val_acc: 0.5740\n",
      "Epoch 288/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.4261 - acc: 0.8940 - val_loss: 0.6918 - val_acc: 0.5740\n",
      "Epoch 289/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.4251 - acc: 0.8950 - val_loss: 0.6918 - val_acc: 0.5740\n",
      "Epoch 290/600\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 0.4245 - acc: 0.8910 - val_loss: 0.6918 - val_acc: 0.5740\n",
      "Epoch 291/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.4235 - acc: 0.8950 - val_loss: 0.6918 - val_acc: 0.5730\n",
      "Epoch 292/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.4229 - acc: 0.8970 - val_loss: 0.6919 - val_acc: 0.5740\n",
      "Epoch 293/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.4218 - acc: 0.8950 - val_loss: 0.6918 - val_acc: 0.5740\n",
      "Epoch 294/600\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.4210 - acc: 0.8980 - val_loss: 0.6918 - val_acc: 0.5740\n",
      "Epoch 295/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.4201 - acc: 0.8980 - val_loss: 0.6917 - val_acc: 0.5730\n",
      "Epoch 296/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.4194 - acc: 0.8970 - val_loss: 0.6917 - val_acc: 0.5750\n",
      "Epoch 297/600\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.4185 - acc: 0.9000 - val_loss: 0.6917 - val_acc: 0.5740\n",
      "Epoch 298/600\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.4208 - acc: 0.890 - 0s 17us/step - loss: 0.4176 - acc: 0.9000 - val_loss: 0.6917 - val_acc: 0.5740\n",
      "Epoch 299/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.4174 - acc: 0.8990 - val_loss: 0.6917 - val_acc: 0.5730\n",
      "Epoch 300/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.4160 - acc: 0.9010 - val_loss: 0.6917 - val_acc: 0.5750\n",
      "Epoch 301/600\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.4158 - acc: 0.8980 - val_loss: 0.6916 - val_acc: 0.5730\n",
      "Epoch 302/600\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.4144 - acc: 0.9010 - val_loss: 0.6917 - val_acc: 0.5730\n",
      "Epoch 303/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.4139 - acc: 0.9020 - val_loss: 0.6917 - val_acc: 0.5740\n",
      "Epoch 304/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.4128 - acc: 0.9040 - val_loss: 0.6916 - val_acc: 0.5730\n",
      "Epoch 305/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.4122 - acc: 0.9050 - val_loss: 0.6917 - val_acc: 0.5720\n",
      "Epoch 306/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.4116 - acc: 0.9090 - val_loss: 0.6916 - val_acc: 0.5730\n",
      "Epoch 307/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.4107 - acc: 0.9040 - val_loss: 0.6916 - val_acc: 0.5740\n",
      "Epoch 308/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.4097 - acc: 0.9070 - val_loss: 0.6916 - val_acc: 0.5730\n",
      "Epoch 309/600\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.4090 - acc: 0.9070 - val_loss: 0.6916 - val_acc: 0.5700\n",
      "Epoch 310/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.4088 - acc: 0.9110 - val_loss: 0.6917 - val_acc: 0.5710\n",
      "Epoch 311/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.4078 - acc: 0.9110 - val_loss: 0.6916 - val_acc: 0.5720\n",
      "Epoch 312/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.4067 - acc: 0.9080 - val_loss: 0.6915 - val_acc: 0.5730\n",
      "Epoch 313/600\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.4060 - acc: 0.9100 - val_loss: 0.6915 - val_acc: 0.5740\n",
      "Epoch 314/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.4058 - acc: 0.9150 - val_loss: 0.6915 - val_acc: 0.5760\n",
      "Epoch 315/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.4045 - acc: 0.9110 - val_loss: 0.6915 - val_acc: 0.5740\n",
      "Epoch 316/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.4041 - acc: 0.9100 - val_loss: 0.6916 - val_acc: 0.5690\n",
      "Epoch 317/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.4032 - acc: 0.9130 - val_loss: 0.6916 - val_acc: 0.5710\n",
      "Epoch 318/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.4022 - acc: 0.9150 - val_loss: 0.6915 - val_acc: 0.5690\n",
      "Epoch 319/600\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.4015 - acc: 0.9180 - val_loss: 0.6915 - val_acc: 0.5690\n",
      "Epoch 320/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.4007 - acc: 0.9180 - val_loss: 0.6915 - val_acc: 0.5700\n",
      "Epoch 321/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.4003 - acc: 0.9180 - val_loss: 0.6916 - val_acc: 0.5700\n",
      "Epoch 322/600\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.3992 - acc: 0.9190 - val_loss: 0.6915 - val_acc: 0.5700\n",
      "Epoch 323/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.3985 - acc: 0.9200 - val_loss: 0.6915 - val_acc: 0.5700\n",
      "Epoch 324/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.3981 - acc: 0.9210 - val_loss: 0.6915 - val_acc: 0.5720\n",
      "Epoch 325/600\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.3970 - acc: 0.9210 - val_loss: 0.6915 - val_acc: 0.5700\n",
      "Epoch 326/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.3962 - acc: 0.9230 - val_loss: 0.6915 - val_acc: 0.5700\n",
      "Epoch 327/600\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.3956 - acc: 0.9240 - val_loss: 0.6915 - val_acc: 0.5710\n",
      "Epoch 328/600\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.3947 - acc: 0.9260 - val_loss: 0.6915 - val_acc: 0.5690\n",
      "Epoch 329/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.3941 - acc: 0.9260 - val_loss: 0.6915 - val_acc: 0.5690\n",
      "Epoch 330/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.3933 - acc: 0.9250 - val_loss: 0.6915 - val_acc: 0.5700\n",
      "Epoch 331/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.3925 - acc: 0.9270 - val_loss: 0.6915 - val_acc: 0.5690\n",
      "Epoch 332/600\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.3923 - acc: 0.9270 - val_loss: 0.6915 - val_acc: 0.5700\n",
      "Epoch 333/600\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.3909 - acc: 0.9300 - val_loss: 0.6915 - val_acc: 0.5690\n",
      "Epoch 334/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.3903 - acc: 0.9290 - val_loss: 0.6915 - val_acc: 0.5700\n",
      "Epoch 335/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.3896 - acc: 0.9280 - val_loss: 0.6915 - val_acc: 0.5700\n",
      "Epoch 336/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.3888 - acc: 0.9300 - val_loss: 0.6915 - val_acc: 0.5670\n",
      "Epoch 337/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.3882 - acc: 0.9290 - val_loss: 0.6915 - val_acc: 0.5700\n",
      "Epoch 338/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.3872 - acc: 0.9310 - val_loss: 0.6915 - val_acc: 0.5670\n",
      "Epoch 339/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.3868 - acc: 0.9320 - val_loss: 0.6916 - val_acc: 0.5690\n",
      "Epoch 340/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.3857 - acc: 0.9320 - val_loss: 0.6915 - val_acc: 0.5670\n",
      "Epoch 341/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.3851 - acc: 0.9340 - val_loss: 0.6915 - val_acc: 0.5690\n",
      "Epoch 342/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.3846 - acc: 0.9320 - val_loss: 0.6915 - val_acc: 0.5690\n",
      "Epoch 343/600\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.3835 - acc: 0.9330 - val_loss: 0.6915 - val_acc: 0.5690\n",
      "Epoch 344/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.3829 - acc: 0.9350 - val_loss: 0.6915 - val_acc: 0.5680\n",
      "Epoch 345/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.3826 - acc: 0.9340 - val_loss: 0.6915 - val_acc: 0.5690\n",
      "Epoch 346/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.3815 - acc: 0.9370 - val_loss: 0.6915 - val_acc: 0.5690\n",
      "Epoch 347/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.3807 - acc: 0.9350 - val_loss: 0.6916 - val_acc: 0.5710\n",
      "Epoch 348/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.3803 - acc: 0.9340 - val_loss: 0.6915 - val_acc: 0.5700\n",
      "Epoch 349/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.3792 - acc: 0.9360 - val_loss: 0.6915 - val_acc: 0.5700\n",
      "Epoch 350/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.3787 - acc: 0.9380 - val_loss: 0.6916 - val_acc: 0.5710\n",
      "Epoch 351/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.3778 - acc: 0.9380 - val_loss: 0.6916 - val_acc: 0.5720\n",
      "Epoch 352/600\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.3771 - acc: 0.9350 - val_loss: 0.6915 - val_acc: 0.5720\n",
      "Epoch 353/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.3764 - acc: 0.9390 - val_loss: 0.6916 - val_acc: 0.5720\n",
      "Epoch 354/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.3757 - acc: 0.9380 - val_loss: 0.6916 - val_acc: 0.5700\n",
      "Epoch 355/600\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.3751 - acc: 0.9370 - val_loss: 0.6916 - val_acc: 0.5690\n",
      "Epoch 356/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.3742 - acc: 0.9400 - val_loss: 0.6916 - val_acc: 0.5720\n",
      "Epoch 357/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.3735 - acc: 0.9400 - val_loss: 0.6916 - val_acc: 0.5730\n",
      "Epoch 358/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.3730 - acc: 0.9400 - val_loss: 0.6917 - val_acc: 0.5730\n",
      "Epoch 359/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.3720 - acc: 0.9380 - val_loss: 0.6917 - val_acc: 0.5750\n",
      "Epoch 360/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.3716 - acc: 0.9400 - val_loss: 0.6918 - val_acc: 0.5730\n",
      "Epoch 361/600\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 0.3706 - acc: 0.9390 - val_loss: 0.6917 - val_acc: 0.5750\n",
      "Epoch 362/600\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 0.3703 - acc: 0.9390 - val_loss: 0.6917 - val_acc: 0.5710\n",
      "Epoch 363/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.3691 - acc: 0.9410 - val_loss: 0.6917 - val_acc: 0.5720\n",
      "Epoch 364/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.3685 - acc: 0.9430 - val_loss: 0.6918 - val_acc: 0.5740\n",
      "Epoch 365/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.3679 - acc: 0.9420 - val_loss: 0.6918 - val_acc: 0.5750\n",
      "Epoch 366/600\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.3672 - acc: 0.9410 - val_loss: 0.6918 - val_acc: 0.5750\n",
      "Epoch 367/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.3670 - acc: 0.9400 - val_loss: 0.6917 - val_acc: 0.5710\n",
      "Epoch 368/600\n",
      "1000/1000 [==============================] - 0s 22us/step - loss: 0.3657 - acc: 0.9420 - val_loss: 0.6918 - val_acc: 0.5730\n",
      "Epoch 369/600\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.3654 - acc: 0.9430 - val_loss: 0.6919 - val_acc: 0.5760\n",
      "Epoch 370/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.3645 - acc: 0.9430 - val_loss: 0.6919 - val_acc: 0.5760\n",
      "Epoch 371/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.3638 - acc: 0.9400 - val_loss: 0.6918 - val_acc: 0.5740\n",
      "Epoch 372/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.3632 - acc: 0.9410 - val_loss: 0.6919 - val_acc: 0.5750\n",
      "Epoch 373/600\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.3625 - acc: 0.9430 - val_loss: 0.6919 - val_acc: 0.5760\n",
      "Epoch 374/600\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 0.3617 - acc: 0.9430 - val_loss: 0.6919 - val_acc: 0.5750\n",
      "Epoch 375/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.3610 - acc: 0.9430 - val_loss: 0.6919 - val_acc: 0.5770\n",
      "Epoch 376/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.3606 - acc: 0.9440 - val_loss: 0.6920 - val_acc: 0.5740\n",
      "Epoch 377/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.3598 - acc: 0.9410 - val_loss: 0.6919 - val_acc: 0.5760\n",
      "Epoch 378/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.3595 - acc: 0.9410 - val_loss: 0.6919 - val_acc: 0.5740\n",
      "Epoch 379/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.3584 - acc: 0.9440 - val_loss: 0.6919 - val_acc: 0.5750\n",
      "Epoch 380/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.3578 - acc: 0.9440 - val_loss: 0.6920 - val_acc: 0.5770\n",
      "Epoch 381/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.3570 - acc: 0.9440 - val_loss: 0.6920 - val_acc: 0.5760\n",
      "Epoch 382/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.3563 - acc: 0.9430 - val_loss: 0.6920 - val_acc: 0.5780\n",
      "Epoch 383/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.3557 - acc: 0.9430 - val_loss: 0.6920 - val_acc: 0.5760\n",
      "Epoch 384/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.3550 - acc: 0.9460 - val_loss: 0.6921 - val_acc: 0.5770\n",
      "Epoch 385/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.3543 - acc: 0.9450 - val_loss: 0.6921 - val_acc: 0.5770\n",
      "Epoch 386/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.3537 - acc: 0.9440 - val_loss: 0.6921 - val_acc: 0.5770\n",
      "Epoch 387/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.3531 - acc: 0.9450 - val_loss: 0.6922 - val_acc: 0.5790\n",
      "Epoch 388/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.3524 - acc: 0.9440 - val_loss: 0.6922 - val_acc: 0.5790\n",
      "Epoch 389/600\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.3517 - acc: 0.9460 - val_loss: 0.6922 - val_acc: 0.5790\n",
      "Epoch 390/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.3514 - acc: 0.9470 - val_loss: 0.6923 - val_acc: 0.5760\n",
      "Epoch 391/600\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.3503 - acc: 0.9480 - val_loss: 0.6922 - val_acc: 0.5760\n",
      "Epoch 392/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.3496 - acc: 0.9460 - val_loss: 0.6923 - val_acc: 0.5760\n",
      "Epoch 393/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.3489 - acc: 0.9470 - val_loss: 0.6922 - val_acc: 0.5770\n",
      "Epoch 394/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.3483 - acc: 0.9470 - val_loss: 0.6923 - val_acc: 0.5780\n",
      "Epoch 395/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.3479 - acc: 0.9500 - val_loss: 0.6924 - val_acc: 0.5750\n",
      "Epoch 396/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.3471 - acc: 0.9520 - val_loss: 0.6924 - val_acc: 0.5760\n",
      "Epoch 397/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.3463 - acc: 0.9500 - val_loss: 0.6923 - val_acc: 0.5790\n",
      "Epoch 398/600\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.3457 - acc: 0.9500 - val_loss: 0.6923 - val_acc: 0.5780\n",
      "Epoch 399/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.3453 - acc: 0.9490 - val_loss: 0.6925 - val_acc: 0.5760\n",
      "Epoch 400/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.3446 - acc: 0.9520 - val_loss: 0.6924 - val_acc: 0.5790\n",
      "Epoch 401/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.3438 - acc: 0.9530 - val_loss: 0.6925 - val_acc: 0.5790\n",
      "Epoch 402/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.3430 - acc: 0.9550 - val_loss: 0.6925 - val_acc: 0.5790\n",
      "Epoch 403/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.3425 - acc: 0.9540 - val_loss: 0.6925 - val_acc: 0.5780\n",
      "Epoch 404/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.3419 - acc: 0.9560 - val_loss: 0.6925 - val_acc: 0.5790\n",
      "Epoch 405/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.3414 - acc: 0.9590 - val_loss: 0.6925 - val_acc: 0.5780\n",
      "Epoch 406/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.3407 - acc: 0.9560 - val_loss: 0.6925 - val_acc: 0.5770\n",
      "Epoch 407/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.3399 - acc: 0.9550 - val_loss: 0.6926 - val_acc: 0.5790\n",
      "Epoch 408/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.3398 - acc: 0.9570 - val_loss: 0.6926 - val_acc: 0.5780\n",
      "Epoch 409/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.3392 - acc: 0.9580 - val_loss: 0.6926 - val_acc: 0.5770\n",
      "Epoch 410/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.3383 - acc: 0.9560 - val_loss: 0.6926 - val_acc: 0.5770\n",
      "Epoch 411/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.3374 - acc: 0.9570 - val_loss: 0.6927 - val_acc: 0.5790\n",
      "Epoch 412/600\n",
      "1000/1000 [==============================] - 0s 22us/step - loss: 0.3370 - acc: 0.9590 - val_loss: 0.6927 - val_acc: 0.5770\n",
      "Epoch 413/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.3363 - acc: 0.9580 - val_loss: 0.6927 - val_acc: 0.5770\n",
      "Epoch 414/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.3359 - acc: 0.9580 - val_loss: 0.6927 - val_acc: 0.5780\n",
      "Epoch 415/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.3352 - acc: 0.9590 - val_loss: 0.6928 - val_acc: 0.5780\n",
      "Epoch 416/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.3344 - acc: 0.9580 - val_loss: 0.6928 - val_acc: 0.5770\n",
      "Epoch 417/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.3339 - acc: 0.9600 - val_loss: 0.6928 - val_acc: 0.5770\n",
      "Epoch 418/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.3334 - acc: 0.9600 - val_loss: 0.6929 - val_acc: 0.5770\n",
      "Epoch 419/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.3326 - acc: 0.9580 - val_loss: 0.6929 - val_acc: 0.5790\n",
      "Epoch 420/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.3319 - acc: 0.9590 - val_loss: 0.6930 - val_acc: 0.5790\n",
      "Epoch 421/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.3317 - acc: 0.9590 - val_loss: 0.6929 - val_acc: 0.5790\n",
      "Epoch 422/600\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.3312 - acc: 0.9570 - val_loss: 0.6931 - val_acc: 0.5780\n",
      "Epoch 423/600\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.3302 - acc: 0.9610 - val_loss: 0.6930 - val_acc: 0.5800\n",
      "Epoch 424/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.3297 - acc: 0.9590 - val_loss: 0.6931 - val_acc: 0.5760\n",
      "Epoch 425/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.3290 - acc: 0.9620 - val_loss: 0.6931 - val_acc: 0.5800\n",
      "Epoch 426/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.3283 - acc: 0.9600 - val_loss: 0.6932 - val_acc: 0.5760\n",
      "Epoch 427/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.3278 - acc: 0.9630 - val_loss: 0.6932 - val_acc: 0.5790\n",
      "Epoch 428/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.3272 - acc: 0.9630 - val_loss: 0.6932 - val_acc: 0.5810\n",
      "Epoch 429/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.3265 - acc: 0.9600 - val_loss: 0.6932 - val_acc: 0.5780\n",
      "Epoch 430/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.3259 - acc: 0.9620 - val_loss: 0.6933 - val_acc: 0.5750\n",
      "Epoch 431/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.3252 - acc: 0.9640 - val_loss: 0.6933 - val_acc: 0.5790\n",
      "Epoch 432/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.3246 - acc: 0.9640 - val_loss: 0.6933 - val_acc: 0.5790\n",
      "Epoch 433/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.3240 - acc: 0.9630 - val_loss: 0.6934 - val_acc: 0.5760\n",
      "Epoch 434/600\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 0.3235 - acc: 0.9640 - val_loss: 0.6935 - val_acc: 0.5780\n",
      "Epoch 435/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.3227 - acc: 0.9660 - val_loss: 0.6935 - val_acc: 0.5770\n",
      "Epoch 436/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.3221 - acc: 0.9660 - val_loss: 0.6935 - val_acc: 0.5780\n",
      "Epoch 437/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.3218 - acc: 0.9640 - val_loss: 0.6937 - val_acc: 0.5760\n",
      "Epoch 438/600\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.3210 - acc: 0.9680 - val_loss: 0.6936 - val_acc: 0.5810\n",
      "Epoch 439/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.3203 - acc: 0.9660 - val_loss: 0.6936 - val_acc: 0.5800\n",
      "Epoch 440/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.3199 - acc: 0.9650 - val_loss: 0.6937 - val_acc: 0.5770\n",
      "Epoch 441/600\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.3190 - acc: 0.9680 - val_loss: 0.6937 - val_acc: 0.5800\n",
      "Epoch 442/600\n",
      "1000/1000 [==============================] - 0s 22us/step - loss: 0.3184 - acc: 0.9680 - val_loss: 0.6937 - val_acc: 0.5800\n",
      "Epoch 443/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.3178 - acc: 0.9680 - val_loss: 0.6938 - val_acc: 0.5760\n",
      "Epoch 444/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.3172 - acc: 0.9700 - val_loss: 0.6939 - val_acc: 0.5780\n",
      "Epoch 445/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.3172 - acc: 0.9670 - val_loss: 0.6939 - val_acc: 0.5810\n",
      "Epoch 446/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.3160 - acc: 0.9690 - val_loss: 0.6939 - val_acc: 0.5820\n",
      "Epoch 447/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.3154 - acc: 0.9670 - val_loss: 0.6940 - val_acc: 0.5790\n",
      "Epoch 448/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.3148 - acc: 0.9700 - val_loss: 0.6940 - val_acc: 0.5770\n",
      "Epoch 449/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.3142 - acc: 0.9700 - val_loss: 0.6941 - val_acc: 0.5790\n",
      "Epoch 450/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.3136 - acc: 0.9700 - val_loss: 0.6941 - val_acc: 0.5790\n",
      "Epoch 451/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.3131 - acc: 0.9710 - val_loss: 0.6941 - val_acc: 0.5800\n",
      "Epoch 452/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.3126 - acc: 0.9710 - val_loss: 0.6941 - val_acc: 0.5810\n",
      "Epoch 453/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.3120 - acc: 0.9700 - val_loss: 0.6943 - val_acc: 0.5820\n",
      "Epoch 454/600\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.3102 - acc: 0.968 - 0s 19us/step - loss: 0.3112 - acc: 0.9720 - val_loss: 0.6943 - val_acc: 0.5810\n",
      "Epoch 455/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.3106 - acc: 0.9720 - val_loss: 0.6943 - val_acc: 0.5820\n",
      "Epoch 456/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.3100 - acc: 0.9720 - val_loss: 0.6943 - val_acc: 0.5790\n",
      "Epoch 457/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.3096 - acc: 0.9710 - val_loss: 0.6945 - val_acc: 0.5830\n",
      "Epoch 458/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.3092 - acc: 0.9720 - val_loss: 0.6946 - val_acc: 0.5830\n",
      "Epoch 459/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.3082 - acc: 0.9720 - val_loss: 0.6945 - val_acc: 0.5850\n",
      "Epoch 460/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.3076 - acc: 0.9720 - val_loss: 0.6945 - val_acc: 0.5840\n",
      "Epoch 461/600\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.3074 - acc: 0.9720 - val_loss: 0.6945 - val_acc: 0.5800\n",
      "Epoch 462/600\n",
      "1000/1000 [==============================] - 0s 22us/step - loss: 0.3065 - acc: 0.9730 - val_loss: 0.6946 - val_acc: 0.5850\n",
      "Epoch 463/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.3060 - acc: 0.9720 - val_loss: 0.6946 - val_acc: 0.5820\n",
      "Epoch 464/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.3054 - acc: 0.9740 - val_loss: 0.6948 - val_acc: 0.5850\n",
      "Epoch 465/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.3048 - acc: 0.9750 - val_loss: 0.6947 - val_acc: 0.5850\n",
      "Epoch 466/600\n",
      "1000/1000 [==============================] - 0s 22us/step - loss: 0.3045 - acc: 0.9750 - val_loss: 0.6948 - val_acc: 0.5830\n",
      "Epoch 467/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.3037 - acc: 0.9730 - val_loss: 0.6948 - val_acc: 0.5830\n",
      "Epoch 468/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.3039 - acc: 0.9750 - val_loss: 0.6948 - val_acc: 0.5830\n",
      "Epoch 469/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.3026 - acc: 0.9750 - val_loss: 0.6949 - val_acc: 0.5860\n",
      "Epoch 470/600\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.3020 - acc: 0.9760 - val_loss: 0.6949 - val_acc: 0.5850\n",
      "Epoch 471/600\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.3014 - acc: 0.9760 - val_loss: 0.6950 - val_acc: 0.5860\n",
      "Epoch 472/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.3009 - acc: 0.9770 - val_loss: 0.6951 - val_acc: 0.5850\n",
      "Epoch 473/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.3003 - acc: 0.9770 - val_loss: 0.6951 - val_acc: 0.5850\n",
      "Epoch 474/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.3000 - acc: 0.9740 - val_loss: 0.6952 - val_acc: 0.5840\n",
      "Epoch 475/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.2993 - acc: 0.9770 - val_loss: 0.6953 - val_acc: 0.5840\n",
      "Epoch 476/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.2987 - acc: 0.9770 - val_loss: 0.6953 - val_acc: 0.5850\n",
      "Epoch 477/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.2981 - acc: 0.9780 - val_loss: 0.6953 - val_acc: 0.5830\n",
      "Epoch 478/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.2976 - acc: 0.9780 - val_loss: 0.6954 - val_acc: 0.5840\n",
      "Epoch 479/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.2975 - acc: 0.9780 - val_loss: 0.6955 - val_acc: 0.5840\n",
      "Epoch 480/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.2972 - acc: 0.9780 - val_loss: 0.6955 - val_acc: 0.5860\n",
      "Epoch 481/600\n",
      "1000/1000 [==============================] - 0s 16us/step - loss: 0.2959 - acc: 0.9790 - val_loss: 0.6955 - val_acc: 0.5850\n",
      "Epoch 482/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.2954 - acc: 0.9800 - val_loss: 0.6955 - val_acc: 0.5830\n",
      "Epoch 483/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.2949 - acc: 0.9790 - val_loss: 0.6955 - val_acc: 0.5850\n",
      "Epoch 484/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.2943 - acc: 0.9790 - val_loss: 0.6956 - val_acc: 0.5840\n",
      "Epoch 485/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.2941 - acc: 0.9790 - val_loss: 0.6956 - val_acc: 0.5840\n",
      "Epoch 486/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.2933 - acc: 0.9800 - val_loss: 0.6956 - val_acc: 0.5850\n",
      "Epoch 487/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.2927 - acc: 0.9800 - val_loss: 0.6957 - val_acc: 0.5850\n",
      "Epoch 488/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.2922 - acc: 0.9800 - val_loss: 0.6957 - val_acc: 0.5840\n",
      "Epoch 489/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.2917 - acc: 0.9800 - val_loss: 0.6958 - val_acc: 0.5840\n",
      "Epoch 490/600\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 0.2911 - acc: 0.9810 - val_loss: 0.6959 - val_acc: 0.5840\n",
      "Epoch 491/600\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.2905 - acc: 0.9800 - val_loss: 0.6959 - val_acc: 0.5850\n",
      "Epoch 492/600\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.2900 - acc: 0.9810 - val_loss: 0.6959 - val_acc: 0.5850\n",
      "Epoch 493/600\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.2894 - acc: 0.9810 - val_loss: 0.6960 - val_acc: 0.5840\n",
      "Epoch 494/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.2890 - acc: 0.9820 - val_loss: 0.6961 - val_acc: 0.5840\n",
      "Epoch 495/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.2883 - acc: 0.9810 - val_loss: 0.6961 - val_acc: 0.5850\n",
      "Epoch 496/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.2880 - acc: 0.9800 - val_loss: 0.6961 - val_acc: 0.5840\n",
      "Epoch 497/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.2873 - acc: 0.9820 - val_loss: 0.6963 - val_acc: 0.5840\n",
      "Epoch 498/600\n",
      "1000/1000 [==============================] - 0s 22us/step - loss: 0.2866 - acc: 0.9820 - val_loss: 0.6963 - val_acc: 0.5840\n",
      "Epoch 499/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.2864 - acc: 0.9840 - val_loss: 0.6964 - val_acc: 0.5840\n",
      "Epoch 500/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.2859 - acc: 0.9820 - val_loss: 0.6965 - val_acc: 0.5860\n",
      "Epoch 501/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.2851 - acc: 0.9810 - val_loss: 0.6965 - val_acc: 0.5850\n",
      "Epoch 502/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.2847 - acc: 0.9810 - val_loss: 0.6964 - val_acc: 0.5830\n",
      "Epoch 503/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.2842 - acc: 0.9830 - val_loss: 0.6966 - val_acc: 0.5850\n",
      "Epoch 504/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.2835 - acc: 0.9820 - val_loss: 0.6966 - val_acc: 0.5840\n",
      "Epoch 505/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.2836 - acc: 0.9830 - val_loss: 0.6967 - val_acc: 0.5860\n",
      "Epoch 506/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.2825 - acc: 0.9820 - val_loss: 0.6967 - val_acc: 0.5860\n",
      "Epoch 507/600\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.2820 - acc: 0.9820 - val_loss: 0.6967 - val_acc: 0.5850\n",
      "Epoch 508/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.2815 - acc: 0.9830 - val_loss: 0.6968 - val_acc: 0.5860\n",
      "Epoch 509/600\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.2809 - acc: 0.9820 - val_loss: 0.6968 - val_acc: 0.5850\n",
      "Epoch 510/600\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 0.2805 - acc: 0.9830 - val_loss: 0.6969 - val_acc: 0.5830\n",
      "Epoch 511/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.2799 - acc: 0.9840 - val_loss: 0.6970 - val_acc: 0.5870\n",
      "Epoch 512/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.2795 - acc: 0.9840 - val_loss: 0.6971 - val_acc: 0.5850\n",
      "Epoch 513/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.2788 - acc: 0.9840 - val_loss: 0.6971 - val_acc: 0.5870\n",
      "Epoch 514/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.2785 - acc: 0.9830 - val_loss: 0.6972 - val_acc: 0.5860\n",
      "Epoch 515/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.2778 - acc: 0.9840 - val_loss: 0.6972 - val_acc: 0.5840\n",
      "Epoch 516/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.2774 - acc: 0.9840 - val_loss: 0.6973 - val_acc: 0.5860\n",
      "Epoch 517/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.2768 - acc: 0.9840 - val_loss: 0.6974 - val_acc: 0.5860\n",
      "Epoch 518/600\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.2766 - acc: 0.9850 - val_loss: 0.6975 - val_acc: 0.5870\n",
      "Epoch 519/600\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.2757 - acc: 0.9850 - val_loss: 0.6974 - val_acc: 0.5860\n",
      "Epoch 520/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.2752 - acc: 0.9840 - val_loss: 0.6975 - val_acc: 0.5860\n",
      "Epoch 521/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.2747 - acc: 0.9840 - val_loss: 0.6976 - val_acc: 0.5860\n",
      "Epoch 522/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.2742 - acc: 0.9850 - val_loss: 0.6977 - val_acc: 0.5850\n",
      "Epoch 523/600\n",
      "1000/1000 [==============================] - 0s 16us/step - loss: 0.2738 - acc: 0.9840 - val_loss: 0.6978 - val_acc: 0.5890\n",
      "Epoch 524/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.2731 - acc: 0.9850 - val_loss: 0.6977 - val_acc: 0.5860\n",
      "Epoch 525/600\n",
      "1000/1000 [==============================] - 0s 15us/step - loss: 0.2726 - acc: 0.9850 - val_loss: 0.6978 - val_acc: 0.5850\n",
      "Epoch 526/600\n",
      "1000/1000 [==============================] - 0s 22us/step - loss: 0.2721 - acc: 0.9850 - val_loss: 0.6978 - val_acc: 0.5870\n",
      "Epoch 527/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.2717 - acc: 0.9850 - val_loss: 0.6980 - val_acc: 0.5880\n",
      "Epoch 528/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.2710 - acc: 0.9850 - val_loss: 0.6980 - val_acc: 0.5870\n",
      "Epoch 529/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.2706 - acc: 0.9850 - val_loss: 0.6981 - val_acc: 0.5880\n",
      "Epoch 530/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.2700 - acc: 0.9850 - val_loss: 0.6982 - val_acc: 0.5860\n",
      "Epoch 531/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.2695 - acc: 0.9850 - val_loss: 0.6982 - val_acc: 0.5850\n",
      "Epoch 532/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.2690 - acc: 0.9850 - val_loss: 0.6982 - val_acc: 0.5850\n",
      "Epoch 533/600\n",
      "1000/1000 [==============================] - 0s 22us/step - loss: 0.2684 - acc: 0.9860 - val_loss: 0.6983 - val_acc: 0.5850\n",
      "Epoch 534/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.2680 - acc: 0.9860 - val_loss: 0.6984 - val_acc: 0.5870\n",
      "Epoch 535/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.2674 - acc: 0.9850 - val_loss: 0.6985 - val_acc: 0.5870\n",
      "Epoch 536/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.2669 - acc: 0.9850 - val_loss: 0.6985 - val_acc: 0.5850\n",
      "Epoch 537/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.2664 - acc: 0.9860 - val_loss: 0.6986 - val_acc: 0.5870\n",
      "Epoch 538/600\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.2658 - acc: 0.9860 - val_loss: 0.6987 - val_acc: 0.5870\n",
      "Epoch 539/600\n",
      "1000/1000 [==============================] - 0s 22us/step - loss: 0.2653 - acc: 0.9860 - val_loss: 0.6987 - val_acc: 0.5870\n",
      "Epoch 540/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.2647 - acc: 0.9860 - val_loss: 0.6988 - val_acc: 0.5880\n",
      "Epoch 541/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.2642 - acc: 0.9870 - val_loss: 0.6989 - val_acc: 0.5870\n",
      "Epoch 542/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.2640 - acc: 0.9860 - val_loss: 0.6989 - val_acc: 0.5830\n",
      "Epoch 543/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.2633 - acc: 0.9870 - val_loss: 0.6991 - val_acc: 0.5880\n",
      "Epoch 544/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.2628 - acc: 0.9870 - val_loss: 0.6990 - val_acc: 0.5850\n",
      "Epoch 545/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.2622 - acc: 0.9870 - val_loss: 0.6991 - val_acc: 0.5860\n",
      "Epoch 546/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.2619 - acc: 0.9900 - val_loss: 0.6993 - val_acc: 0.5860\n",
      "Epoch 547/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.2612 - acc: 0.9880 - val_loss: 0.6993 - val_acc: 0.5850\n",
      "Epoch 548/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.2607 - acc: 0.9880 - val_loss: 0.6994 - val_acc: 0.5870\n",
      "Epoch 549/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.2604 - acc: 0.9890 - val_loss: 0.6995 - val_acc: 0.5870\n",
      "Epoch 550/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.2597 - acc: 0.9880 - val_loss: 0.6995 - val_acc: 0.5860\n",
      "Epoch 551/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.2592 - acc: 0.9880 - val_loss: 0.6995 - val_acc: 0.5870\n",
      "Epoch 552/600\n",
      "1000/1000 [==============================] - 0s 22us/step - loss: 0.2587 - acc: 0.9880 - val_loss: 0.6996 - val_acc: 0.5860\n",
      "Epoch 553/600\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.2582 - acc: 0.9880 - val_loss: 0.6997 - val_acc: 0.5860\n",
      "Epoch 554/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.2578 - acc: 0.9900 - val_loss: 0.6998 - val_acc: 0.5880\n",
      "Epoch 555/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.2572 - acc: 0.9890 - val_loss: 0.6998 - val_acc: 0.5870\n",
      "Epoch 556/600\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.2567 - acc: 0.9890 - val_loss: 0.6999 - val_acc: 0.5870\n",
      "Epoch 557/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.2562 - acc: 0.9900 - val_loss: 0.7000 - val_acc: 0.5870\n",
      "Epoch 558/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.2557 - acc: 0.9910 - val_loss: 0.7001 - val_acc: 0.5870\n",
      "Epoch 559/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.2551 - acc: 0.9910 - val_loss: 0.7002 - val_acc: 0.5880\n",
      "Epoch 560/600\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.2546 - acc: 0.9900 - val_loss: 0.7002 - val_acc: 0.5880\n",
      "Epoch 561/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.2541 - acc: 0.9910 - val_loss: 0.7003 - val_acc: 0.5880\n",
      "Epoch 562/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.2536 - acc: 0.9910 - val_loss: 0.7004 - val_acc: 0.5870\n",
      "Epoch 563/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.2534 - acc: 0.9890 - val_loss: 0.7004 - val_acc: 0.5840\n",
      "Epoch 564/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.2526 - acc: 0.9920 - val_loss: 0.7005 - val_acc: 0.5870\n",
      "Epoch 565/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.2521 - acc: 0.9910 - val_loss: 0.7006 - val_acc: 0.5870\n",
      "Epoch 566/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.2516 - acc: 0.9920 - val_loss: 0.7007 - val_acc: 0.5870\n",
      "Epoch 567/600\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.2512 - acc: 0.9920 - val_loss: 0.7007 - val_acc: 0.5850\n",
      "Epoch 568/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.2507 - acc: 0.9930 - val_loss: 0.7008 - val_acc: 0.5860\n",
      "Epoch 569/600\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.2504 - acc: 0.9930 - val_loss: 0.7009 - val_acc: 0.5840\n",
      "Epoch 570/600\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.2499 - acc: 0.9920 - val_loss: 0.7009 - val_acc: 0.5840\n",
      "Epoch 571/600\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.2492 - acc: 0.9930 - val_loss: 0.7011 - val_acc: 0.5900\n",
      "Epoch 572/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.2490 - acc: 0.9920 - val_loss: 0.7011 - val_acc: 0.5850\n",
      "Epoch 573/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.2483 - acc: 0.9930 - val_loss: 0.7012 - val_acc: 0.5890\n",
      "Epoch 574/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.2478 - acc: 0.9920 - val_loss: 0.7012 - val_acc: 0.5880\n",
      "Epoch 575/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.2473 - acc: 0.9930 - val_loss: 0.7013 - val_acc: 0.5900\n",
      "Epoch 576/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.2469 - acc: 0.9920 - val_loss: 0.7014 - val_acc: 0.5880\n",
      "Epoch 577/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.2464 - acc: 0.9930 - val_loss: 0.7015 - val_acc: 0.5890\n",
      "Epoch 578/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.2461 - acc: 0.9930 - val_loss: 0.7015 - val_acc: 0.5850\n",
      "Epoch 579/600\n",
      "1000/1000 [==============================] - 0s 22us/step - loss: 0.2454 - acc: 0.9940 - val_loss: 0.7016 - val_acc: 0.5890\n",
      "Epoch 580/600\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.2452 - acc: 0.9940 - val_loss: 0.7018 - val_acc: 0.5870\n",
      "Epoch 581/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.2445 - acc: 0.9940 - val_loss: 0.7018 - val_acc: 0.5880\n",
      "Epoch 582/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.2440 - acc: 0.9950 - val_loss: 0.7018 - val_acc: 0.5890\n",
      "Epoch 583/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.2436 - acc: 0.9950 - val_loss: 0.7020 - val_acc: 0.5900\n",
      "Epoch 584/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.2431 - acc: 0.9950 - val_loss: 0.7020 - val_acc: 0.5900\n",
      "Epoch 585/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.2426 - acc: 0.9950 - val_loss: 0.7021 - val_acc: 0.5910\n",
      "Epoch 586/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.2422 - acc: 0.9950 - val_loss: 0.7021 - val_acc: 0.5850\n",
      "Epoch 587/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.2417 - acc: 0.9950 - val_loss: 0.7022 - val_acc: 0.5880\n",
      "Epoch 588/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.2412 - acc: 0.9950 - val_loss: 0.7023 - val_acc: 0.5890\n",
      "Epoch 589/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.2409 - acc: 0.9950 - val_loss: 0.7024 - val_acc: 0.5860\n",
      "Epoch 590/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.2404 - acc: 0.9950 - val_loss: 0.7024 - val_acc: 0.5860\n",
      "Epoch 591/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.2398 - acc: 0.9950 - val_loss: 0.7026 - val_acc: 0.5900\n",
      "Epoch 592/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.2393 - acc: 0.9950 - val_loss: 0.7026 - val_acc: 0.5900\n",
      "Epoch 593/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.2389 - acc: 0.9950 - val_loss: 0.7027 - val_acc: 0.5860\n",
      "Epoch 594/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.2385 - acc: 0.9950 - val_loss: 0.7029 - val_acc: 0.5880\n",
      "Epoch 595/600\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.2381 - acc: 0.9950 - val_loss: 0.7030 - val_acc: 0.5880\n",
      "Epoch 596/600\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.2376 - acc: 0.9960 - val_loss: 0.7030 - val_acc: 0.5880\n",
      "Epoch 597/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.2370 - acc: 0.9950 - val_loss: 0.7031 - val_acc: 0.5890\n",
      "Epoch 598/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.2366 - acc: 0.9950 - val_loss: 0.7031 - val_acc: 0.5860\n",
      "Epoch 599/600\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.2364 - acc: 0.9950 - val_loss: 0.7031 - val_acc: 0.5870\n",
      "Epoch 600/600\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.2359 - acc: 0.9960 - val_loss: 0.7032 - val_acc: 0.5870\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 600\n",
    "batch_size = 512\n",
    "\n",
    "history = model.fit(x_train_cut,\n",
    "                    y_train, \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs,\n",
    "                    validation_data=(x_val_cut, y_val),\n",
    "                    verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e) Plot the training and validation loss and accuracies and evaluate the trained model on the test set.\n",
    "\n",
    "What do you observe from the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.utils.fixes import signature\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, roc_curve, auc\n",
    "\n",
    "def plot_train_val_acc_curve(_hist):\n",
    "    \n",
    "    plt.plot(_hist.history['acc'])\n",
    "    plt.plot(_hist.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    \n",
    "    return plt.show()\n",
    "\n",
    "\n",
    "def plot_train_val_loss_curve(_hist):\n",
    "    \n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    \n",
    "\n",
    "def plot_roc_curve(_y, _y_hat):\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(_y.argmax(axis=1).ravel(), first_arg(_y_hat))\n",
    "    auc_simple = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label='Keras (area = {:.3f})'.format(auc_simple))\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    return plt.show()\n",
    "\n",
    "\n",
    "def plot_precision_recall_curve(labels, predictions, classifier_name=None):\n",
    "    \n",
    "    title = \"Precision-Recall curve\"\n",
    "    if classifier_name is not None:\n",
    "        title = \"Precision-Recall curve for {} predictions\".format(classifier_name)\n",
    "        \n",
    "    precision, recall, thresholds = precision_recall_curve(labels, predictions)\n",
    "    \n",
    "    average_precision = average_precision_score(labels, predictions)\n",
    "\n",
    "    # In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\n",
    "    step_kwargs = ({'step': 'post'}\n",
    "                   if 'step' in signature(plt.fill_between).parameters\n",
    "                   else {})\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    fig.set_figwidth(10)\n",
    "    fig.suptitle('Precision-Recall curve for {0}: AP={1:0.2f}'.format(classifier_name, average_precision))\n",
    "    \n",
    "    plot = plt.subplot(1, 2, 2)\n",
    "    \n",
    "    plot.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "    \n",
    "    plot.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "    plot.set_xlabel('Recall')\n",
    "    plot.set_ylabel('Precision')\n",
    "    plot.set_ylim([0.0, 1.05])\n",
    "    plot.set_xlim([0.0, 1.0])\n",
    "       \n",
    "    return plot\n",
    "\n",
    "\n",
    "def first_arg(arr):\n",
    "    \n",
    "    _arr = []\n",
    "    \n",
    "    for i in range(len(arr)):\n",
    "        _arr.append(arr[i][1])\n",
    "    \n",
    "    return _arr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27793979e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_train_val_acc_curve(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27782ba6f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plot_train_val_loss_curve(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6992032380867005\n",
      "Test accuracy: 0.6026\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_predictions = model.predict(x_test_cut)\n",
    "\n",
    "score = model.evaluate(x_test_cut, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xd4VFX6wPHvC4TeE3qAhN6kSOiKsFRBAUUBFVFBWVBsuLuyqyKWVdFVV9eKoiA/UMGKiGJBRFBK6B0CBEgIECAEQkiZ5P39MZcYIIEBMjOZ5P08zzzccube9yZh3jnn3HuOqCrGGGMMQBF/B2CMMSb/sKRgjDEmiyUFY4wxWSwpGGOMyWJJwRhjTBZLCsYYY7JYUjDGGJPFkoIpcEQkWkROiUiSiBwQkWkiUvasMp1FZKGInBCRRBH5RkSanVWmvIj8V0T2Osfa6ayH+PaKjPEdSwqmoLpeVcsCrYE2wD9P7xCRTsAPwNdATSAcWAcsFZF6TpniwM9Ac6AvUB7oBBwB2nsraBEp5q1jG+MJSwqmQFPVA8AC3MnhtBeBj1T1NVU9oapHVfVxYBkwySkzAqgD3KCqm1U1U1UPqeozqjo/p3OJSHMR+VFEjorIQRH5l7N9mog8m61cNxGJybYeLSKPish64KSz/NlZx35NRF53liuIyFQRiRORWBF5VkSKXuaPyhjAkoIp4EQkFLgWiHLWSwOdgTk5FJ8N9HKWewLfq2qSh+cpB/wEfI+79tEAd03DU7cA/YGKwCdAP+eYOB/4Q4BZTtlpgMs5RxugN3D3RZzLmFxZUjAF1VcicgLYBxwCnnS2V8b9dx+Xw3vigNP9BcG5lMnNdcABVX1ZVVOcGsjyi3j/66q6T1VPqeoeYDVwg7PvL0Cyqi4TkWpAP+AhVT2pqoeAV4FhF3EuY3JlScEUVINUtRzQDWjCnx/2CUAmUCOH99QADjvLR3Ipk5vawM5LitRt31nrs3DXHgBu5c9aQl0gCIgTkWMicgx4F6h6Gec2JoslBVOgqeqvuJtb/uOsnwT+AG7OofgQ/mzy+QnoIyJlPDzVPqBeLvtOAqWzrVfPKdSz1ucA3Zzmrxv4MynsA1KBEFWt6LzKq2pzD+M05rwsKZjC4L9ALxFp5axPAO4QkQdEpJyIVHI6gjsBTzllZuD+AP5cRJqISBERCRaRf4lIvxzOMQ+oISIPiUgJ57gdnH1rcfcRVBaR6sBDFwpYVeOBRcCHwG5V3eJsj8N959TLzi2zRUSkvohccwk/F2POYUnBFHjOB+xHwERnfQnQB7gRd7/BHtwdtlep6g6nTCruzuatwI/AcWAF7maoc/oKVPUE7k7q64EDwA6gu7N7Bu5bXqNxf6B/6mHos5wYZp21fQRQHNiMuznsMy6uqcuYXIlNsmOMMeY0qykYY4zJYknBGGNMFksKxhhjslhSMMYYkyXgBt8KCQnRsLAwf4dhjDEBZdWqVYdVtcqFygVcUggLCyMyMtLfYRhjTEARkT2elLPmI2OMMVksKRhjjMliScEYY0yWgOtTyEl6ejoxMTGkpKT4OxQTAEqWLEloaChBQUH+DsWYfKdAJIWYmBjKlStHWFgYIuLvcEw+pqocOXKEmJgYwsPD/R2OMfmO15qPROQDETkkIhtz2S8i8rqIRInIehG58lLPlZKSQnBwsCUEc0EiQnBwsNUqjcmFN/sUpuGe8Dw31wINnddo4O3LOZklBOMp+1sxJndeaz5S1cUiEnaeIgNxT56uwDIRqSgiNZzx4o0xptBLdWWw50gyWw+cYN2+YwxoVZNWtSt69Zz+7FOoxZlTEMY4285JCiIyGndtgjp16vgkOGOM8bXdh08SGX2UA4kpfLkmll2HT56xv3zJIK8nhYC4JVVVp6hqhKpGVKlywae0/aJs2bJZy/Pnz6dRo0bs2ePRA4R54qabbmLXrl0+O9/F2r17Nx06dKBBgwYMHTqUtLS0HMutX7+eTp060bx5c6644opz2v4HDBhAixYtstaPHj1Kr169aNiwIb169SIhIQGAefPmMXHiRO9dkDGXKc2Vybp9x5iyeCejpq0kbMK3dP/PIv7+2Xpe/nE7ItCneTUmXteMb8ZdxZan+/Jgz4Zej8ufNYVY3JOdnxbqbAtoP//8Mw888AALFiygbt26Hr3H5XJRrNil/yo2bdpERkYG9erlNkXwuTIyMihatOgln/NiPfroozz88MMMGzaMMWPGMHXqVMaOHXtGGZfLxfDhw5kxYwatWrXiyJEjZ9w2+sUXX5yRfAFeeOEFevTowYQJE3jhhRd44YUXmDx5Mv379+eJJ55gwoQJlC5dGmPyg7X7jjFv3X5WRB9lfUxi1vbKZYpzZZ2KtAytyPWtahIeUobKZYr7JUZ/JoW5wDgR+QToACTmRX/CU99sYvP+45cdXHbNapbnyesvPC/64sWLueeee5g/fz7169cHID4+njFjxrB3714A/vvf/9KlSxcmTZrE/v37iY6OJiQkhOeee47bb7+dkyfd1cU33niDzp07ExcXx9ChQzl+/Dgul4u3336bq6+++ozzzpw5k4EDB2atjx07lpUrV3Lq1CluuukmnnrKPe1wWFgYI0eO5IcffmDcuHG0a9eO++67j/j4eEqXLs17771HkyZN+Oabb3j22WdJS0sjODiYmTNnUq1atUv++akqCxcuZNYs96ySd9xxB5MmTTonKfzwww+0bNmSVq3cUykHBwdn7UtKSuKVV15hypQpDBkyJGv7119/zaJFi7KO261bNyZPnoyI0K1bN+bNm3dGeWN8JSNTWbM3gSVRh9kQk8i6mEQOJ6UC0D6sMiO7hFO7cin6XVGDauVL+jnaP3ktKYjIx0A3IEREYoAngSAAVX0HmA/0A6KAZOAub8XiC6mpqQwaNIhFixbRpEmTrO0PPvggDz/8MFdddRV79+6lT58+bNmyBYBVq1axZMkSSpUqRXJyMj/++CMlS5Zkx44d3HLLLURGRjJr1iz69OnDY489RkZGBsnJyeece+nSpdxyyy1Z6//+97+pXLkyGRkZ9OjRg/Xr19OyZUvA/eDWkiVLAOjRowfvvPMODRs2ZPny5dx7770sXLiQq666imXLliEivP/++7z44ou8/PLLZ5xz27ZtDB06NMefxaJFi6hY8c92zyNHjlCxYsWs2lBoaCixsedWCrdv346I0KdPH+Lj4xk2bBj/+Mc/AHjiiSd45JFHzvnWf/DgQWrUcE9PXL16dQ4ePJi1LyIigt9++82SgvGZk6ku/th5hB83H+TX7fEcOO5u/qxfpQwd61Wmbd1KXN+qJiFlS/g50tx58+6jWy6wX4H78vq8nnyj94agoCA6d+7M1KlTee2117K2//TTT2zevDlr/fjx4yQlJQHu9vFSpUoB7qeyx40bx9q1aylatCjbt28HoF27dowcOZL09HQGDRpE69atzzl3XFwc2ftaZs+ezZQpU3C5XMTFxbF58+aspHD6gzwpKYnff/+dm2++Oet9qanubzExMTEMHTqUuLg40tLScnzIq3Hjxqxdu/bSfli5cLlcLFmyhJUrV1K6dGl69OhB27ZtCQ4OZufOnbz66qtER0fn+n4ROeN206pVq7J///48jdGY01SVvUeT2RJ3nGW7jrIy+ijbDpzAlamUDCpC5/ohjO/ViB5NqxKcj5PA2QrEE835QZEiRZg9ezY9evTgueee41//+hcAmZmZLFu2jJIlz60elilTJmv51VdfpVq1aqxbt47MzMys8l27dmXx4sV8++233H777fz9739nxIgRZxynVKlSWR2yu3fv5j//+Q8rV66kUqVK3HnnnWd01p4+Z2ZmJhUrVszxg/3+++9n/PjxDBgwgEWLFjFp0qRzylxMTSE4OJhjx45l9Z3ExMRQq1atc94XGhpK165dCQkJAaBfv36sXr2asmXLEhkZSVhYGC6Xi0OHDtGtWzcWLVpEtWrViIuLo0aNGsTFxVG1atWs46WkpGQlXWPyQmam8sWaWP7YeYSlUYezagLFighX1qnEqKvD6Vw/hA7hlSkZ5Ls+u7wUEHcfBYrSpUvz7bffMnPmTKZOnQpA7969+d///pdVJrdv14mJidSoUYMiRYowY8YMMjIyANizZw/VqlXjnnvuYdSoUaxevfqc9zZt2pSoqCjAXRMpU6YMFSpU4ODBg3z33Xc5nq98+fKEh4czZ84cwP2tZ926dVmxnP7Qnj59eo7vP11TyOmVPSGA+xt89+7d+eyzz7KOmb0P5LQ+ffqwYcMGkpOTcblc/PrrrzRr1oyxY8dm9b8sWbKERo0aZfUjDBgwICvGs4+7ffv2M+5UMuZSpboyeP67LbR++gf+Nmcdn6+OoVH1cjzcsxGfjO7I2id7M3tMJ/55bVOuaVQlYBMCWE0hz1WuXJnvv/+erl27UqVKFV5//XXuu+8+WrZsicvlomvXrrzzzjvnvO/ee+9l8ODBzJkzh+7du2d9o1+0aBEvvfQSQUFBlC1blo8++uic9/bv359FixbRs2dPWrVqRZs2bWjevDn16tWjS5cuucY6c+ZMxo4dy7PPPkt6ejrDhg2jVatWTJo0iZtvvplatWrRsWNHdu/efdk/l8mTJzNs2DAef/xx2rRpw6hRowCYO3cukZGRPP3001SqVInx48fTrl07RIR+/frRv3//8x53woQJDBkyhKlTp1K3bl1mz56dte+XX37h+eefv+zYTeF0PCWdVdEJzFi2hzV7E0hITie0Uike6NGQWzvUoXTxgvnxKe6m/cARERGhZ8+8tmXLFpo2beqniPzv1KlTdO/enaVLl/r0NtP87ODBg9x66638/PPPOe4v7H8zJmcJJ9OYtyGOn7cc5Lcdh8nIdH8+Vi5TnMf6NeXGK2sF7DApIrJKVSMuVK5gprpCplSpUjz11FPExsbaE9+OvXv3nnPHlDFnU1U27T/OD5sO8FvUYdbtO0amupPAoNa16NuiOu3DKlOhdOEZZr3AJAVVDdgMnhf69Onj7xDylXbt2uW6L9BqxyZvqSqJp9L5YdNBZizbw4ZY90NkjauV46/X1KdboypEhFWmaJHC+XlSIJJCyZIlOXLkiA2fbS7o9HwKOd0NZgquNFcm8zfEMWv5XlbtTchqFgopW5xH+zZhUJua1Khgd6pBAUkKoaGhxMTEEB8f7+9QTAA4PfOaKfh2xifx9ZpYPo3cx8HjqVQqHUSb2hVpVbsiVzUMoWvDKoW2RpCbApEUgoKCbBYtYwzgHl5ixh/RfLkmlnXO+ELtwyvz5PXN6du8OkUsCZxXgUgKxhiz72gy03+PZuHWQ+w6fJKq5Upw91XhDG4bStMa5f0dXsCwpGCMCVgp6Rl8tiqGBZsOsDTqMJkKNSqU5MXBLRncNtSahi6BJQVjTEDJzFR+2nKQ9TGJ/N/yPRxLTqdquRLc1SWc4R3rEh5S5sIHMbmypGCMCQgnU118uz6OdxbvZFe8e4j5tnUrMTSiNjdHhNqdh3nEkoIxJt9KdWXw9Zr9fLU2lsg9CaS5MqlduRQTr2tG/5b5ax6CgsKSgjEmXzmZ6mLh1kN8v/EAi7fHcyLVReniRbmtQx36NK9Oh/DKVivwIksKxph8YUNMItP/iGbuuv2kuTIpGVSElqEVGdklnN7NqtmtpD5iScEY4zdRh5KYvyGO6b9Hc+RkGgC3dqhD/ytq0LFesN095AeWFIwxPpXmyuSrtbF8tiqGFbuPAtCoWll6N6/OQz0bWj+Bn1lSMMb4xJa44yzceoiPV+wlJuEUpYsX5a9d63Fbh7rUCS594QMYn7CkYIzxGlVlXUwibyyM4qctBwH3aKSvDm3FdS1rElTUJn/MbywpGGPylCsjk+83HeCbdfvZHHecfUdPAXBXlzDGXFPfmofyOUsKxpg8cSIlnQ+XRvPazzuyhqZuXrM8zwysx3Uta1KpTHE/R2g8YUnBGHNZ1sccY9byvXy1NpaU9EwaVSvLyC7hDGpTK6AnsC+sLCkYYy5a4ql0vlwdw4xle9gZf5KgokKf5tW5o3MYEXUr2cNlAcySgjHGYxtjE3n71518v/EAGZlKo2plebx/U25oU4vgsiX8HZ7JA5YUjDHndfB4Cm8v2sn8DXEcOpFK2RLFuLFNLYa1r8OVdSparaCAsaRgjMlRSnoGHyzdzX9/2kGaK5NrGlXh3m5V6N+yJlXKWa2goLKkYIw5Q6org09W7OP1n3dw5GQaDauW5dlBLehQL9jfoRkfsKRgjEFVmR25j1+3x/PL1nhOpWfQrEZ5Jg9uSfcmVW0MokLEkoIxhVxMQjL3zVydNcl9/ytqMKB1TXo2rWbJoBCypGBMIRUZfZRpv0fz7YY4BLj7qnAmXNuEYjb0RKHm1aQgIn2B14CiwPuq+sJZ++sA04GKTpkJqjrfmzEZU5glpbqYtXwPUxbv4nBSGqWCijI0ojaju9ajXpWy/g7P5ANeSwoiUhR4E+gFxAArRWSuqm7OVuxxYLaqvi0izYD5QJi3YjKmsIo6lMRrP+/guw1xuDKVVrUrMrxjXe6+uh5lS1iDgfmTN/8a2gNRqroLQEQ+AQYC2ZOCAuWd5QrAfi/GY0yhszM+ifd/283HK/ZSRGBQm1oMiahNR7uTyOTCm0mhFrAv23oM0OGsMpOAH0TkfqAM0DOnA4nIaGA0QJ06dfI8UGMKElVl6pLdfLshjjV7jwEwqHVNHundmNqVbd4Cc37+rjfeAkxT1ZdFpBMwQ0RaqGpm9kKqOgWYAhAREaF+iNOYgLBmbwJPzt3E+phESgYV4e6rwhnesS5hIWX8HZoJEN5MCrFA7Wzroc627EYBfQFU9Q8RKQmEAIe8GJcxBUpmpvLjloPMidzHT1sOUbF0EBOva8YdncPsllJz0byZFFYCDUUkHHcyGAbcelaZvUAPYJqINAVKAvFejMmYAuN4SjrfbzjAzOV7WBeTSLkSxRjZJZxxf2lAZZu7wFwiryUFVXWJyDhgAe7bTT9Q1U0i8jQQqapzgUeA90TkYdydzneqqjUPGXMeiafSeWPhDqb/sYc0Vya1Kpbi8f5NGd6xrs1fYC6bV/sUnGcO5p+1bWK25c1AF2/GYExBcTLVxTu/7uR/C6MA6N2sGiM6hdGpfrA1E5k84++OZmPMBRw6nsKLC7bx/cYDJKW66Na4Cvd2a0D78Mr+Ds0UQJYUjMmnNu1P5Pn5W1kSdRiA0EqlmHpHhI1WarzKkoIx+Yiq8vXa/Uz/IzrrGYNWoRX4W5/GXN2win+DM4WCJQVj8oGd8Ul8tSaWz1bFEJeYQpniRRlzTX3uvjqcEJvm0viQJQVj/Gjx9nim/x7Nz1vdj+Y0q1Gee66ux+2d6hJko5UaP7CkYIwfbDtwgie+2siK6KMADI2ozbi/NLBhKIzfWVIwxofW7E3g+40HeHfxLgD+2rUe9/2lAeVLBvk5MmPcLCkY4wOn0jL45xfr+WrtfooI/KVJVf7VrwkNqpbzd2jGnMGSgjFetnh7PP/4bD0Hjqdwe8e6/K13YyqUtpqByZ8sKRjjJVGHknjsyw0s332U8iWL8f6ICHo2q+bvsIw5L0sKxnjB12tjefCTtQCM7lqP+7o1sNqBCQgXTAoiUgp4CKirqmNEpAHQUFW/83p0xgSY3YdPMuHz9SzffZSWoRV48vpmtK1rw1GYwOFJTeEDYANwlbO+H5gDWFIwxhF77BTv/rqTmcv3kpGpXNOoCq8Pa2O1AxNwPEkKDVX1FhG5GUBVk0XEhmQ0BkhJz2Dy91v5cGk0AG3qVOTlm1tRr0pZ/wZmzCXyJCmkOTOiKYAzaU6aV6MyJgDMXbeff36+npNpGbQKrcArQ1tT35KBCXCeJIVngO+BUBGZDlwD3O3VqIzJx+at38/k77ey7+gpwkPK8GjfxvRpXh2rQJuC4IJJQVW/E5FIoDMgwN9V1eZQNoVO1KETPPvtFhZtc88YO7prPf7WuzHFi9kYRabg8OTuox9UtTfwdQ7bjCnwktNcTP1tN28uiiIjU3m4ZyPu6RpO6eJ2R7cpeHL9qxaR4kBJoJqIlMNdSwAoD9TxQWzG+FVKegZfronljYVRxB47RcvQCrwypDUNqlq/gSm4zvdV5z5gPFAV2MSfSeE48I6X4zLGr1bvTeD+WWuIPXaK0Eql7GlkU2jkmhRU9VXgVRF5SFX/68OYjPGbxOR0Ji/YyqzleylTvCiTB1/BkIja1olsCg1POpr/KyJNgGa4m5NOb5/lzcCM8bUfNx/kH5+tIyE5nZvbhvJY/6ZULF3c32EZ41OedDQ/DvQGmgALgD7AEsCSgikQUtIzmDR3E5+s3EdopVJ8eFd7Wteu6O+wjPELT26fGAq0Blar6u0iUgN417thGeN96RmZfPTHHv6zYBun0jO4plEV/ndrG5vwxhRqniSFU6qaISIu5y6kA0A9L8dljNeoKh+v2McrP27ncFIqVcqV4JUhrejbwh5AM8aTpLBGRCriHhgvEvfdR6u9GpUxXrJqz1GenreFdfuO0bxmeZ4a0Jy+LapTtIglA2PgAknBGfhukqoeA94UkQVAeVW1pGACysHjKUyau4nvNh4A4JFejRjbrT7FitrTyMZkd96koKoqIvOAts56lE+iMiaPpLoyePXHHby7eCeqcFuHOozv1YjgsiX8HZox+ZInzUcrRKSNqq7xejTG5KEvVsfw8g/biT12irrBpXnz1itpUauCv8MyJl/zJClcBdwjIjuBk7ifbFZVvdKrkRlziVSV8bPX8eWaWMJDyjDl9rb0bl7d32EZExA8SQqDLvXgItIXeA0oCryvqi/kUGYIMAn3fA3rVPXWSz2fMYeTUnn0s/X8vPUQd3YO44nrmlknsjEXwZMnmndeyoFFpCjwJtALiAFWishcVd2crUxD4J9AF1VNEJGql3IuYwB+2xHPqGmRpGVkcmfnMCZe14wilhCMuSjeHPu3PRClqrsAROQTYCCwOVuZe4A3VTUBwOZpMJdq6pLdPDNvM5VKB/Hvfi24OaK2v0MyJiB5MynUAvZlW48BOpxVphGAiCzF3cQ0SVW/P/tAIjIaGA1Qp46N2m3+lJmpPPH1RmYu30v18iX54t7O1KxYyt9hGROwPEoKIhIKNFTVX0SkBFBMVU/m0fkbAt2AUGCxiFzhPBeRRVWnAFMAIiIiNA/OawqA6MMneXj2WtbsPcbA1jX59w1XULaETXxjzOXwZEC8kcA4oAJQH6gLvAX0vMBbY4HsdfhQZ1t2McByVU0HdovIdtxJYqVH0ZtCKSnVxVu/RPHu4l1kZCrjezXi/r80sCEqjMkDnnytegB3/8ByAFXd7mGH8EqgoYiE404Gw4Cz7yz6CrgF+FBEQnA3J+3yMHZTCMUkJDNi6gr2Hk2me+OqPHFdU+oGl/F3WMYUGJ4khRRVTTv9Lcy5q+iCX8lU1SUi43APt10U+EBVN4nI00Ckqs519vUWkc1ABvB3VT1yiddiCjBV5cs1sTwzbzPJaRm8M7ytzYRmjBd4khSWisg/gJIi0h33NJ3zPDm4qs4H5p+1bWK2ZcU95ed4jyM2hc6h4yk8PHstS6OO0KR6OV4e0ormNe3JZGO8wZOk8A/cd/5sBR7E/e3e5lMwXpeZqXwauY9n523mZFoGY66pzyO9GxFkg9gZ4zWeJIX+uJ9GftvbwRhzWtShJG58aynHU1yEh5Thv0Nb08pmQzPG6zz5ynUzECUiH4pIX6dPwRivWbbrCIPeXEqmwuiu9fjh4a6WEIzxEU+GubjdeTahP3AX8K6IfKeqY7wenSl0vl0fx32zVlO1XAn+7+4ONKpWzt8hGVOoePSkj6qmisjXwCncdxINASwpmDyjqrzy43b+tzCKsODSfD62s815YIwfePLwWi9gKO6H1ZYAH3Hu8wbGXLKEk2kMnfIH2w8m0bp2Rd69va0lBGP8xJOawmjgU+B+VT3l5XhMIfPbjnj+Nmcdh06kMuaa+vyjT2Mb2dQYP/KkT+FmXwRiCpeU9Awmf7+VD5dGE1ymOJ+N6UzbupX8HZYxhV6uSUFEflXVa0QkAfcEOFm7cD93Vtnr0ZkCKT0jk3tnrmbh1kNc26I6z994BRVLF/d3WMYYzl9T6O78G+KLQEzhkJKewYOfrGHh1kP8vU9j7uvewN8hGWOyyfU5BVXNdBanqmpG9hcw1TfhmYIkzZXJXR+uZMGmgzzYo6ElBGPyIU86mltmX3EeXmvnnXBMQTV75T5e+XE7B46n8FDPhjzUs5G/QzLG5OB8fQqPAhOAciJy9PRm3P0LVlMwHjl0IoUnvtrIgk0HqRdShim3t6V38+r+DssYk4vz1RReBF4GnsedHABwmo+MuaAdB08w4oMVxCWmMLJLOBOubULxYjaYnTH52fmSQgNV3SEiM4DmpzeenldBVdd7OTYTwL5YHcPjX20kU5WZd3egSwO7X8GYQHC+pDABGAW8mcM+Bbp6JSIT0FLSMxg3aw0/bTlIsxrleXVoaxpXt/GLjAkUuSYFVR3l/Hu178IxgexkqotBby5lx6Ek7uhUl8f6N7PmImMCzAX/x4rIjSJSzlmeICKzRaSV90MzgSThZBqD3/6dqPgkxnarz1MDW1hCMCYAefK/dpKqnhCRzsD1uMdBspnXTJZft8fT69Vf2XrgBJMHt+TRvk38HZIx5hJ5khRO3210HfCWqn4O2BCWhpT0DB6ZvY47PliBiDBnTCeGRNT2d1jGmMvgycNrcSLyJnAt0FZEiuNZMjEFWOyxU4yYupyd8SdpVK0sn4/tTLmSQf4OyxhzmTxJCkOAfsD/VDVBRGqS7bkFU/gsjTrMmP9bxclUF88MbM7wjnWzblU2xgQ2T4bOThKRTUA3EekG/Kaq33k9MpPvJCan8+y3m/l8dQx1KpfmszGd7XZTYwoYT2ZeGwfcC3zlbJotIm+q6ltejczkK5+viuGROesA6N64Ci8PaU3lMjbctTEFjaczr7VX1SQAEXkO+B2wpFAIuDIyeeLrjXy8Yh/1q5Th8f7N6N6kqr/DMsZ4iSdJQYC0bOvpzjZTwJ1Ky+CGt5ay9cAJBl8Zyr9vaEHJoKL+DssY40WeJIUZwHIR+Rx3MhgETPdqVMbvNu3IrLgOAAAUrUlEQVRPZPj7y0lITmd013r8q19Tf4dkjPEBTzqaXxSRRcBVuMc8GqOqK70dmPEPVeWlBdt459edFCtahLduu5J+V9Twd1jGGB/xpKYAkAKkApnOv6YAijp0ggmfbyByTwKNq5Xjg7vaUatiKX+HZYzxIU/uPnoMuBX4Enfz0SwRmamqz3s7OOM7X6+N5cFP1gJwX/f6PNKrMUWKWNeRMYWNJzWF4UBbVU0GEJF/A6twT75jApyq8vS8zXy4NJr6VcrwwZ3tqBtcxt9hGWP8xJPhKvZwZvIoBuzy5OAi0ldEtolIlIjk+hS0iAwWERWRCE+Oa/KGKyOTcbPW8OHSaPq3rMG8+6+2hGBMIedJTSEZ2CQiC3B3NPcGlojIKwCqOj6nN4lIUdwT9PQCYoCVIjJXVTefVa4c8CCw/JKvwly0VFcGo6ZFsiTqMLd3rMszg1r4OyRjTD7gSVL41nmdtszDY7cHolR1F4CIfAIMBDafVe4ZYDLwdw+Pay5Twsk0/vp/q1ix+yiP9m3C2G71/R2SMSaf8OSW1KmXeOxawL5s6zFAh+wFRORKoLaqfisiuSYFERmN+8lq6tSpc4nhGHAPdz186nI27T/O0wObM6JTmL9DMsbkI34bAltEigCvAI9cqKyqTlHVCFWNqFKliveDK8DGz17Lpv3HeWVIK0sIxphzeDMpxALZZ1wJdbadVg5oASwSkWigIzDXOpu9Z8rinczfcIDbOtThxitD/R2OMSYf8jgpiMjFzra2EmgoIuHOxDzDgLmnd6pqoqqGqGqYqobh7qsYoKqRF3ke44EZf0Tz3PytdGtchUkDmvs7HGNMPnXBpCAi7UVkA7DDWW8lIv+70PtU1QWMAxYAW4DZqrpJRJ4WkQGXGbfxUHKai39+sZ6JczfRIbwyb9/WlqCiNnGeMSZnntx99Dru+Zm/AlDVdSLS3ZODq+p8YP5Z2ybmUrabJ8c0njuRks7IaStZGZ3AwNY1eXZQC0oVt1FOjTG58yQpFFHVPWdNt5jhpXhMHklKdXHHBytYvfcYT1zXjFFXhfs7JGNMAPAkKewTkfaAOg+k3Q9s925Y5nIcSExh5LSVbI47zqTrm3FnF0sIxhjPeJIUxuJuQqoDHAR+craZfCj+RCo3vfM7B4+n8Nqw1gxsXcvfIRljAognD68dwn3nkMnnDielMmzKH8QknOKT0R3pWC/Y3yEZYwKMJ0Nnv4d7zKMzqOpor0RkLsmWuOPc+t4yEpLTeaRXI0sIxphL4knz0U/ZlksCN3Dm8BXGz1btSWD4+8tRlGl3taNb46r+DskYE6A8aT76NPu6iMwAfvRaROaiREYfZeS0lRQvVoSP7+lIs5rl/R2SMSaAXcpTTOFA3bwOxFy8hVsPMmzKMoKKWkIwxuQNT/oUEvizT6EIcBTIdcIc4xtz1+3ngY/XEFK2BF/e25nalUv7OyRjTAFw3qQg7ifWWvHnQHaZqnpOp7PxrZnL9/DYlxtpUr0cM0Z1oEq5ix2Wyhhjcnbe5iMnAXypqhnOyxKCn329NpYnvtpI69oVmTOmkyUEY0ye8qRPYYWItPF6JOaCklJdPPjJWkSE90ZEUK5kkL9DMsYUMLk2H4lIMWek06uAe0RkJ3ASENyViCt9FKMBMjOVe2euBuC9EW2thmCM8Yrz9SmsAK4EBvkoFpOL7QdPMG7WarYfTGJ8r0b8pUk1f4dkjCmgzpcUBEBVd/ooFpODX7Yd4p7pkRQvVoQnr2/GnZ3D/B2SMaYAO19SqCIi43PbqaqveCEek83G2ET++tEqQsqW4P07ImhRq4K/QzLGFHDnSwpFgbI4NQbjW79HHeaejyIpX6oYH4/uSHhIGX+HZIwpBM6XFOJU9WmfRWKy7IxP4t5ZqwkqVoRP/9rJEoIxxmfOd0uq1RD8YO+RZG57bzmp6Zl8fE9H6lcp6++QjDGFyPlqCj18FoUB3M8h9H/9N06kuvh0dEea1rCxjIwxvpVrTUFVj/oykMLOlZHJX2dEciLVxb9vaEEHmw/BGOMHlzJKqsljGZnK/R+vYWnUESZe14zbOtggtMYY//Bkkh3jRbHHTnHbe8uIPpLMjW1qcVeXMH+HZIwpxCwp+NGh4ync9eEKYo+dYuJ1zbirSxjugWmNMcY/LCn4SWT0UUZNjyTxVDovDm7JkHa1/R2SMcZYUvCHBZsOMOb/VlG9fEk+GtmeVrUr+jskY4wBLCn43Mroozzw8RqCihbhy3u7UL1CSX+HZIwxWezuIx/aeuA4d0+PJLhMcX54qKslBGNMvmM1BR85dDyFoe8uIynVxcy7OxBmQ1cYY/Ihqyn4QNShEwx+53dOpWfw0cj2NtqpMSbf8mpSEJG+IrJNRKJEZEIO+8eLyGYRWS8iP4tIgXtqa8fBE9z41u8kJqfz0cj2dGkQ4u+QjDEmV15LCiJSFHgTuBZoBtwiIs3OKrYGiFDVlsBnwIveiscfDh5PYdT0SDIVPhndiY42dIUxJp/zZk2hPRClqrtUNQ34BBiYvYCq/qKqyc7qMiDUi/H41IaYRHq98itxiaeYcntbmtW0we2MMfmfN5NCLWBftvUYZ1tuRgHf5bRDREaLSKSIRMbHx+dhiN5xJCmVe2etIlNhzpjOdLYmI2NMgMgXHc0iMhyIAF7Kab+qTlHVCFWNqFKlim+Du0hHklLp+cqvxCac4vVbWtPaHkwzxgQQb96SGgtkH7sh1Nl2BhHpCTwGXKOqqV6Mx+sST6Uz4oMVJCSn8/6ICP7SpJq/QzLGmIvizZrCSqChiISLSHFgGDA3ewERaQO8CwxQ1UNejMXrUtIzuOGtpWzaf5znbriCns0sIRhjAo/XkoKquoBxwAJgCzBbVTeJyNMiMsAp9hJQFpgjImtFZG4uh8vXVJUHPl7DrviTvDi4Jbd2qOPvkIwx5pJ49YlmVZ0PzD9r28Rsyz29eX5fmbFsDz9sPsidncNstFNjTEDLFx3NgWz13gQmfr2JVqEVeLx/U3+HY4wxl8WSwmVQVZ6fvwWA9+6IoFhR+3EaYwKbfYpdhqe+2czK6ATu/0sDqpazEU+NMYHPksIl+mpNLNN+j+bqhiE83LORv8Mxxpg8YUnhEpxISefZbzdTs0JJ3rztSooUsXmVjTEFg82ncAle/3kHh5PSePf2tpQvGeTvcIwxJs9YTeEifb02lvd+2811LWvQ2x5QM8YUMJYULkJicjqPfbmR8JAyPH/jFYhYs5ExpmCx5iMPpWdkMuLDFSSlupgxqj3lrNnIGFMAWU3BQ5O/28q6fcd4qGdD2tSp5O9wjDHGKywpeODNX6J4f8luBl8ZyoM9Gvo7HGOM8RpLChewaNshXlqwjWsaVWHyYOtHMMYUbJYUziPxVDp3friSWhVL8dZtV9owFsaYAs8+5XKhqjz86VoAnh3UgjIlrE/eGFPwWVLIxUd/7GHh1kOM79WI7k2q+jscY4zxCUsKOUhMTuff87fQqV4w47o38Hc4xhjjM5YUcvDRH9GkuTL5W59GNq6RMaZQsaRwltmR+3j5x+20C6tE27qV/R2OMcb4lCWFbFbtSeAfn62nVsVSvD28rb/DMcYYn7Ok4Nh3NJlxs1ZTrIgw654OhJQt4e+QjDHG5+w+SyAjU7nzwxXEJaYw7a521A0u4++QjDHGL6ymALz1SxQ7409yQ5tadGtst58aYwqvQp8UjiWnMWXxLiqXKc4rQ1r5OxxjjPGrQt18lJ6RyS3vLedEqovZd3aycY2MMYVeoa4pTJq7iS1xxxndtR7tw+32U2OMKbRJIfFUOjOX7yWkbHH+eW0Tf4djjDH5QqFNCrOW7wXgyeubW7ORMcY4CmVS2BmfxH9+2EbZEsXof0UNf4djjDH5RqFLCpmZyj0fRQIw+6+dbGwjY4zJptAlha/XxbIr/iQP9mhIs5rl/R2OMcbkK4UqKbgyMnnp+22Eh5RhbLf6/g7HGGPyHa8mBRHpKyLbRCRKRCbksL+EiHzq7F8uImHejGfWir3sT0zhoZ4NCbKpNY0x5hxe+2QUkaLAm8C1QDPgFhFpdlaxUUCCqjYAXgUmeyueEynpTPx6E81rlue6ljW9dRpjjAlo3vy63B6IUtVdqpoGfAIMPKvMQGC6s/wZ0EO8dH/oGwujABjdtR5FrXPZGGNy5M2kUAvYl209xtmWYxlVdQGJQPDZBxKR0SISKSKR8fHxlxRM27qVuLNzmNUSjDHmPAJi7CNVnQJMAYiIiNBLOUbv5tXp3bx6nsZljDEFjTdrCrFA7Wzroc62HMuISDGgAnDEizEZY4w5D28mhZVAQxEJF5HiwDBg7lll5gJ3OMs3AQtV9ZJqAsYYYy6f15qPVNUlIuOABUBR4ANV3SQiTwORqjoXmArMEJEo4CjuxGGMMcZPvNqnoKrzgflnbZuYbTkFuNmbMRhjjPGcPcFljDEmiyUFY4wxWSwpGGOMyWJJwRhjTBYJtDtARSQe2HOJbw8BDudhOIHArrlwsGsuHC7nmuuqapULFQq4pHA5RCRSVSP8HYcv2TUXDnbNhYMvrtmaj4wxxmSxpGCMMSZLYUsKU/wdgB/YNRcOds2Fg9evuVD1KRhjjDm/wlZTMMYYcx6WFIwxxmQpkElBRPqKyDYRiRKRCTnsLyEinzr7l4tImO+jzFseXPN4EdksIutF5GcRqeuPOPPSha45W7nBIqIiEvC3L3pyzSIyxPldbxKRWb6OMa958LddR0R+EZE1zt93P3/EmVdE5AMROSQiG3PZLyLyuvPzWC8iV+ZpAKpaoF64h+neCdQDigPrgGZnlbkXeMdZHgZ86u+4fXDN3YHSzvLYwnDNTrlywGJgGRDh77h98HtuCKwBKjnrVf0dtw+ueQow1lluBkT7O+7LvOauwJXAxlz29wO+AwToCCzPy/MXxJpCeyBKVXepahrwCTDwrDIDgenO8mdADxERH8aY1y54zar6i6omO6vLcM+EF8g8+T0DPANMBlJ8GZyXeHLN9wBvqmoCgKoe8nGMec2Ta1agvLNcAdjvw/jynKouxj2/TG4GAh+p2zKgoojUyKvzF8SkUAvYl209xtmWYxlVdQGJQLBPovMOT645u1G4v2kEsgtes1Otrq2q3/oyMC/y5PfcCGgkIktFZJmI9PVZdN7hyTVPAoaLSAzu+Vvu901ofnOx/98vilcn2TH5j4gMByKAa/wdizeJSBHgFeBOP4fia8VwNyF1w10bXCwiV6jqMb9G5V23ANNU9WUR6YR7NscWqprp78ACUUGsKcQCtbOthzrbciwjIsVwVzmP+CQ67/DkmhGRnsBjwABVTfVRbN5yoWsuB7QAFolINO6217kB3tnsye85BpirqumquhvYjjtJBCpPrnkUMBtAVf8ASuIeOK6g8uj/+6UqiElhJdBQRMJFpDjujuS5Z5WZC9zhLN8ELFSnBydAXfCaRaQN8C7uhBDo7cxwgWtW1URVDVHVMFUNw92PMkBVI/0Tbp7w5G/7K9y1BEQkBHdz0i5fBpnHPLnmvUAPABFpijspxPs0St+aC4xw7kLqCCSqalxeHbzANR+pqktExgELcN+58IGqbhKRp4FIVZ0LTMVdxYzC3aEzzH8RXz4Pr/kloCwwx+lT36uqA/wW9GXy8JoLFA+veQHQW0Q2AxnA31U1YGvBHl7zI8B7IvIw7k7nOwP5S56IfIw7sYc4/SRPAkEAqvoO7n6TfkAUkAzclafnD+CfnTHGmDxWEJuPjDHGXCJLCsYYY7JYUjDGGJPFkoIxxpgslhSMMcZksaRg8i0RyRCRtdleYecpG5bbqJK+JiIRIvK6s9xNRDpn2zdGREb4MJbWgT5qqPGtAvecgilQTqlqa38HcbGcB+ROPyTXDUgCfnf2vZPX5xORYs4YXjlpjXtYk/l5fV5TMFlNwQQUp0bwm4isdl6dcyjTXERWOLWL9SLS0Nk+PNv2d0WkaA7vjRaRyU65FSLSINt5F8qf81HUcbbfLCIbRWSdiCx2tnUTkXlOzWYM8LBzzqtFZJKI/E1EmojIirOua4Oz3FZEfhWRVSKyIKcRMEVkmoi8IiK/AJNFpL2I/CHuOQV+F5HGzhPATwNDnfMPFZEy4h6vf4VTNqeRZU1h5u+xw+1lr9xeuJ/IXeu8vnS2lQZKOssNcT/VChCGM/488D/gNme5OFAKaAp8AwQ5298CRuRwzmjgMWd5BDDPWf4GuMNZHgl85SxvAGo5yxWdf7tle98k4G/Zjp+17lxXuLP8KPA47idXfweqONuH4n6K9+w4pwHzgKLOenmgmLPcE/jcWb4TeCPb+54Dhp+OF/fYSGX8/bu2V/55WfORyc9yaj4KAt4Qkda4k0ajHN73B/CYiIQCX6jqDhHpAbQFVjrDfJQCchsD6uNs/77qLHcCbnSWZwAvOstLgWkiMhv44mIuDvcgbkOBF5x/hwKNcQ/k96MTZ1Egt3Ft5qhqhrNcAZju1IoUZ1iEHPQGBojI35z1kkAdYMtFxm4KKEsKJtA8DBwEWuFu/jxn8hxVnSUiy4H+wAIRuRv3LFXTVfWfHpxDc1k+t6DqGBHp4JxrrZOsPPUp7rGovnAfSneIyBXAJlXt5MH7T2Zbfgb4RVVvcJqtFuXyHgEGq+q2i4jTFCLWp2ACTQUgTt1j5d+O+5v0GUSkHrBLVV/HPaJkS+Bn4CYRqeqUqSy5z1M9NNu/fzjLv/PnwIm3Ab85x6mvqstVdSJwmDOHNAY4gXsY73Oo6k7ctZ0ncCcIgG1AFXHPC4CIBIlI81zizK4Cfw6ffOd5zr8AuF+caoi4R881JoslBRNo3gLuEJFluJuOTuZQZgiwUUTWAk1wT124GXeb/Q8ish74EchtCsMSTk3jQdw1E3DP5nWX897bnX0AL4nIBud22MW45xDO7hvghtMdzTmc61NgOH/OB5CGezj3ySKyDne/wzmd6Tl4EXheRJZyZqL8BWh2uqMZd40iCFgvIpucdWOy2CipxmQj7gl5IlT1sL9jMcYfrKZgjDEmi9UUjDHGZLGagjHGmCyWFIwxxmSxpGCMMSaLJQVjjDFZLCkYY4zJ8v+ReV653+85KwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27793d3c4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_roc_curve(y_test,  model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Axes(0.547727,0.125;0.352273x0.755)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27793e07cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "print(plot_precision_recall_curve(first_arg(y_test), first_arg(model_predictions), \"Simple Sentiment Analysis\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f) Add an LSTM layer into the simple neural network architecture and re-train the model on the training set, plot the training and validation loss/accuracies, also evaluate the trained model on the test set and report the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train_lstm = x_train_cut.reshape((x_train_cut.shape[0], 1, x_train_cut.shape[1]))\n",
    "x_val_lstm = x_val_cut.reshape((x_val_cut.shape[0], 1, x_val_cut.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import keras\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(400, input_shape=(x_train_lstm.shape[1], x_train_lstm.shape[2])))\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.00001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 100, 100)          1917200   \n",
      "_________________________________________________________________\n",
      "lstm_17 (LSTM)               (None, 400)               801600    \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 256)               102656    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 2,821,970\n",
      "Trainable params: 904,770\n",
      "Non-trainable params: 1,917,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 1000 samples\n",
      "Epoch 1/600\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.7013 - acc: 0.4840 - val_loss: 0.6969 - val_acc: 0.5040\n",
      "Epoch 2/600\n",
      "1000/1000 [==============================] - 1s 682us/step - loss: 0.6976 - acc: 0.5110 - val_loss: 0.6960 - val_acc: 0.5150\n",
      "Epoch 3/600\n",
      "1000/1000 [==============================] - 1s 661us/step - loss: 0.6987 - acc: 0.4950 - val_loss: 0.6954 - val_acc: 0.5190\n",
      "Epoch 4/600\n",
      "1000/1000 [==============================] - 1s 667us/step - loss: 0.6948 - acc: 0.5110 - val_loss: 0.6949 - val_acc: 0.5210\n",
      "Epoch 5/600\n",
      "1000/1000 [==============================] - 1s 656us/step - loss: 0.6957 - acc: 0.5190 - val_loss: 0.6944 - val_acc: 0.5240\n",
      "Epoch 6/600\n",
      "1000/1000 [==============================] - 1s 659us/step - loss: 0.6974 - acc: 0.5090 - val_loss: 0.6939 - val_acc: 0.5200\n",
      "Epoch 7/600\n",
      "1000/1000 [==============================] - 1s 668us/step - loss: 0.6941 - acc: 0.5140 - val_loss: 0.6934 - val_acc: 0.5260\n",
      "Epoch 8/600\n",
      "1000/1000 [==============================] - 1s 663us/step - loss: 0.6964 - acc: 0.5020 - val_loss: 0.6929 - val_acc: 0.5230\n",
      "Epoch 9/600\n",
      "1000/1000 [==============================] - 1s 660us/step - loss: 0.6944 - acc: 0.5130 - val_loss: 0.6925 - val_acc: 0.5290\n",
      "Epoch 10/600\n",
      "1000/1000 [==============================] - 1s 671us/step - loss: 0.6914 - acc: 0.5180 - val_loss: 0.6921 - val_acc: 0.5280\n",
      "Epoch 11/600\n",
      "1000/1000 [==============================] - 1s 661us/step - loss: 0.6965 - acc: 0.5090 - val_loss: 0.6916 - val_acc: 0.5360\n",
      "Epoch 12/600\n",
      "1000/1000 [==============================] - 1s 671us/step - loss: 0.6877 - acc: 0.5320 - val_loss: 0.6911 - val_acc: 0.5360\n",
      "Epoch 13/600\n",
      "1000/1000 [==============================] - 1s 657us/step - loss: 0.6953 - acc: 0.5010 - val_loss: 0.6906 - val_acc: 0.5330\n",
      "Epoch 14/600\n",
      "1000/1000 [==============================] - 1s 670us/step - loss: 0.6898 - acc: 0.5370 - val_loss: 0.6901 - val_acc: 0.5340\n",
      "Epoch 15/600\n",
      "1000/1000 [==============================] - 1s 664us/step - loss: 0.6878 - acc: 0.5490 - val_loss: 0.6897 - val_acc: 0.5410\n",
      "Epoch 16/600\n",
      "1000/1000 [==============================] - 1s 682us/step - loss: 0.6885 - acc: 0.5310 - val_loss: 0.6893 - val_acc: 0.5420\n",
      "Epoch 17/600\n",
      "1000/1000 [==============================] - 1s 659us/step - loss: 0.6888 - acc: 0.5360 - val_loss: 0.6888 - val_acc: 0.5400\n",
      "Epoch 18/600\n",
      "1000/1000 [==============================] - 1s 663us/step - loss: 0.6868 - acc: 0.5550 - val_loss: 0.6883 - val_acc: 0.5430\n",
      "Epoch 19/600\n",
      "1000/1000 [==============================] - 1s 677us/step - loss: 0.6890 - acc: 0.5380 - val_loss: 0.6878 - val_acc: 0.5470\n",
      "Epoch 20/600\n",
      "1000/1000 [==============================] - 1s 672us/step - loss: 0.6868 - acc: 0.5480 - val_loss: 0.6873 - val_acc: 0.5500\n",
      "Epoch 21/600\n",
      "1000/1000 [==============================] - 1s 659us/step - loss: 0.6866 - acc: 0.5320 - val_loss: 0.6870 - val_acc: 0.5500\n",
      "Epoch 22/600\n",
      "1000/1000 [==============================] - 1s 666us/step - loss: 0.6848 - acc: 0.5580 - val_loss: 0.6864 - val_acc: 0.5540\n",
      "Epoch 23/600\n",
      "1000/1000 [==============================] - 1s 671us/step - loss: 0.6888 - acc: 0.5260 - val_loss: 0.6860 - val_acc: 0.5560\n",
      "Epoch 24/600\n",
      "1000/1000 [==============================] - 1s 670us/step - loss: 0.6857 - acc: 0.5500 - val_loss: 0.6856 - val_acc: 0.5600\n",
      "Epoch 25/600\n",
      "1000/1000 [==============================] - 1s 669us/step - loss: 0.6826 - acc: 0.5490 - val_loss: 0.6851 - val_acc: 0.5610\n",
      "Epoch 26/600\n",
      "1000/1000 [==============================] - 1s 662us/step - loss: 0.6819 - acc: 0.5790 - val_loss: 0.6847 - val_acc: 0.5610\n",
      "Epoch 27/600\n",
      "1000/1000 [==============================] - 1s 662us/step - loss: 0.6818 - acc: 0.5680 - val_loss: 0.6843 - val_acc: 0.5650\n",
      "Epoch 28/600\n",
      "1000/1000 [==============================] - 1s 668us/step - loss: 0.6843 - acc: 0.5410 - val_loss: 0.6838 - val_acc: 0.5680\n",
      "Epoch 29/600\n",
      "1000/1000 [==============================] - 1s 662us/step - loss: 0.6831 - acc: 0.5660 - val_loss: 0.6834 - val_acc: 0.5720\n",
      "Epoch 30/600\n",
      "1000/1000 [==============================] - 1s 657us/step - loss: 0.6799 - acc: 0.5770 - val_loss: 0.6830 - val_acc: 0.5740\n",
      "Epoch 31/600\n",
      "1000/1000 [==============================] - 1s 658us/step - loss: 0.6819 - acc: 0.5880 - val_loss: 0.6825 - val_acc: 0.5750\n",
      "Epoch 32/600\n",
      "1000/1000 [==============================] - 1s 667us/step - loss: 0.6803 - acc: 0.5570 - val_loss: 0.6821 - val_acc: 0.5740\n",
      "Epoch 33/600\n",
      "1000/1000 [==============================] - 1s 686us/step - loss: 0.6798 - acc: 0.5650 - val_loss: 0.6816 - val_acc: 0.5770\n",
      "Epoch 34/600\n",
      "1000/1000 [==============================] - 1s 664us/step - loss: 0.6794 - acc: 0.5630 - val_loss: 0.6811 - val_acc: 0.5780\n",
      "Epoch 35/600\n",
      "1000/1000 [==============================] - 1s 664us/step - loss: 0.6822 - acc: 0.5620 - val_loss: 0.6807 - val_acc: 0.5730\n",
      "Epoch 36/600\n",
      " 512/1000 [==============>...............] - ETA: 0s - loss: 0.6794 - acc: 0.5801"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 600\n",
    "batch_size = 512\n",
    "\n",
    "history = model.fit(x_train_cut,\n",
    "                    y_train, \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs,\n",
    "                    validation_data=(x_val_cut, y_val),\n",
    "                    verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_val_acc_curve(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_train_val_loss_curve(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_predictions = model.predict(x_test_cut)\n",
    "\n",
    "score = model.evaluate(x_test_cut, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_roc_curve(y_test,  model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "print(plot_precision_recall_curve(first_arg(y_test), first_arg(model_predictions), \"LSTM Sentiment Analysis\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Classification: classifying IMDB reviews\n",
    "\n",
    "In this task, you will learn how to process text data and how to train neural networks with limited input text data using pre-trained embeddings for sentiment classification (classifying a review document as \"positive\" or \"negative\" based solely on the text content of the review)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `Embedding` layer in Keras to represent text input. The `Embedding` layer is best understood as a dictionary mapping integer indices (which stand for specific words) to dense vectors. It takes as input integers, then looks up these integers into an internal dictionary, and finally returns the associated vectors. It's effectively a dictionary lookup.\n",
    "\n",
    "The `Embedding` layer takes as input a 2D tensor of integers, of shape `(samples, sequence_length)`, where each entry is a sequence of integers. It can embed sequences of variable lengths, so for instance we could feed into our embedding layer above batches that could have  shapes `(32, 10)` (batch of 32 sequences of length 10) or `(64, 15)` (batch of 64 sequences of length 15). All sequences in a batch must have the same length, though (since we need to pack them into a single tensor), so sequences that are shorter than others should be padded with zeros, and sequences that are longer should be truncated.\n",
    "\n",
    "This layer returns a 3D floating point tensor, of shape `(samples, sequence_length, embedding_dimensionality)`. Such a 3D tensor can then be processed by a RNN layer or a 1D convolution layer.\n",
    "\n",
    "You can instantiate the `Embedding` layer by randomly initialising its weights (its internal dictionary of token vectors). During training, these word vectors will be gradually adjusted via backpropagation, structuring the space into something that the downstream model can exploit. Once fully trained, your embedding space will show a lot of structure -- a kind of structure specialized for the specific problem you were training your model for. You can also instantiate the `Embedding` layer by intialising its weights using the pre-trained word embeddings, such as GloVe word embeddings pretrained from Wikipedia articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Download the IMDB data as raw text\n",
    "\n",
    "First, create a \"data\" directory, then head to `http://ai.stanford.edu/~amaas/data/sentiment/` and download the raw IMDB dataset (if the URL isn't working anymore, just Google \"IMDB dataset\"). Save it into the \"data\" directory. Uncompress it. Store the individual reviews into a list of strings, one string per review, and also collect the review labels (positive / negative) into a separate `labels` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/train/pos\n",
      "./data/test/neg\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "\n",
    "# write your code here\n",
    "\n",
    "# Numpy random seed\n",
    "SEED = 200\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_DIR = './data'\n",
    "TRAIN_DIR = DATA_DIR+ \"/\" +\"train\"\n",
    "TEST_DIR = DATA_DIR+ \"/\" +\"test\"\n",
    "\n",
    "POS_DIR = \"pos\"\n",
    "NEG_DIR = \"neg\"\n",
    "\n",
    "TRAIN_POS_DIR = TRAIN_DIR + \"/\" + POS_DIR\n",
    "TRAIN_NEG_DIR = TRAIN_DIR + \"/\" + NEG_DIR\n",
    "\n",
    "TEST_POS_DIR = TEST_DIR + \"/\" + POS_DIR\n",
    "TEST_NEG_DIR = TEST_DIR+ \"/\" + NEG_DIR\n",
    "\n",
    "print(TRAIN_POS_DIR)\n",
    "print(TEST_NEG_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_review(file_path):\n",
    "    \n",
    "    f = open(file_path, \"r\", encoding=\"utf8\")\n",
    "    return f.read()\n",
    "\n",
    "\n",
    "def get_file_paths_in_dir(dir_path):\n",
    "    \n",
    "    return [ dir_path + \"/\" + file_path for file_path in os.listdir(dir_path) ]\n",
    "\n",
    "\n",
    "def read_all_reviews_in_dir(dir_path):\n",
    "    \n",
    "    _reviews = []\n",
    "    file_paths = get_file_paths_in_dir(dir_path)\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        _reviews.append(read_review(file_path))\n",
    "    \n",
    "    return _reviews\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reviews = []\n",
    "labels = []\n",
    "\n",
    "rev_train_pos = read_all_reviews_in_dir(TRAIN_POS_DIR)\n",
    "rev_train_neg = read_all_reviews_in_dir(TRAIN_NEG_DIR)\n",
    "\n",
    "for i in range(len(rev_train_pos)):\n",
    "    labels.append(1) #1: positive 0: negative\n",
    "\n",
    "for i in range(len(rev_train_neg)):\n",
    "    labels.append(0)\n",
    "    \n",
    "reviews.extend(rev_train_pos)\n",
    "reviews.extend(rev_train_neg)\n",
    "\n",
    "\n",
    "\n",
    "test_reviews = []\n",
    "test_labels = []\n",
    "\n",
    "rev_test_pos = read_all_reviews_in_dir(TEST_POS_DIR)\n",
    "rev_test_neg = read_all_reviews_in_dir(TEST_NEG_DIR)\n",
    "\n",
    "test_reviews.extend(rev_test_pos)\n",
    "test_reviews.extend(rev_test_neg)\n",
    "\n",
    "for i in range(len(rev_test_pos)):\n",
    "    test_labels.append(1) #1: positive 0: negative\n",
    "\n",
    "for i in range(len(rev_test_neg)):\n",
    "    test_labels.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "25000\n",
      "25000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(len(reviews))\n",
    "print(len(labels))\n",
    "print(len(test_reviews))\n",
    "print(len(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Pre-process the review documents \n",
    "\n",
    "Pre-process review documents by tokenisation and split the data into the training and testing sets. You can restrict the training data to the first 1000 reviews and only consider the top 5,000 words in the dataset. You can also cut reviews after 100 words (that is, each review contains a maximum of 100 words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Applications\\Python3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TRAIN_SIZE = 1000 / len(reviews)\n",
    "\n",
    "\n",
    "print(TRAIN_SIZE)\n",
    "\n",
    "x_train, _, y_train, __ = train_test_split(reviews, labels, test_size=1 - TRAIN_SIZE, random_state=SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SENT_LEN = 100\n",
    "\n",
    "def limit_to_x_words(sent_arr, x_words):\n",
    "    \n",
    "    _cut_arr = []\n",
    "    for i in range(len(sent_arr)):\n",
    "        words = sent_arr[i].split(\" \")\n",
    "        sent = \" \".join(words[:100])\n",
    "        _cut_arr.append(sent)\n",
    "    \n",
    "    return _cut_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_cut = limit_to_x_words(x_train, SENT_LEN)\n",
    "x_test_cut = limit_to_x_words(test_reviews, SENT_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_WORD_COUNT = 5000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=TOP_WORD_COUNT+1, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ')\n",
    "\n",
    "tokenizer.fit_on_texts(x_train_cut)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train_tokenized = tokenizer.texts_to_sequences(x_train_cut)\n",
    "# x_test_tokenized = tokenizer.texts_to_sequences(x_test_cut)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train_tokenized))\n",
    "print(len(x_test_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(tokenizer.word_index.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Download the GloVe word embeddings and map each word in the dataset into its pre-trained GloVe word embedding.\n",
    "\n",
    "First go to `https://nlp.stanford.edu/projects/glove/` and download the pre-trained \n",
    "embeddings from 2014 English Wikipedia into the \"data\" directory. It's a 822MB zip file named `glove.6B.zip`, containing 100-dimensional embedding vectors for \n",
    "400,000 words (or non-word tokens). Un-zip it.\n",
    "\n",
    "Parse the un-zipped file (it's a `txt` file) to build an index mapping words (as strings) to their vector representation (as number vectors).\n",
    "\n",
    "Build an embedding matrix that will be loaded into an `Embedding` layer later. It must be a matrix of shape `(max_words, embedding_dim)`, where each entry `i` contains the `embedding_dim`-dimensional vector for the word of index `i` in our reference word index \n",
    "(built during tokenization). Note that the index `0` is not supposed to stand for any word or token -- it's a placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11557"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write your code here\n",
    "len(list(tokenizer.word_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GLOVE_FILE_PATH = \"\"\n",
    "\n",
    "def load_glove(file_path):\n",
    "    \n",
    "    embeddings_index = np.array([])\n",
    "    f = open('/kaggle/input/embeddings/glove.840B.300d/glove.840B.300d.txt')\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0] ## The first entry is the word\n",
    "        coefs = np.asarray(values[1:], dtype='float32') ## These are the vecotrs representing the embedding for the word\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    \n",
    "    return embeddings_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Build and train a simple Sequential model\n",
    "\n",
    "The model contains an Embedding Layer with maximum number of tokens to be 10,000 and embedding dimensionality as 100. Initialise the Embedding Layer with the pre-trained GloVe word vectors. Set the maximum length of each review to 100. Flatten the 3D embedding output to 2D and add a Dense Layer which is the classifier. Train the model with a 'rmsprop' optimiser. You need to freeze the embedding layer by setting its `trainable` attribute to `False` so that its weights will not be updated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "# write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e) Plot the training and validation loss and accuracies and evaluate the trained model on the test set.\n",
    "\n",
    "What do you observe from the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f) Add an LSTM layer into the simple neural network architecture and re-train the model on the training set, plot the training and validation loss/accuracies, also evaluate the trained model on the test set and report the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "\n",
    "# write your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

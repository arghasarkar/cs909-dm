{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Classification: classifying IMDB reviews\n",
    "\n",
    "In this task, you will learn how to process text data and how to train neural networks with limited input text data using pre-trained embeddings for sentiment classification (classifying a review document as \"positive\" or \"negative\" based solely on the text content of the review)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `Embedding` layer in Keras to represent text input. The `Embedding` layer is best understood as a dictionary mapping integer indices (which stand for specific words) to dense vectors. It takes as input integers, then looks up these integers into an internal dictionary, and finally returns the associated vectors. It's effectively a dictionary lookup.\n",
    "\n",
    "The `Embedding` layer takes as input a 2D tensor of integers, of shape `(samples, sequence_length)`, where each entry is a sequence of integers. It can embed sequences of variable lengths, so for instance we could feed into our embedding layer above batches that could have  shapes `(32, 10)` (batch of 32 sequences of length 10) or `(64, 15)` (batch of 64 sequences of length 15). All sequences in a batch must have the same length, though (since we need to pack them into a single tensor), so sequences that are shorter than others should be padded with zeros, and sequences that are longer should be truncated.\n",
    "\n",
    "This layer returns a 3D floating point tensor, of shape `(samples, sequence_length, embedding_dimensionality)`. Such a 3D tensor can then be processed by a RNN layer or a 1D convolution layer.\n",
    "\n",
    "You can instantiate the `Embedding` layer by randomly initialising its weights (its internal dictionary of token vectors). During training, these word vectors will be gradually adjusted via backpropagation, structuring the space into something that the downstream model can exploit. Once fully trained, your embedding space will show a lot of structure -- a kind of structure specialized for the specific problem you were training your model for. You can also instantiate the `Embedding` layer by intialising its weights using the pre-trained word embeddings, such as GloVe word embeddings pretrained from Wikipedia articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Download the IMDB data as raw text\n",
    "\n",
    "First, create a \"data\" directory, then head to `http://ai.stanford.edu/~amaas/data/sentiment/` and download the raw IMDB dataset (if the URL isn't working anymore, just Google \"IMDB dataset\"). Save it into the \"data\" directory. Uncompress it. Store the individual reviews into a list of strings, one string per review, and also collect the review labels (positive / negative) into a separate `labels` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/train/pos\n",
      "./data/test/neg\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "\n",
    "# write your code here\n",
    "\n",
    "# Numpy random seed\n",
    "SEED = 200\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_DIR = './data'\n",
    "TRAIN_DIR = DATA_DIR+ \"/\" +\"train\"\n",
    "TEST_DIR = DATA_DIR+ \"/\" +\"test\"\n",
    "\n",
    "POS_DIR = \"pos\"\n",
    "NEG_DIR = \"neg\"\n",
    "\n",
    "TRAIN_POS_DIR = TRAIN_DIR + \"/\" + POS_DIR\n",
    "TRAIN_NEG_DIR = TRAIN_DIR + \"/\" + NEG_DIR\n",
    "\n",
    "TEST_POS_DIR = TEST_DIR + \"/\" + POS_DIR\n",
    "TEST_NEG_DIR = TEST_DIR+ \"/\" + NEG_DIR\n",
    "\n",
    "print(TRAIN_POS_DIR)\n",
    "print(TEST_NEG_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_review(file_path):\n",
    "    \n",
    "    f = open(file_path, \"r\", encoding=\"utf8\")\n",
    "    return f.read()\n",
    "\n",
    "\n",
    "def get_file_paths_in_dir(dir_path):\n",
    "    \n",
    "    return [ dir_path + \"/\" + file_path for file_path in os.listdir(dir_path) ]\n",
    "\n",
    "\n",
    "def read_all_reviews_in_dir(dir_path):\n",
    "    \n",
    "    _reviews = []\n",
    "    file_paths = get_file_paths_in_dir(dir_path)\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        _reviews.append(read_review(file_path))\n",
    "    \n",
    "    return _reviews\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reviews = []\n",
    "labels = []\n",
    "\n",
    "rev_train_pos = read_all_reviews_in_dir(TRAIN_POS_DIR)\n",
    "rev_train_neg = read_all_reviews_in_dir(TRAIN_NEG_DIR)\n",
    "\n",
    "for i in range(len(rev_train_pos)):\n",
    "    labels.append(1) #1: positive 0: negative\n",
    "\n",
    "for i in range(len(rev_train_neg)):\n",
    "    labels.append(0)\n",
    "    \n",
    "reviews.extend(rev_train_pos)\n",
    "reviews.extend(rev_train_neg)\n",
    "\n",
    "\n",
    "\n",
    "test_reviews = []\n",
    "test_labels = []\n",
    "\n",
    "rev_test_pos = read_all_reviews_in_dir(TEST_POS_DIR)\n",
    "rev_test_neg = read_all_reviews_in_dir(TEST_NEG_DIR)\n",
    "\n",
    "test_reviews.extend(rev_test_pos)\n",
    "test_reviews.extend(rev_test_neg)\n",
    "\n",
    "for i in range(len(rev_test_pos)):\n",
    "    test_labels.append(1) #1: positive 0: negative\n",
    "\n",
    "for i in range(len(rev_test_neg)):\n",
    "    test_labels.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "25000\n",
      "25000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(len(reviews))\n",
    "print(len(labels))\n",
    "print(len(test_reviews))\n",
    "print(len(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Pre-process the review documents \n",
    "\n",
    "Pre-process review documents by tokenisation and split the data into the training and testing sets. You can restrict the training data to the first 1000 reviews and only consider the top 5,000 words in the dataset. You can also cut reviews after 100 words (that is, each review contains a maximum of 100 words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Applications\\Python3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TRAIN_SIZE = 1000 / len(reviews)\n",
    "\n",
    "\n",
    "print(TRAIN_SIZE)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(reviews, labels, test_size=1 - TRAIN_SIZE, random_state=SEED)\n",
    "\n",
    "x_val = x_val[:1000]\n",
    "y_val = y_val[:1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_WORD_COUNT = 5000\n",
    "SENT_LEN = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=TOP_WORD_COUNT+1, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ')\n",
    "\n",
    "tokenizer.fit_on_texts(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train_tokenized = tokenizer.texts_to_sequences(x_train)\n",
    "x_test_tokenized = tokenizer.texts_to_sequences(test_reviews)\n",
    "x_val_tokenized = tokenizer.texts_to_sequences(x_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train_cut = pad_sequences(x_train_tokenized, maxlen=SENT_LEN)\n",
    "x_test_cut = pad_sequences(x_test_tokenized, maxlen=SENT_LEN)\n",
    "x_val_cut = pad_sequences(x_val_tokenized, maxlen=SENT_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "25000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train_cut))\n",
    "print(len(x_test_cut))\n",
    "print(len(x_val_cut))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19171"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(tokenizer.word_index.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Download the GloVe word embeddings and map each word in the dataset into its pre-trained GloVe word embedding.\n",
    "\n",
    "First go to `https://nlp.stanford.edu/projects/glove/` and download the pre-trained \n",
    "embeddings from 2014 English Wikipedia into the \"data\" directory. It's a 822MB zip file named `glove.6B.zip`, containing 100-dimensional embedding vectors for \n",
    "400,000 words (or non-word tokens). Un-zip it.\n",
    "\n",
    "Parse the un-zipped file (it's a `txt` file) to build an index mapping words (as strings) to their vector representation (as number vectors).\n",
    "\n",
    "Build an embedding matrix that will be loaded into an `Embedding` layer later. It must be a matrix of shape `(max_words, embedding_dim)`, where each entry `i` contains the `embedding_dim`-dimensional vector for the word of index `i` in our reference word index \n",
    "(built during tokenization). Note that the index `0` is not supposed to stand for any word or token -- it's a placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19171"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write your code here\n",
    "len(list(tokenizer.word_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GLOVE_FILE_PATH = \"./glove.6B.100d.txt\"\n",
    "\n",
    "def load_glove(file_path):\n",
    "    \n",
    "    embeddings_index = {}\n",
    "    f = open(file_path, \"r\", encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    \n",
    "    return embeddings_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word_emb = load_glove(GLOVE_FILE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = word_emb.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19172, 100)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_to_two_labels(label_arr):\n",
    "    \n",
    "    _lab_arr = []\n",
    "    for i in range(len(label_arr)):\n",
    "        if label_arr[i] == 0:\n",
    "            _lab_arr.append(np.array([0, 1]))\n",
    "        else:\n",
    "            _lab_arr.append(np.array([1, 0]))\n",
    "    \n",
    "    return np.array(_lab_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = convert_to_two_labels(y_train)\n",
    "y_val = convert_to_two_labels(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Build and train a simple Sequential model\n",
    "\n",
    "The model contains an Embedding Layer with maximum number of tokens to be 10,000 and embedding dimensionality as 100. Initialise the Embedding Layer with the pre-trained GloVe word vectors. Set the maximum length of each review to 100. Flatten the 3D embedding output to 2D and add a Dense Layer which is the classifier. Train the model with a 'rmsprop' optimiser. You need to freeze the embedding layer by setting its `trainable` attribute to `False` so that its weights will not be updated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.layers.embeddings.Embedding object at 0x0000026466A35BA8>\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "\n",
    "# write your code here\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=SENT_LEN,\n",
    "                            trainable=False)\n",
    "\n",
    "print(embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(embedding_layer)\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(200, activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.5)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 100, 100)          1917200   \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 200)               2000200   \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 3,917,802\n",
      "Trainable params: 2,000,602\n",
      "Non-trainable params: 1,917,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 1000 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 0s 409us/step - loss: 0.0597 - acc: 0.9760 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s 56us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s 55us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s 54us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s 56us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 0s 56us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 0s 56us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 0s 54us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 0s 54us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 0s 54us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 0s 54us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 0s 56us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 0s 55us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 0s 56us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 0s 55us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 0s 55us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 0s 54us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 0s 55us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 0s 54us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit(x_train_cut,\n",
    "                    y_train, \n",
    "                    batch_size=128, \n",
    "                    epochs=50,\n",
    "                    validation_data=(x_val_cut, y_val),\n",
    "                    verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e) Plot the training and validation loss and accuracies and evaluate the trained model on the test set.\n",
    "\n",
    "What do you observe from the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_train_val_acc_curve(_hist):\n",
    "    \n",
    "    plt.plot(_hist.history['acc'])\n",
    "    plt.plot(_hist.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    \n",
    "    return plt.show()\n",
    "\n",
    "\n",
    "def plot_train_val_loss_curve(_hist):\n",
    "    \n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    \n",
    "\n",
    "# write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x265ffb0ef28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_train_val_acc_curve(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x265ffb0ee80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plot_train_val_loss_curve(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f) Add an LSTM layer into the simple neural network architecture and re-train the model on the training set, plot the training and validation loss/accuracies, also evaluate the trained model on the test set and report the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "\n",
    "# write your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
